{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":765568},"median":{"secs":0,"nanos":615680},"variance":{"secs":0,"nanos":200},"min":{"secs":0,"nanos":613888},"max":{"secs":0,"nanos":2108928}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":30950},"median":{"secs":0,"nanos":31232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28160},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":69401},"median":{"secs":0,"nanos":69376},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":68608},"max":{"secs":0,"nanos":70912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":114944},"median":{"secs":0,"nanos":115712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104192},"max":{"secs":0,"nanos":117248}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17356},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16384},"max":{"secs":0,"nanos":18944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":58803},"median":{"secs":0,"nanos":58624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58112},"max":{"secs":0,"nanos":60672}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":590771},"median":{"secs":0,"nanos":590848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":588800},"max":{"secs":0,"nanos":592384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20608},"median":{"secs":0,"nanos":22528},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16640},"max":{"secs":0,"nanos":23808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":63820},"median":{"secs":0,"nanos":66560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52224},"max":{"secs":0,"nanos":67328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":99481},"median":{"secs":0,"nanos":95744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95488},"max":{"secs":0,"nanos":108288}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":14182},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":16128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":54451},"median":{"secs":0,"nanos":55808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":56320}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":775936},"median":{"secs":0,"nanos":625152},"variance":{"secs":0,"nanos":206},"min":{"secs":0,"nanos":622080},"max":{"secs":0,"nanos":2137344}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32025},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31232},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":113843},"median":{"secs":0,"nanos":113920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":113152},"max":{"secs":0,"nanos":114432}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":619084},"median":{"secs":0,"nanos":619008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":611072},"max":{"secs":0,"nanos":622848}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32204},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":33024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116326},"median":{"secs":0,"nanos":116224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":115712},"max":{"secs":0,"nanos":116992}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":31488},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30720},"max":{"secs":0,"nanos":32000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":111744},"median":{"secs":0,"nanos":111616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":110848},"max":{"secs":0,"nanos":112384}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":736153},"median":{"secs":0,"nanos":616960},"variance":{"secs":0,"nanos":135},"min":{"secs":0,"nanos":605696},"max":{"secs":0,"nanos":1838592}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":627507},"median":{"secs":0,"nanos":631040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":618496},"max":{"secs":0,"nanos":633344}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":873625},"median":{"secs":0,"nanos":605696},"variance":{"secs":0,"nanos":290},"min":{"secs":0,"nanos":604160},"max":{"secs":0,"nanos":2083072}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":24012},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23552},"max":{"secs":0,"nanos":24576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":108364},"median":{"secs":0,"nanos":109568},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96768},"max":{"secs":0,"nanos":110080}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4240230},"median":{"secs":0,"nanos":4316160},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":4046848},"max":{"secs":0,"nanos":4456448}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4211097},"median":{"secs":0,"nanos":4317440},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":3990528},"max":{"secs":0,"nanos":4470784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4334822},"median":{"secs":0,"nanos":4368896},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":4058112},"max":{"secs":0,"nanos":4598784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3676416},"median":{"secs":0,"nanos":3652608},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":3367168},"max":{"secs":0,"nanos":4134656}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":41088},"median":{"secs":0,"nanos":40704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39936},"max":{"secs":0,"nanos":43008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":119142},"median":{"secs":0,"nanos":117248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":114176},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":216729},"median":{"secs":0,"nanos":197120},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":195328},"max":{"secs":0,"nanos":325120}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":48409},"median":{"secs":0,"nanos":43520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39936},"max":{"secs":0,"nanos":76544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":289484},"median":{"secs":0,"nanos":252416},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":250112},"max":{"secs":0,"nanos":391680}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27904},"median":{"secs":0,"nanos":25856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24320},"max":{"secs":0,"nanos":40192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":66406},"median":{"secs":0,"nanos":55296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":53504},"max":{"secs":0,"nanos":98560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":159564},"median":{"secs":0,"nanos":155904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":153856},"max":{"secs":0,"nanos":194816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":475392},"median":{"secs":0,"nanos":197120},"variance":{"secs":0,"nanos":634},"min":{"secs":0,"nanos":193792},"max":{"secs":0,"nanos":2864896}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3578828},"median":{"secs":0,"nanos":3540992},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":3368960},"max":{"secs":0,"nanos":4009216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":40704},"median":{"secs":0,"nanos":40448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39424},"max":{"secs":0,"nanos":43008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":106188},"median":{"secs":0,"nanos":105984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":105472},"max":{"secs":0,"nanos":109312}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":61491},"median":{"secs":0,"nanos":61440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":60928},"max":{"secs":0,"nanos":62208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":272256},"median":{"secs":0,"nanos":269312},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":215808},"max":{"secs":0,"nanos":400128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":471987},"median":{"secs":0,"nanos":465664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":447744},"max":{"secs":0,"nanos":513536}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17408},"median":{"secs":0,"nanos":16640},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15872},"max":{"secs":0,"nanos":20480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":34585},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":39680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":79001},"median":{"secs":0,"nanos":73216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":61184},"max":{"secs":0,"nanos":137984}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":46720},"median":{"secs":0,"nanos":50688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":36608},"max":{"secs":0,"nanos":52736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":100838},"median":{"secs":0,"nanos":90368},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":82176},"max":{"secs":0,"nanos":138752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":114508},"median":{"secs":0,"nanos":151552},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":156928}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9303628},"median":{"secs":0,"nanos":9185792},"variance":{"secs":0,"nanos":902},"min":{"secs":0,"nanos":7922432},"max":{"secs":0,"nanos":11433728}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":350310},"median":{"secs":0,"nanos":390400},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":306176},"max":{"secs":0,"nanos":398848}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20198},"median":{"secs":0,"nanos":22784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14848},"max":{"secs":0,"nanos":25856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":102630},"median":{"secs":0,"nanos":98816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":98304},"max":{"secs":0,"nanos":111360}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9676},"median":{"secs":0,"nanos":9216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":11520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":25728},"median":{"secs":0,"nanos":25856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":25088},"max":{"secs":0,"nanos":26112}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17587},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16384},"max":{"secs":0,"nanos":19968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":40140},"median":{"secs":0,"nanos":37632},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":37120},"max":{"secs":0,"nanos":48384}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4450124},"median":{"secs":0,"nanos":4514304},"variance":{"secs":0,"nanos":65},"min":{"secs":0,"nanos":4007424},"max":{"secs":0,"nanos":4752384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":186009},"median":{"secs":0,"nanos":190208},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175360},"max":{"secs":0,"nanos":191232}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17382},"median":{"secs":0,"nanos":16896},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":19200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":40192},"median":{"secs":0,"nanos":36608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35072},"max":{"secs":0,"nanos":47104}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":18073},"median":{"secs":0,"nanos":18944},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":19456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":32793},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23296},"max":{"secs":0,"nanos":35072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":72704},"median":{"secs":0,"nanos":46080},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":43776},"max":{"secs":0,"nanos":115712}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8140},"median":{"secs":0,"nanos":7936},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7680},"max":{"secs":0,"nanos":8704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":14694},"median":{"secs":0,"nanos":14592},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":17152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":96998},"median":{"secs":0,"nanos":96768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96000},"max":{"secs":0,"nanos":98048}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":82892},"median":{"secs":0,"nanos":82432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81408},"max":{"secs":0,"nanos":84736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":166067},"median":{"secs":0,"nanos":164864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":164096},"max":{"secs":0,"nanos":173568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":284876},"median":{"secs":0,"nanos":281088},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":275456},"max":{"secs":0,"nanos":324096}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15816780},"median":{"secs":0,"nanos":15639552},"variance":{"secs":0,"nanos":133},"min":{"secs":0,"nanos":15468032},"max":{"secs":0,"nanos":16578816}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":206233},"median":{"secs":0,"nanos":206080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":204800},"max":{"secs":0,"nanos":208384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1272704},"median":{"secs":0,"nanos":1189376},"variance":{"secs":0,"nanos":54},"min":{"secs":0,"nanos":1183744},"max":{"secs":0,"nanos":1971456}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":57728},"median":{"secs":0,"nanos":55296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52736},"max":{"secs":0,"nanos":85760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":79052},"median":{"secs":0,"nanos":69888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":66560},"max":{"secs":0,"nanos":119296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":172390},"median":{"secs":0,"nanos":151296},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":146688},"max":{"secs":0,"nanos":212992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":333312},"median":{"secs":0,"nanos":323584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":322048},"max":{"secs":0,"nanos":385024}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32102},"median":{"secs":0,"nanos":29696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26624},"max":{"secs":0,"nanos":47616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":57420},"median":{"secs":0,"nanos":53760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":51200},"max":{"secs":0,"nanos":89856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":118041},"median":{"secs":0,"nanos":74240},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":71680},"max":{"secs":0,"nanos":226560}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":104345},"median":{"secs":0,"nanos":113408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88064},"max":{"secs":0,"nanos":118784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":212940},"median":{"secs":0,"nanos":183296},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":180480},"max":{"secs":0,"nanos":336128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":215577},"median":{"secs":0,"nanos":292608},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":105984},"max":{"secs":0,"nanos":337664}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9138252},"median":{"secs":0,"nanos":9066752},"variance":{"secs":0,"nanos":369},"min":{"secs":0,"nanos":8475648},"max":{"secs":0,"nanos":10605056}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":752998},"median":{"secs":0,"nanos":719616},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":716288},"max":{"secs":0,"nanos":999424}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1311488},"median":{"secs":0,"nanos":1308928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":1306112},"max":{"secs":0,"nanos":1345024}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9856},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9216},"max":{"secs":0,"nanos":12032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":16614},"median":{"secs":0,"nanos":15872},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15104},"max":{"secs":0,"nanos":25344}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":16947},"median":{"secs":0,"nanos":17152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16128},"max":{"secs":0,"nanos":17408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":37248},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":34048},"max":{"secs":0,"nanos":44544}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2542156},"median":{"secs":0,"nanos":2523648},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":2521856},"max":{"secs":0,"nanos":2711808}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":214912},"median":{"secs":0,"nanos":214784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":214528},"max":{"secs":0,"nanos":216064}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23347},"median":{"secs":0,"nanos":23040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":21504},"max":{"secs":0,"nanos":26112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":41344},"median":{"secs":0,"nanos":41984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":38400},"max":{"secs":0,"nanos":44544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":127155},"median":{"secs":0,"nanos":108032},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":100608},"max":{"secs":0,"nanos":166656}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":167808},"median":{"secs":0,"nanos":167936},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":165376},"max":{"secs":0,"nanos":169728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":255052},"median":{"secs":0,"nanos":169216},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":166912},"max":{"secs":0,"nanos":389376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":364876},"median":{"secs":0,"nanos":416512},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":298240},"max":{"secs":0,"nanos":446208}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":12051020},"median":{"secs":0,"nanos":12047104},"variance":{"secs":0,"nanos":50},"min":{"secs":0,"nanos":11534336},"max":{"secs":0,"nanos":12433408}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":216678},"median":{"secs":0,"nanos":216576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":215552},"max":{"secs":0,"nanos":219136}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1350195},"median":{"secs":0,"nanos":1242880},"variance":{"secs":0,"nanos":81},"min":{"secs":0,"nanos":1241344},"max":{"secs":0,"nanos":2199552}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":10393},"median":{"secs":0,"nanos":10240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":12288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":16409},"median":{"secs":0,"nanos":15616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14848},"max":{"secs":0,"nanos":25088}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":22374},"median":{"secs":0,"nanos":22016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":21504},"max":{"secs":0,"nanos":24320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":35814},"median":{"secs":0,"nanos":33536},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":33024},"max":{"secs":0,"nanos":42240}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4673689},"median":{"secs":0,"nanos":4719360},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":4537088},"max":{"secs":0,"nanos":4898560}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":204416},"median":{"secs":0,"nanos":204288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":203776},"max":{"secs":0,"nanos":205056}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":247859},"median":{"secs":0,"nanos":202496},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":201472},"max":{"secs":0,"nanos":657152}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":21606},"median":{"secs":0,"nanos":21760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":20736},"max":{"secs":0,"nanos":22272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":33152},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32512},"max":{"secs":0,"nanos":33792}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
