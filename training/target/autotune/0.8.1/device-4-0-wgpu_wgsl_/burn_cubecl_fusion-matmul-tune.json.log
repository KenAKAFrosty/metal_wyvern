{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":765568},"median":{"secs":0,"nanos":615680},"variance":{"secs":0,"nanos":200},"min":{"secs":0,"nanos":613888},"max":{"secs":0,"nanos":2108928}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":30950},"median":{"secs":0,"nanos":31232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28160},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":69401},"median":{"secs":0,"nanos":69376},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":68608},"max":{"secs":0,"nanos":70912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":114944},"median":{"secs":0,"nanos":115712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104192},"max":{"secs":0,"nanos":117248}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17356},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16384},"max":{"secs":0,"nanos":18944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":58803},"median":{"secs":0,"nanos":58624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58112},"max":{"secs":0,"nanos":60672}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":590771},"median":{"secs":0,"nanos":590848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":588800},"max":{"secs":0,"nanos":592384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20608},"median":{"secs":0,"nanos":22528},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16640},"max":{"secs":0,"nanos":23808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":63820},"median":{"secs":0,"nanos":66560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52224},"max":{"secs":0,"nanos":67328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":99481},"median":{"secs":0,"nanos":95744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95488},"max":{"secs":0,"nanos":108288}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":14182},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":16128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":54451},"median":{"secs":0,"nanos":55808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":56320}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":775936},"median":{"secs":0,"nanos":625152},"variance":{"secs":0,"nanos":206},"min":{"secs":0,"nanos":622080},"max":{"secs":0,"nanos":2137344}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32025},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31232},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":113843},"median":{"secs":0,"nanos":113920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":113152},"max":{"secs":0,"nanos":114432}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":619084},"median":{"secs":0,"nanos":619008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":611072},"max":{"secs":0,"nanos":622848}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32204},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":33024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116326},"median":{"secs":0,"nanos":116224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":115712},"max":{"secs":0,"nanos":116992}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":31488},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30720},"max":{"secs":0,"nanos":32000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":111744},"median":{"secs":0,"nanos":111616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":110848},"max":{"secs":0,"nanos":112384}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":736153},"median":{"secs":0,"nanos":616960},"variance":{"secs":0,"nanos":135},"min":{"secs":0,"nanos":605696},"max":{"secs":0,"nanos":1838592}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":627507},"median":{"secs":0,"nanos":631040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":618496},"max":{"secs":0,"nanos":633344}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":873625},"median":{"secs":0,"nanos":605696},"variance":{"secs":0,"nanos":290},"min":{"secs":0,"nanos":604160},"max":{"secs":0,"nanos":2083072}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":24012},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23552},"max":{"secs":0,"nanos":24576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":108364},"median":{"secs":0,"nanos":109568},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96768},"max":{"secs":0,"nanos":110080}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4240230},"median":{"secs":0,"nanos":4316160},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":4046848},"max":{"secs":0,"nanos":4456448}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4211097},"median":{"secs":0,"nanos":4317440},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":3990528},"max":{"secs":0,"nanos":4470784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4334822},"median":{"secs":0,"nanos":4368896},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":4058112},"max":{"secs":0,"nanos":4598784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3676416},"median":{"secs":0,"nanos":3652608},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":3367168},"max":{"secs":0,"nanos":4134656}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":41088},"median":{"secs":0,"nanos":40704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39936},"max":{"secs":0,"nanos":43008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":119142},"median":{"secs":0,"nanos":117248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":114176},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":216729},"median":{"secs":0,"nanos":197120},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":195328},"max":{"secs":0,"nanos":325120}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":48409},"median":{"secs":0,"nanos":43520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39936},"max":{"secs":0,"nanos":76544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":289484},"median":{"secs":0,"nanos":252416},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":250112},"max":{"secs":0,"nanos":391680}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27904},"median":{"secs":0,"nanos":25856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24320},"max":{"secs":0,"nanos":40192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":66406},"median":{"secs":0,"nanos":55296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":53504},"max":{"secs":0,"nanos":98560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":159564},"median":{"secs":0,"nanos":155904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":153856},"max":{"secs":0,"nanos":194816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":475392},"median":{"secs":0,"nanos":197120},"variance":{"secs":0,"nanos":634},"min":{"secs":0,"nanos":193792},"max":{"secs":0,"nanos":2864896}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3578828},"median":{"secs":0,"nanos":3540992},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":3368960},"max":{"secs":0,"nanos":4009216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":40704},"median":{"secs":0,"nanos":40448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39424},"max":{"secs":0,"nanos":43008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":106188},"median":{"secs":0,"nanos":105984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":105472},"max":{"secs":0,"nanos":109312}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":61491},"median":{"secs":0,"nanos":61440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":60928},"max":{"secs":0,"nanos":62208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":272256},"median":{"secs":0,"nanos":269312},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":215808},"max":{"secs":0,"nanos":400128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":471987},"median":{"secs":0,"nanos":465664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":447744},"max":{"secs":0,"nanos":513536}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17408},"median":{"secs":0,"nanos":16640},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15872},"max":{"secs":0,"nanos":20480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":34585},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":39680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":79001},"median":{"secs":0,"nanos":73216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":61184},"max":{"secs":0,"nanos":137984}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":46720},"median":{"secs":0,"nanos":50688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":36608},"max":{"secs":0,"nanos":52736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":100838},"median":{"secs":0,"nanos":90368},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":82176},"max":{"secs":0,"nanos":138752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":114508},"median":{"secs":0,"nanos":151552},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":156928}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9303628},"median":{"secs":0,"nanos":9185792},"variance":{"secs":0,"nanos":902},"min":{"secs":0,"nanos":7922432},"max":{"secs":0,"nanos":11433728}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":350310},"median":{"secs":0,"nanos":390400},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":306176},"max":{"secs":0,"nanos":398848}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20198},"median":{"secs":0,"nanos":22784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14848},"max":{"secs":0,"nanos":25856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":102630},"median":{"secs":0,"nanos":98816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":98304},"max":{"secs":0,"nanos":111360}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9676},"median":{"secs":0,"nanos":9216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":11520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":25728},"median":{"secs":0,"nanos":25856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":25088},"max":{"secs":0,"nanos":26112}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17587},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16384},"max":{"secs":0,"nanos":19968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":40140},"median":{"secs":0,"nanos":37632},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":37120},"max":{"secs":0,"nanos":48384}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4450124},"median":{"secs":0,"nanos":4514304},"variance":{"secs":0,"nanos":65},"min":{"secs":0,"nanos":4007424},"max":{"secs":0,"nanos":4752384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":186009},"median":{"secs":0,"nanos":190208},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175360},"max":{"secs":0,"nanos":191232}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17382},"median":{"secs":0,"nanos":16896},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":19200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":40192},"median":{"secs":0,"nanos":36608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35072},"max":{"secs":0,"nanos":47104}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":18073},"median":{"secs":0,"nanos":18944},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":19456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":32793},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23296},"max":{"secs":0,"nanos":35072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":72704},"median":{"secs":0,"nanos":46080},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":43776},"max":{"secs":0,"nanos":115712}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8140},"median":{"secs":0,"nanos":7936},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7680},"max":{"secs":0,"nanos":8704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":14694},"median":{"secs":0,"nanos":14592},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":17152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":96998},"median":{"secs":0,"nanos":96768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96000},"max":{"secs":0,"nanos":98048}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":82892},"median":{"secs":0,"nanos":82432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81408},"max":{"secs":0,"nanos":84736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":166067},"median":{"secs":0,"nanos":164864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":164096},"max":{"secs":0,"nanos":173568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":284876},"median":{"secs":0,"nanos":281088},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":275456},"max":{"secs":0,"nanos":324096}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15816780},"median":{"secs":0,"nanos":15639552},"variance":{"secs":0,"nanos":133},"min":{"secs":0,"nanos":15468032},"max":{"secs":0,"nanos":16578816}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":206233},"median":{"secs":0,"nanos":206080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":204800},"max":{"secs":0,"nanos":208384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1272704},"median":{"secs":0,"nanos":1189376},"variance":{"secs":0,"nanos":54},"min":{"secs":0,"nanos":1183744},"max":{"secs":0,"nanos":1971456}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":57728},"median":{"secs":0,"nanos":55296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52736},"max":{"secs":0,"nanos":85760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":79052},"median":{"secs":0,"nanos":69888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":66560},"max":{"secs":0,"nanos":119296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":172390},"median":{"secs":0,"nanos":151296},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":146688},"max":{"secs":0,"nanos":212992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":333312},"median":{"secs":0,"nanos":323584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":322048},"max":{"secs":0,"nanos":385024}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32102},"median":{"secs":0,"nanos":29696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26624},"max":{"secs":0,"nanos":47616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":57420},"median":{"secs":0,"nanos":53760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":51200},"max":{"secs":0,"nanos":89856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":118041},"median":{"secs":0,"nanos":74240},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":71680},"max":{"secs":0,"nanos":226560}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":104345},"median":{"secs":0,"nanos":113408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88064},"max":{"secs":0,"nanos":118784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":212940},"median":{"secs":0,"nanos":183296},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":180480},"max":{"secs":0,"nanos":336128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":215577},"median":{"secs":0,"nanos":292608},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":105984},"max":{"secs":0,"nanos":337664}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9138252},"median":{"secs":0,"nanos":9066752},"variance":{"secs":0,"nanos":369},"min":{"secs":0,"nanos":8475648},"max":{"secs":0,"nanos":10605056}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":752998},"median":{"secs":0,"nanos":719616},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":716288},"max":{"secs":0,"nanos":999424}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1311488},"median":{"secs":0,"nanos":1308928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":1306112},"max":{"secs":0,"nanos":1345024}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9856},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9216},"max":{"secs":0,"nanos":12032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":16614},"median":{"secs":0,"nanos":15872},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15104},"max":{"secs":0,"nanos":25344}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":16947},"median":{"secs":0,"nanos":17152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16128},"max":{"secs":0,"nanos":17408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":37248},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":34048},"max":{"secs":0,"nanos":44544}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2542156},"median":{"secs":0,"nanos":2523648},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":2521856},"max":{"secs":0,"nanos":2711808}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":214912},"median":{"secs":0,"nanos":214784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":214528},"max":{"secs":0,"nanos":216064}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23347},"median":{"secs":0,"nanos":23040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":21504},"max":{"secs":0,"nanos":26112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":41344},"median":{"secs":0,"nanos":41984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":38400},"max":{"secs":0,"nanos":44544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":127155},"median":{"secs":0,"nanos":108032},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":100608},"max":{"secs":0,"nanos":166656}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":167808},"median":{"secs":0,"nanos":167936},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":165376},"max":{"secs":0,"nanos":169728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":255052},"median":{"secs":0,"nanos":169216},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":166912},"max":{"secs":0,"nanos":389376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":364876},"median":{"secs":0,"nanos":416512},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":298240},"max":{"secs":0,"nanos":446208}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":12051020},"median":{"secs":0,"nanos":12047104},"variance":{"secs":0,"nanos":50},"min":{"secs":0,"nanos":11534336},"max":{"secs":0,"nanos":12433408}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":216678},"median":{"secs":0,"nanos":216576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":215552},"max":{"secs":0,"nanos":219136}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1350195},"median":{"secs":0,"nanos":1242880},"variance":{"secs":0,"nanos":81},"min":{"secs":0,"nanos":1241344},"max":{"secs":0,"nanos":2199552}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":10393},"median":{"secs":0,"nanos":10240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":12288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":16409},"median":{"secs":0,"nanos":15616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14848},"max":{"secs":0,"nanos":25088}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":22374},"median":{"secs":0,"nanos":22016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":21504},"max":{"secs":0,"nanos":24320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":35814},"median":{"secs":0,"nanos":33536},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":33024},"max":{"secs":0,"nanos":42240}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4673689},"median":{"secs":0,"nanos":4719360},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":4537088},"max":{"secs":0,"nanos":4898560}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":204416},"median":{"secs":0,"nanos":204288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":203776},"max":{"secs":0,"nanos":205056}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":247859},"median":{"secs":0,"nanos":202496},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":201472},"max":{"secs":0,"nanos":657152}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":21606},"median":{"secs":0,"nanos":21760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":20736},"max":{"secs":0,"nanos":22272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":33152},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32512},"max":{"secs":0,"nanos":33792}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":379801},"median":{"secs":0,"nanos":377856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":376576},"max":{"secs":0,"nanos":393984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":459673},"median":{"secs":0,"nanos":460032},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":457728},"max":{"secs":0,"nanos":461568}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":371225},"median":{"secs":0,"nanos":296704},"variance":{"secs":0,"nanos":51},"min":{"secs":0,"nanos":292864},"max":{"secs":0,"nanos":1051648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":491289},"median":{"secs":0,"nanos":349440},"variance":{"secs":0,"nanos":95},"min":{"secs":0,"nanos":347904},"max":{"secs":0,"nanos":1327872}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":28748},"median":{"secs":0,"nanos":28416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27904},"max":{"secs":0,"nanos":33024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":56627},"median":{"secs":0,"nanos":57088},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":55552},"max":{"secs":0,"nanos":57856}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1943884},"median":{"secs":0,"nanos":1594112},"variance":{"secs":0,"nanos":451},"min":{"secs":0,"nanos":1440768},"max":{"secs":0,"nanos":3222784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2106188},"median":{"secs":0,"nanos":1771520},"variance":{"secs":0,"nanos":340},"min":{"secs":0,"nanos":1713152},"max":{"secs":0,"nanos":3223552}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":16,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"HighlyPermuted","matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":5498828},"median":{"secs":0,"nanos":1632768},"variance":{"secs":0,"nanos":33793},"min":{"secs":0,"nanos":1457664},"max":{"secs":0,"nanos":17440768}}}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":669286},"median":{"secs":0,"nanos":645120},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":632320},"max":{"secs":0,"nanos":893184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":794060},"median":{"secs":0,"nanos":769536},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":766720},"max":{"secs":0,"nanos":960768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1930905},"median":{"secs":0,"nanos":1976064},"variance":{"secs":0,"nanos":172},"min":{"secs":0,"nanos":1574144},"max":{"secs":0,"nanos":2869760}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2500428},"median":{"secs":0,"nanos":2237952},"variance":{"secs":0,"nanos":207},"min":{"secs":0,"nanos":2233600},"max":{"secs":0,"nanos":3501056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2276300},"median":{"secs":0,"nanos":2244096},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":2237440},"max":{"secs":0,"nanos":2577152}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":18585},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16384},"max":{"secs":0,"nanos":25344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":56576},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":55808},"max":{"secs":0,"nanos":58368}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1108121},"median":{"secs":0,"nanos":1018112},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":1014016},"max":{"secs":0,"nanos":1286656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1105356},"median":{"secs":0,"nanos":1089024},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1086720},"max":{"secs":0,"nanos":1245184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6200140},"median":{"secs":0,"nanos":6103296},"variance":{"secs":0,"nanos":204},"min":{"secs":0,"nanos":5737728},"max":{"secs":0,"nanos":7407616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":7210598},"median":{"secs":0,"nanos":7287040},"variance":{"secs":0,"nanos":235},"min":{"secs":0,"nanos":6632192},"max":{"secs":0,"nanos":8420096}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":307635},"median":{"secs":0,"nanos":307712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":306944},"max":{"secs":0,"nanos":309504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":354073},"median":{"secs":0,"nanos":353024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":351744},"max":{"secs":0,"nanos":365312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3393561},"median":{"secs":0,"nanos":3241472},"variance":{"secs":0,"nanos":55},"min":{"secs":0,"nanos":3238144},"max":{"secs":0,"nanos":3901440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5515878},"median":{"secs":0,"nanos":5453568},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":5440768},"max":{"secs":0,"nanos":5636352}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":183116},"median":{"secs":0,"nanos":142592},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":142080},"max":{"secs":0,"nanos":547328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":155084},"median":{"secs":0,"nanos":155136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":154368},"max":{"secs":0,"nanos":155904}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":307020},"median":{"secs":0,"nanos":273664},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":272384},"max":{"secs":0,"nanos":609280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":308633},"median":{"secs":0,"nanos":290816},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":289792},"max":{"secs":0,"nanos":467968}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":560614},"median":{"secs":0,"nanos":539648},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":538368},"max":{"secs":0,"nanos":747520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":780723},"median":{"secs":0,"nanos":670208},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":665856},"max":{"secs":0,"nanos":1268224}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1035315},"median":{"secs":0,"nanos":923136},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":919552},"max":{"secs":0,"nanos":1292032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":968320},"median":{"secs":0,"nanos":952320},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":950528},"max":{"secs":0,"nanos":1113856}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":90905},"median":{"secs":0,"nanos":90880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90112},"max":{"secs":0,"nanos":91648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":105651},"median":{"secs":0,"nanos":105472},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104448},"max":{"secs":0,"nanos":106752}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":256,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":10854},"median":{"secs":0,"nanos":11008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9472},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":15564},"median":{"secs":0,"nanos":13824},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12800},"max":{"secs":0,"nanos":22528}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":512,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":48537},"median":{"secs":0,"nanos":44544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":41984},"max":{"secs":0,"nanos":93696}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":512,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":18636},"median":{"secs":0,"nanos":18432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17920},"max":{"secs":0,"nanos":19712}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":1024,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":33075},"median":{"secs":0,"nanos":26112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":66560}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":1024,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":33331},"median":{"secs":0,"nanos":30464},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":29440},"max":{"secs":0,"nanos":45312}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":363827},"median":{"secs":0,"nanos":364032},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":361728},"max":{"secs":0,"nanos":365568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":538854},"median":{"secs":0,"nanos":474624},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":469248},"max":{"secs":0,"nanos":854784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":634265},"median":{"secs":0,"nanos":602368},"variance":{"secs":0,"nanos":9},"min":{"secs":0,"nanos":601344},"max":{"secs":0,"nanos":920064}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":811750},"median":{"secs":0,"nanos":730112},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":727296},"max":{"secs":0,"nanos":1164032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1165388},"median":{"secs":0,"nanos":957952},"variance":{"secs":0,"nanos":82},"min":{"secs":0,"nanos":950784},"max":{"secs":0,"nanos":1696512}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":923238},"median":{"secs":0,"nanos":898048},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":892160},"max":{"secs":0,"nanos":1156864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1017856},"median":{"secs":0,"nanos":991488},"variance":{"secs":0,"nanos":121},"min":{"secs":0,"nanos":714752},"max":{"secs":0,"nanos":1678336}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":512,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":19200},"median":{"secs":0,"nanos":17920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16640},"max":{"secs":0,"nanos":26880}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3664000},"median":{"secs":0,"nanos":3313152},"variance":{"secs":0,"nanos":217},"min":{"secs":0,"nanos":3309824},"max":{"secs":0,"nanos":4662784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4001766},"median":{"secs":0,"nanos":3877632},"variance":{"secs":0,"nanos":208},"min":{"secs":0,"nanos":3590912},"max":{"secs":0,"nanos":4888832}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":32,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"HighlyPermuted","matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":7604249},"median":{"secs":0,"nanos":7937536},"variance":{"secs":0,"nanos":742},"min":{"secs":0,"nanos":6503680},"max":{"secs":0,"nanos":9001472}}}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}},{"Err":{"Unknown":"RunnerError(InvalidInput(\"Lhs needs to be contiguous, but can't when fusing.\"))"}}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17137792},"median":{"secs":0,"nanos":17041920},"variance":{"secs":0,"nanos":108},"min":{"secs":0,"nanos":16729600},"max":{"secs":0,"nanos":17720576}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":14312960},"median":{"secs":0,"nanos":14342144},"variance":{"secs":0,"nanos":172},"min":{"secs":0,"nanos":13315840},"max":{"secs":0,"nanos":14918144}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":512,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":102886},"median":{"secs":0,"nanos":102912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":101376},"max":{"secs":0,"nanos":105216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":512,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":294937},"median":{"secs":0,"nanos":252160},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":250624},"max":{"secs":0,"nanos":369408}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":512,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":43699},"median":{"secs":0,"nanos":36608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35328},"max":{"secs":0,"nanos":78080}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":106020326},"median":{"secs":0,"nanos":111795712},"variance":{"secs":0,"nanos":85266},"min":{"secs":0,"nanos":92564480},"max":{"secs":0,"nanos":116114176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":49418112},"median":{"secs":0,"nanos":49547008},"variance":{"secs":0,"nanos":557},"min":{"secs":0,"nanos":48515072},"max":{"secs":0,"nanos":51182592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":50331750},"median":{"secs":0,"nanos":50849792},"variance":{"secs":0,"nanos":609},"min":{"secs":0,"nanos":49505280},"max":{"secs":0,"nanos":51761664}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":54823398},"median":{"secs":0,"nanos":54332416},"variance":{"secs":0,"nanos":1423},"min":{"secs":0,"nanos":52959232},"max":{"secs":0,"nanos":56702208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":55476761},"median":{"secs":0,"nanos":55899136},"variance":{"secs":0,"nanos":816},"min":{"secs":0,"nanos":54375168},"max":{"secs":0,"nanos":57258496}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":25267},"median":{"secs":0,"nanos":23808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23040},"max":{"secs":0,"nanos":33280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":45337},"median":{"secs":0,"nanos":45312},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":44032},"max":{"secs":0,"nanos":46848}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":56934},"median":{"secs":0,"nanos":55808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":55296},"max":{"secs":0,"nanos":65792}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":107443},"median":{"secs":0,"nanos":107520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":107008},"max":{"secs":0,"nanos":108032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":119116},"median":{"secs":0,"nanos":119040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":117760},"max":{"secs":0,"nanos":120576}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":472704},"median":{"secs":0,"nanos":472576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":471552},"max":{"secs":0,"nanos":474112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":506265},"median":{"secs":0,"nanos":506112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":503808},"max":{"secs":0,"nanos":513024}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":77900},"median":{"secs":0,"nanos":78080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":76288},"max":{"secs":0,"nanos":79360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":97792},"median":{"secs":0,"nanos":97792},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":97024},"max":{"secs":0,"nanos":98816}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9548},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9216},"max":{"secs":0,"nanos":9984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":13670},"median":{"secs":0,"nanos":13568},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12800},"max":{"secs":0,"nanos":14592}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9958},"median":{"secs":0,"nanos":9984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8192},"max":{"secs":0,"nanos":11520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21145},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":24832}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":57574},"median":{"secs":0,"nanos":61952},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":51968},"max":{"secs":0,"nanos":62976}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":98099},"median":{"secs":0,"nanos":94720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":93184},"max":{"secs":0,"nanos":105216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4046284},"median":{"secs":0,"nanos":4356096},"variance":{"secs":0,"nanos":313},"min":{"secs":0,"nanos":3116288},"max":{"secs":0,"nanos":4534784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3749427},"median":{"secs":0,"nanos":4158464},"variance":{"secs":0,"nanos":476},"min":{"secs":0,"nanos":2990848},"max":{"secs":0,"nanos":4762112}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":179481},"median":{"secs":0,"nanos":52480},"variance":{"secs":0,"nanos":142},"min":{"secs":0,"nanos":51712},"max":{"secs":0,"nanos":1313536}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":102784},"median":{"secs":0,"nanos":102656},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":101888},"max":{"secs":0,"nanos":103936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116121},"median":{"secs":0,"nanos":115968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":113408},"max":{"secs":0,"nanos":119552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":122086},"median":{"secs":0,"nanos":122112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":121088},"max":{"secs":0,"nanos":123136}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2861849},"median":{"secs":0,"nanos":3113472},"variance":{"secs":0,"nanos":856},"min":{"secs":0,"nanos":1855232},"max":{"secs":0,"nanos":4395776}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2159334},"median":{"secs":0,"nanos":1946112},"variance":{"secs":0,"nanos":72},"min":{"secs":0,"nanos":1940992},"max":{"secs":0,"nanos":2589440}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9113},"median":{"secs":0,"nanos":8960},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":9984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":16179},"median":{"secs":0,"nanos":15360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14592},"max":{"secs":0,"nanos":23808}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1460556},"median":{"secs":0,"nanos":1442304},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1441024},"max":{"secs":0,"nanos":1623296}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":56550},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":58624}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":196352},"median":{"secs":0,"nanos":196352},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":195840},"max":{"secs":0,"nanos":197376}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":41881},"median":{"secs":0,"nanos":40960},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":40192},"max":{"secs":0,"nanos":44032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":55424},"median":{"secs":0,"nanos":55296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":54784},"max":{"secs":0,"nanos":56576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":85222},"median":{"secs":0,"nanos":85248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":84224},"max":{"secs":0,"nanos":86016}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1373235},"median":{"secs":0,"nanos":1341952},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":1339904},"max":{"secs":0,"nanos":1514752}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":52275},"median":{"secs":0,"nanos":51712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":51200},"max":{"secs":0,"nanos":57088}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
