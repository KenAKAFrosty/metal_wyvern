{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":765568},"median":{"secs":0,"nanos":615680},"variance":{"secs":0,"nanos":200},"min":{"secs":0,"nanos":613888},"max":{"secs":0,"nanos":2108928}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":30950},"median":{"secs":0,"nanos":31232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28160},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":69401},"median":{"secs":0,"nanos":69376},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":68608},"max":{"secs":0,"nanos":70912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":114944},"median":{"secs":0,"nanos":115712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104192},"max":{"secs":0,"nanos":117248}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17356},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16384},"max":{"secs":0,"nanos":18944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":58803},"median":{"secs":0,"nanos":58624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58112},"max":{"secs":0,"nanos":60672}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":590771},"median":{"secs":0,"nanos":590848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":588800},"max":{"secs":0,"nanos":592384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20608},"median":{"secs":0,"nanos":22528},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16640},"max":{"secs":0,"nanos":23808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":63820},"median":{"secs":0,"nanos":66560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52224},"max":{"secs":0,"nanos":67328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":99481},"median":{"secs":0,"nanos":95744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95488},"max":{"secs":0,"nanos":108288}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":14182},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":16128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":54451},"median":{"secs":0,"nanos":55808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":56320}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":775936},"median":{"secs":0,"nanos":625152},"variance":{"secs":0,"nanos":206},"min":{"secs":0,"nanos":622080},"max":{"secs":0,"nanos":2137344}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32025},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31232},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":113843},"median":{"secs":0,"nanos":113920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":113152},"max":{"secs":0,"nanos":114432}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":619084},"median":{"secs":0,"nanos":619008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":611072},"max":{"secs":0,"nanos":622848}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":32204},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":33024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116326},"median":{"secs":0,"nanos":116224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":115712},"max":{"secs":0,"nanos":116992}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":31488},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30720},"max":{"secs":0,"nanos":32000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":111744},"median":{"secs":0,"nanos":111616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":110848},"max":{"secs":0,"nanos":112384}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":736153},"median":{"secs":0,"nanos":616960},"variance":{"secs":0,"nanos":135},"min":{"secs":0,"nanos":605696},"max":{"secs":0,"nanos":1838592}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":627507},"median":{"secs":0,"nanos":631040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":618496},"max":{"secs":0,"nanos":633344}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":873625},"median":{"secs":0,"nanos":605696},"variance":{"secs":0,"nanos":290},"min":{"secs":0,"nanos":604160},"max":{"secs":0,"nanos":2083072}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":24012},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23552},"max":{"secs":0,"nanos":24576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":108364},"median":{"secs":0,"nanos":109568},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96768},"max":{"secs":0,"nanos":110080}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4240230},"median":{"secs":0,"nanos":4316160},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":4046848},"max":{"secs":0,"nanos":4456448}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":32,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4211097},"median":{"secs":0,"nanos":4317440},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":3990528},"max":{"secs":0,"nanos":4470784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4334822},"median":{"secs":0,"nanos":4368896},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":4058112},"max":{"secs":0,"nanos":4598784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3676416},"median":{"secs":0,"nanos":3652608},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":3367168},"max":{"secs":0,"nanos":4134656}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":41088},"median":{"secs":0,"nanos":40704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39936},"max":{"secs":0,"nanos":43008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":119142},"median":{"secs":0,"nanos":117248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":114176},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":216729},"median":{"secs":0,"nanos":197120},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":195328},"max":{"secs":0,"nanos":325120}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":48409},"median":{"secs":0,"nanos":43520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39936},"max":{"secs":0,"nanos":76544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":289484},"median":{"secs":0,"nanos":252416},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":250112},"max":{"secs":0,"nanos":391680}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":2},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27904},"median":{"secs":0,"nanos":25856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24320},"max":{"secs":0,"nanos":40192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":66406},"median":{"secs":0,"nanos":55296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":53504},"max":{"secs":0,"nanos":98560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":159564},"median":{"secs":0,"nanos":155904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":153856},"max":{"secs":0,"nanos":194816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleVecMat>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":475392},"median":{"secs":0,"nanos":197120},"variance":{"secs":0,"nanos":634},"min":{"secs":0,"nanos":193792},"max":{"secs":0,"nanos":2864896}}}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"num_out_buffers":1,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3578828},"median":{"secs":0,"nanos":3540992},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":3368960},"max":{"secs":0,"nanos":4009216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":1,"num_ops":4},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":40704},"median":{"secs":0,"nanos":40448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":39424},"max":{"secs":0,"nanos":43008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":106188},"median":{"secs":0,"nanos":105984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":105472},"max":{"secs":0,"nanos":109312}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":"Skip"},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"matmul_key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"num_out_buffers":2,"num_ops":8},"checksum":"b460b2b5faab200d678115506a9ba0b1"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fallback<cubecl_wgpu::runtime::WgpuRuntime, u32>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":61491},"median":{"secs":0,"nanos":61440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":60928},"max":{"secs":0,"nanos":62208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::DoubleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":272256},"median":{"secs":0,"nanos":269312},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":215808},"max":{"secs":0,"nanos":400128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl_fusion::matmul::tune::tune_fused<cubecl_wgpu::runtime::WgpuRuntime, u32, burn_cubecl_fusion::matmul::optimization::SimpleUnit>, fn(burn_cubecl_fusion::tune::TuneInput<cubecl_wgpu::runtime::WgpuRuntime, burn_cubecl_fusion::matmul::optimization::MatmulOptimizationTuneArg<cubecl_wgpu::runtime::WgpuRuntime>>) -> core::result::Result<burn_cubecl_fusion::shared::trace::base::TuneOutput<cubecl_wgpu::runtime::WgpuRuntime>, alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":471987},"median":{"secs":0,"nanos":465664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":447744},"max":{"secs":0,"nanos":513536}}}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":{"Unknown":"RunnerError(LaunchError(Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n))"}},{"Err":"Skip"},{"Err":"Skip"}]}}
