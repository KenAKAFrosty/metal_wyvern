{"key":{"key":{"definition":{"m":16384,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":78924},"median":{"secs":0,"nanos":78848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":77824},"max":{"secs":0,"nanos":79872}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84838},"median":{"secs":0,"nanos":83200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81920},"max":{"secs":0,"nanos":89856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":127513},"median":{"secs":0,"nanos":127744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126208},"max":{"secs":0,"nanos":128256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1813376},"median":{"secs":0,"nanos":2008576},"variance":{"secs":0,"nanos":277},"min":{"secs":0,"nanos":776448},"max":{"secs":0,"nanos":2255104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3172121},"median":{"secs":0,"nanos":3086592},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":2596352},"max":{"secs":0,"nanos":4029696}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":47513},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46592},"max":{"secs":0,"nanos":49152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48640},"median":{"secs":0,"nanos":48640},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":87270},"median":{"secs":0,"nanos":87040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":86016},"max":{"secs":0,"nanos":88576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":369971},"median":{"secs":0,"nanos":370176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368896},"max":{"secs":0,"nanos":371456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1455641},"median":{"secs":0,"nanos":1934592},"variance":{"secs":0,"nanos":425},"min":{"secs":0,"nanos":657920},"max":{"secs":0,"nanos":2091264}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241100},"median":{"secs":0,"nanos":241152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":239360},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":258713},"median":{"secs":0,"nanos":258816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":257792},"max":{"secs":0,"nanos":260352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":926848},"median":{"secs":0,"nanos":446976},"variance":{"secs":0,"nanos":566},"min":{"secs":0,"nanos":443904},"max":{"secs":0,"nanos":2368512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":9396428},"median":{"secs":0,"nanos":9568256},"variance":{"secs":0,"nanos":253},"min":{"secs":0,"nanos":8057856},"max":{"secs":0,"nanos":10051072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12117401},"median":{"secs":0,"nanos":12157696},"variance":{"secs":0,"nanos":48},"min":{"secs":0,"nanos":11742720},"max":{"secs":0,"nanos":12550144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":131865},"median":{"secs":0,"nanos":132096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":133888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":157004},"median":{"secs":0,"nanos":156672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":156416},"max":{"secs":0,"nanos":159232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":299315},"median":{"secs":0,"nanos":299264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":298240},"max":{"secs":0,"nanos":300800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4648089},"median":{"secs":0,"nanos":4743168},"variance":{"secs":0,"nanos":330},"min":{"secs":0,"nanos":3098624},"max":{"secs":0,"nanos":5299200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5799168},"median":{"secs":0,"nanos":5455104},"variance":{"secs":0,"nanos":367},"min":{"secs":0,"nanos":5152512},"max":{"secs":0,"nanos":6633984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1475686},"median":{"secs":0,"nanos":1986304},"variance":{"secs":0,"nanos":559},"min":{"secs":0,"nanos":703744},"max":{"secs":0,"nanos":2598400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1749657},"median":{"secs":0,"nanos":2099712},"variance":{"secs":0,"nanos":401},"min":{"secs":0,"nanos":787200},"max":{"secs":0,"nanos":2355456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1802905},"median":{"secs":0,"nanos":2217728},"variance":{"secs":0,"nanos":454},"min":{"secs":0,"nanos":980992},"max":{"secs":0,"nanos":2553344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":30080076},"median":{"secs":0,"nanos":29821184},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":29470208},"max":{"secs":0,"nanos":31312896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":36225868},"median":{"secs":0,"nanos":36247552},"variance":{"secs":0,"nanos":72},"min":{"secs":0,"nanos":35538688},"max":{"secs":0,"nanos":36578304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":589824},"median":{"secs":0,"nanos":445184},"variance":{"secs":0,"nanos":188},"min":{"secs":0,"nanos":442880},"max":{"secs":0,"nanos":1893376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1064960},"median":{"secs":0,"nanos":496640},"variance":{"secs":0,"nanos":504},"min":{"secs":0,"nanos":493568},"max":{"secs":0,"nanos":2270720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1264742},"median":{"secs":0,"nanos":1919232},"variance":{"secs":0,"nanos":545},"min":{"secs":0,"nanos":525312},"max":{"secs":0,"nanos":2094592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":14605619},"median":{"secs":0,"nanos":14573568},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":14064128},"max":{"secs":0,"nanos":15123456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17460556},"median":{"secs":0,"nanos":17382912},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":16971264},"max":{"secs":0,"nanos":18785792}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1322649},"median":{"secs":0,"nanos":1814784},"variance":{"secs":0,"nanos":495},"min":{"secs":0,"nanos":621056},"max":{"secs":0,"nanos":2107136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1333196},"median":{"secs":0,"nanos":1818880},"variance":{"secs":0,"nanos":491},"min":{"secs":0,"nanos":636416},"max":{"secs":0,"nanos":2146304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2547737},"median":{"secs":0,"nanos":2468608},"variance":{"secs":0,"nanos":90},"min":{"secs":0,"nanos":2291200},"max":{"secs":0,"nanos":3327744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27955},"median":{"secs":0,"nanos":28160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":28672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99865},"median":{"secs":0,"nanos":101888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":93696},"max":{"secs":0,"nanos":103168}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":13798},"median":{"secs":0,"nanos":14080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53222},"median":{"secs":0,"nanos":54784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49920},"max":{"secs":0,"nanos":56576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6297},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":7680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11571},"median":{"secs":0,"nanos":12032},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":13824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20326},"median":{"secs":0,"nanos":20736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":22016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":32256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9600},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":11008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":29875},"median":{"secs":0,"nanos":30720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27136},"max":{"secs":0,"nanos":32256}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20070},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18944},"max":{"secs":0,"nanos":21760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":33587},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30208},"max":{"secs":0,"nanos":35328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":54860},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50432},"max":{"secs":0,"nanos":58368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":62438},"median":{"secs":0,"nanos":63232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":57856},"max":{"secs":0,"nanos":65024}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23859},"median":{"secs":0,"nanos":23808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23296},"max":{"secs":0,"nanos":25088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":32588},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":29696},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84659},"median":{"secs":0,"nanos":84480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":83456},"max":{"secs":0,"nanos":87040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":95820},"median":{"secs":0,"nanos":96000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":94976},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":106035},"median":{"secs":0,"nanos":105728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104960},"max":{"secs":0,"nanos":108288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1168512},"median":{"secs":0,"nanos":590592},"variance":{"secs":0,"nanos":513},"min":{"secs":0,"nanos":585472},"max":{"secs":0,"nanos":2264576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1024358},"median":{"secs":0,"nanos":652544},"variance":{"secs":0,"nanos":404},"min":{"secs":0,"nanos":580096},"max":{"secs":0,"nanos":2198784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":33049},"median":{"secs":0,"nanos":33280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":33792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49536},"median":{"secs":0,"nanos":49664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":51456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":60697},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":68608}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3159936},"median":{"secs":0,"nanos":2837504},"variance":{"secs":0,"nanos":549},"min":{"secs":0,"nanos":2586368},"max":{"secs":0,"nanos":4791296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3912883},"median":{"secs":0,"nanos":4368384},"variance":{"secs":0,"nanos":620},"min":{"secs":0,"nanos":2815744},"max":{"secs":0,"nanos":4789504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4364313},"median":{"secs":0,"nanos":4728320},"variance":{"secs":0,"nanos":418},"min":{"secs":0,"nanos":3090688},"max":{"secs":0,"nanos":4894208}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":663577},"median":{"secs":0,"nanos":396800},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":390656},"max":{"secs":0,"nanos":1881088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":477593},"median":{"secs":0,"nanos":477440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":475904},"max":{"secs":0,"nanos":481792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1051648},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":491776},"max":{"secs":0,"nanos":1971712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4388812},"median":{"secs":0,"nanos":4470272},"variance":{"secs":0,"nanos":105},"min":{"secs":0,"nanos":3484416},"max":{"secs":0,"nanos":4717056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4677811},"median":{"secs":0,"nanos":4743936},"variance":{"secs":0,"nanos":87},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5408179},"median":{"secs":0,"nanos":5401088},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":5086720},"max":{"secs":0,"nanos":5629952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":89216230},"median":{"secs":0,"nanos":89440256},"variance":{"secs":0,"nanos":360},"min":{"secs":0,"nanos":88281600},"max":{"secs":0,"nanos":89995776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":90050073},"median":{"secs":0,"nanos":90768640},"variance":{"secs":0,"nanos":3784},"min":{"secs":0,"nanos":85283840},"max":{"secs":0,"nanos":92812544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2846592},"median":{"secs":0,"nanos":2911744},"variance":{"secs":0,"nanos":38},"min":{"secs":0,"nanos":2603776},"max":{"secs":0,"nanos":3213824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3134438},"median":{"secs":0,"nanos":3144704},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":2890240},"max":{"secs":0,"nanos":3370752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4747571},"median":{"secs":0,"nanos":4781824},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":4491008},"max":{"secs":0,"nanos":5148672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12969395},"median":{"secs":0,"nanos":12839424},"variance":{"secs":0,"nanos":482},"min":{"secs":0,"nanos":11920640},"max":{"secs":0,"nanos":14241536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":12890009},"median":{"secs":0,"nanos":12916736},"variance":{"secs":0,"nanos":402},"min":{"secs":0,"nanos":12069888},"max":{"secs":0,"nanos":14033664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":457395},"median":{"secs":0,"nanos":459008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":448256},"max":{"secs":0,"nanos":466432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1108249},"median":{"secs":0,"nanos":567808},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":556032},"max":{"secs":0,"nanos":2232320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1257881},"median":{"secs":0,"nanos":1859072},"variance":{"secs":0,"nanos":500},"min":{"secs":0,"nanos":551168},"max":{"secs":0,"nanos":2175488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":154393},"median":{"secs":0,"nanos":154112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152576},"max":{"secs":0,"nanos":157184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":171008},"median":{"secs":0,"nanos":170752},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":169984},"max":{"secs":0,"nanos":174080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":176844},"median":{"secs":0,"nanos":177152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175104},"max":{"secs":0,"nanos":178944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2747673},"median":{"secs":0,"nanos":2727424},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":2589952},"max":{"secs":0,"nanos":2972928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3355545},"median":{"secs":0,"nanos":3318784},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":3174144},"max":{"secs":0,"nanos":3715584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4769228},"median":{"secs":0,"nanos":4724992},"variance":{"secs":0,"nanos":23},"min":{"secs":0,"nanos":4530176},"max":{"secs":0,"nanos":5030144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":22860083},"median":{"secs":0,"nanos":22610432},"variance":{"secs":0,"nanos":692},"min":{"secs":0,"nanos":22076928},"max":{"secs":0,"nanos":24517888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":23029324},"median":{"secs":0,"nanos":23006208},"variance":{"secs":0,"nanos":351},"min":{"secs":0,"nanos":22123776},"max":{"secs":0,"nanos":23974144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2768409},"median":{"secs":0,"nanos":2835200},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2596096},"max":{"secs":0,"nanos":2918912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2956953},"median":{"secs":0,"nanos":2889984},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":2803456},"max":{"secs":0,"nanos":3351040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3377254},"median":{"secs":0,"nanos":3007232},"variance":{"secs":0,"nanos":408},"min":{"secs":0,"nanos":2758400},"max":{"secs":0,"nanos":4370176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3361459},"median":{"secs":0,"nanos":3155200},"variance":{"secs":0,"nanos":386},"min":{"secs":0,"nanos":2711552},"max":{"secs":0,"nanos":4545024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4098585},"median":{"secs":0,"nanos":4202496},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":3863552},"max":{"secs":0,"nanos":4319232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2723609},"median":{"secs":0,"nanos":2778112},"variance":{"secs":0,"nanos":9},"min":{"secs":0,"nanos":2591744},"max":{"secs":0,"nanos":2860032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2994636},"median":{"secs":0,"nanos":2810368},"variance":{"secs":0,"nanos":242},"min":{"secs":0,"nanos":2573824},"max":{"secs":0,"nanos":4035584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3095347},"median":{"secs":0,"nanos":2846720},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":2555904},"max":{"secs":0,"nanos":4527360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2918297},"median":{"secs":0,"nanos":3033088},"variance":{"secs":0,"nanos":178},"min":{"secs":0,"nanos":1715712},"max":{"secs":0,"nanos":3332352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4619648},"median":{"secs":0,"nanos":4735744},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":4385024},"max":{"secs":0,"nanos":4851712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29465},"median":{"secs":0,"nanos":31744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19968},"max":{"secs":0,"nanos":32256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":35584},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":39680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":48486},"median":{"secs":0,"nanos":50176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":41984},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":151884},"median":{"secs":0,"nanos":151808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":148992},"max":{"secs":0,"nanos":156160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":265267},"median":{"secs":0,"nanos":265984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":262912},"max":{"secs":0,"nanos":267008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":67865},"median":{"secs":0,"nanos":68096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":67072},"max":{"secs":0,"nanos":68608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":85760},"median":{"secs":0,"nanos":86016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":84992},"max":{"secs":0,"nanos":86528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":153395},"median":{"secs":0,"nanos":153600},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152320},"max":{"secs":0,"nanos":154112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1614310},"median":{"secs":0,"nanos":1999104},"variance":{"secs":0,"nanos":523},"min":{"secs":0,"nanos":746496},"max":{"secs":0,"nanos":2496768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2211968},"median":{"secs":0,"nanos":2457856},"variance":{"secs":0,"nanos":411},"min":{"secs":0,"nanos":977152},"max":{"secs":0,"nanos":2748928}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":171699},"median":{"secs":0,"nanos":171520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":170752},"max":{"secs":0,"nanos":173568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":203161},"median":{"secs":0,"nanos":203264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":201984},"max":{"secs":0,"nanos":204032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":265164},"median":{"secs":0,"nanos":265216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":264704},"max":{"secs":0,"nanos":265728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5617945},"median":{"secs":0,"nanos":5809408},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":4139264},"max":{"secs":0,"nanos":6855680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6383744},"median":{"secs":0,"nanos":7002368},"variance":{"secs":0,"nanos":2283},"min":{"secs":0,"nanos":3175936},"max":{"secs":0,"nanos":8116736}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":854041},"median":{"secs":0,"nanos":588032},"variance":{"secs":0,"nanos":272},"min":{"secs":0,"nanos":586240},"max":{"secs":0,"nanos":2317056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1432345},"median":{"secs":0,"nanos":1067520},"variance":{"secs":0,"nanos":481},"min":{"secs":0,"nanos":1064960},"max":{"secs":0,"nanos":2841600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1408563},"median":{"secs":0,"nanos":1902848},"variance":{"secs":0,"nanos":627},"min":{"secs":0,"nanos":624896},"max":{"secs":0,"nanos":2342400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17843},"median":{"secs":0,"nanos":19456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":20736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":96358},"median":{"secs":0,"nanos":99584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90624},"max":{"secs":0,"nanos":99840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":11161},"median":{"secs":0,"nanos":12544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":13312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49433},"median":{"secs":0,"nanos":50688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":52224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4761},"median":{"secs":0,"nanos":4864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":3840},"max":{"secs":0,"nanos":5376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10342},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11392},"median":{"secs":0,"nanos":11520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11008},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":15718},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12032},"max":{"secs":0,"nanos":18176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9779},"median":{"secs":0,"nanos":10240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":10496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16512},"median":{"secs":0,"nanos":16128},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":18688}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":12211},"median":{"secs":0,"nanos":12288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11264},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":16307},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":17920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24755},"median":{"secs":0,"nanos":23296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":28928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":56883},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52992},"max":{"secs":0,"nanos":59392}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15078},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14592},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18764},"median":{"secs":0,"nanos":18432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17152},"max":{"secs":0,"nanos":20480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":81382},"median":{"secs":0,"nanos":81664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":80384},"max":{"secs":0,"nanos":81920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":96102},"median":{"secs":0,"nanos":96256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95232},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":103552},"median":{"secs":0,"nanos":103424},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102912},"max":{"secs":0,"nanos":104192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":230092},"median":{"secs":0,"nanos":230144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":229120},"max":{"secs":0,"nanos":230912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":235468},"median":{"secs":0,"nanos":235520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":234496},"max":{"secs":0,"nanos":236288}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":27008},"median":{"secs":0,"nanos":27136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26112},"max":{"secs":0,"nanos":27648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":35788},"median":{"secs":0,"nanos":35584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35328},"max":{"secs":0,"nanos":36608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48716},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46336},"max":{"secs":0,"nanos":53760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1744870},"median":{"secs":0,"nanos":2263040},"variance":{"secs":0,"nanos":781},"min":{"secs":0,"nanos":699648},"max":{"secs":0,"nanos":2929152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2141081},"median":{"secs":0,"nanos":2369280},"variance":{"secs":0,"nanos":456},"min":{"secs":0,"nanos":866304},"max":{"secs":0,"nanos":2792192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2445542},"median":{"secs":0,"nanos":2424320},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":2179584},"max":{"secs":0,"nanos":2655232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4096,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":287923},"median":{"secs":0,"nanos":287744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":286208},"max":{"secs":0,"nanos":291840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":292531},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288512},"max":{"secs":0,"nanos":293888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":314700},"median":{"secs":0,"nanos":316160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":297728},"max":{"secs":0,"nanos":322048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2858675},"median":{"secs":0,"nanos":2860032},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":2631424},"max":{"secs":0,"nanos":3207936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3077504},"median":{"secs":0,"nanos":3134976},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":2892032},"max":{"secs":0,"nanos":3203840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3393510},"median":{"secs":0,"nanos":3485696},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":3169280},"max":{"secs":0,"nanos":3584768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":46301209},"median":{"secs":0,"nanos":46567680},"variance":{"secs":0,"nanos":917},"min":{"secs":0,"nanos":45009152},"max":{"secs":0,"nanos":47776256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":48641536},"median":{"secs":0,"nanos":49084928},"variance":{"secs":0,"nanos":897},"min":{"secs":0,"nanos":47056896},"max":{"secs":0,"nanos":49919488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1511398},"median":{"secs":0,"nanos":2010112},"variance":{"secs":0,"nanos":498},"min":{"secs":0,"nanos":811264},"max":{"secs":0,"nanos":2388224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1580953},"median":{"secs":0,"nanos":2100736},"variance":{"secs":0,"nanos":484},"min":{"secs":0,"nanos":894208},"max":{"secs":0,"nanos":2541312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2823014},"median":{"secs":0,"nanos":2794240},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":2614272},"max":{"secs":0,"nanos":3104768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7104921},"median":{"secs":0,"nanos":7248640},"variance":{"secs":0,"nanos":527},"min":{"secs":0,"nanos":5942784},"max":{"secs":0,"nanos":8421632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":7807820},"median":{"secs":0,"nanos":7682304},"variance":{"secs":0,"nanos":329},"min":{"secs":0,"nanos":7330048},"max":{"secs":0,"nanos":9185536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241228},"median":{"secs":0,"nanos":242176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":237056},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":284313},"median":{"secs":0,"nanos":284160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":283136},"max":{"secs":0,"nanos":286464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":291609},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":287232},"max":{"secs":0,"nanos":294144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4096,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":123417},"median":{"secs":0,"nanos":123136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":121600},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":131097},"median":{"secs":0,"nanos":130816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":132352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":135321},"median":{"secs":0,"nanos":138240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126976},"max":{"secs":0,"nanos":139776}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1374924},"median":{"secs":0,"nanos":1930240},"variance":{"secs":0,"nanos":447},"min":{"secs":0,"nanos":709888},"max":{"secs":0,"nanos":2182400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1608448},"median":{"secs":0,"nanos":2105088},"variance":{"secs":0,"nanos":487},"min":{"secs":0,"nanos":913408},"max":{"secs":0,"nanos":2474240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3066265},"median":{"secs":0,"nanos":3121664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":2834688},"max":{"secs":0,"nanos":3227904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":11702323},"median":{"secs":0,"nanos":12060928},"variance":{"secs":0,"nanos":740},"min":{"secs":0,"nanos":10090240},"max":{"secs":0,"nanos":12796416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12374656},"median":{"secs":0,"nanos":12242688},"variance":{"secs":0,"nanos":279},"min":{"secs":0,"nanos":11604992},"max":{"secs":0,"nanos":13507584}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1454131},"median":{"secs":0,"nanos":1996544},"variance":{"secs":0,"nanos":430},"min":{"secs":0,"nanos":798208},"max":{"secs":0,"nanos":2208256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1685990},"median":{"secs":0,"nanos":2030848},"variance":{"secs":0,"nanos":497},"min":{"secs":0,"nanos":817920},"max":{"secs":0,"nanos":2525184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1548697},"median":{"secs":0,"nanos":2036480},"variance":{"secs":0,"nanos":489},"min":{"secs":0,"nanos":856064},"max":{"secs":0,"nanos":2447104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2235340},"median":{"secs":0,"nanos":2341376},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":879104},"max":{"secs":0,"nanos":2958848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2489856},"median":{"secs":0,"nanos":2488320},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":2264832},"max":{"secs":0,"nanos":2931456}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1403494},"median":{"secs":0,"nanos":1913600},"variance":{"secs":0,"nanos":493},"min":{"secs":0,"nanos":714496},"max":{"secs":0,"nanos":2437888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1716454},"median":{"secs":0,"nanos":2025472},"variance":{"secs":0,"nanos":460},"min":{"secs":0,"nanos":704000},"max":{"secs":0,"nanos":2404608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1742336},"median":{"secs":0,"nanos":2047488},"variance":{"secs":0,"nanos":520},"min":{"secs":0,"nanos":684544},"max":{"secs":0,"nanos":2651904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1674419},"median":{"secs":0,"nanos":2202112},"variance":{"secs":0,"nanos":669},"min":{"secs":0,"nanos":866560},"max":{"secs":0,"nanos":2809344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3120486},"median":{"secs":0,"nanos":3109120},"variance":{"secs":0,"nanos":63},"min":{"secs":0,"nanos":2810624},"max":{"secs":0,"nanos":3551744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":141696},"median":{"secs":0,"nanos":148224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":149760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":149785},"median":{"secs":0,"nanos":156928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":137984},"max":{"secs":0,"nanos":157696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1397478},"median":{"secs":0,"nanos":1858304},"variance":{"secs":0,"nanos":412},"min":{"secs":0,"nanos":762368},"max":{"secs":0,"nanos":2256640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1442457},"median":{"secs":0,"nanos":1890048},"variance":{"secs":0,"nanos":446},"min":{"secs":0,"nanos":793856},"max":{"secs":0,"nanos":2423808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2390169},"median":{"secs":0,"nanos":2469632},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":2235136},"max":{"secs":0,"nanos":2723328}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4303820},"median":{"secs":0,"nanos":4352768},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":4071936},"max":{"secs":0,"nanos":4536832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4471091},"median":{"secs":0,"nanos":4539904},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":4215040},"max":{"secs":0,"nanos":4889600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6750694},"median":{"secs":0,"nanos":6797312},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":6456576},"max":{"secs":0,"nanos":6977280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":452070},"median":{"secs":0,"nanos":334592},"variance":{"secs":0,"nanos":131},"min":{"secs":0,"nanos":309760},"max":{"secs":0,"nanos":1537280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":530636},"median":{"secs":0,"nanos":394240},"variance":{"secs":0,"nanos":167},"min":{"secs":0,"nanos":393472},"max":{"secs":0,"nanos":1756416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":931174},"median":{"secs":0,"nanos":505856},"variance":{"secs":0,"nanos":352},"min":{"secs":0,"nanos":500992},"max":{"secs":0,"nanos":1889280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6408576},"median":{"secs":0,"nanos":6811136},"variance":{"secs":0,"nanos":486},"min":{"secs":0,"nanos":5229568},"max":{"secs":0,"nanos":7111424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6496025},"median":{"secs":0,"nanos":6904064},"variance":{"secs":0,"nanos":505},"min":{"secs":0,"nanos":5234176},"max":{"secs":0,"nanos":7331840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116812},"median":{"secs":0,"nanos":116736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":114176},"max":{"secs":0,"nanos":120832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":209689},"median":{"secs":0,"nanos":209408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":207872},"max":{"secs":0,"nanos":214016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":222873},"median":{"secs":0,"nanos":223232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":221184},"max":{"secs":0,"nanos":224768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4199270},"median":{"secs":0,"nanos":4310016},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":3977728},"max":{"secs":0,"nanos":4468992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4398438},"median":{"secs":0,"nanos":4438784},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":4174080},"max":{"secs":0,"nanos":4658176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6667136},"median":{"secs":0,"nanos":6691328},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":6347264},"max":{"secs":0,"nanos":7027968}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":308454},"median":{"secs":0,"nanos":303872},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":296192},"max":{"secs":0,"nanos":331520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":507852},"median":{"secs":0,"nanos":388864},"variance":{"secs":0,"nanos":127},"min":{"secs":0,"nanos":387840},"max":{"secs":0,"nanos":1579520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":862924},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":319},"min":{"secs":0,"nanos":492032},"max":{"secs":0,"nanos":1786368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2512588},"median":{"secs":0,"nanos":2562560},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2298368},"max":{"secs":0,"nanos":2671104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2504243},"median":{"secs":0,"nanos":2598912},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":2280704},"max":{"secs":0,"nanos":2753024}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":94284},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88320},"max":{"secs":0,"nanos":109312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":140672},"median":{"secs":0,"nanos":139520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":138752},"max":{"secs":0,"nanos":153344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":144153},"median":{"secs":0,"nanos":143616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":142592},"max":{"secs":0,"nanos":149248}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1093657},"median":{"secs":0,"nanos":742912},"variance":{"secs":0,"nanos":576},"min":{"secs":0,"nanos":686336},"max":{"secs":0,"nanos":3087616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1415398},"median":{"secs":0,"nanos":1443584},"variance":{"secs":0,"nanos":149},"min":{"secs":0,"nanos":911872},"max":{"secs":0,"nanos":2127360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1982233},"median":{"secs":0,"nanos":2361344},"variance":{"secs":0,"nanos":998},"min":{"secs":0,"nanos":629760},"max":{"secs":0,"nanos":3198464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10076851},"median":{"secs":0,"nanos":9892864},"variance":{"secs":0,"nanos":2699},"min":{"secs":0,"nanos":7765760},"max":{"secs":0,"nanos":12916224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":14860697},"median":{"secs":0,"nanos":15601664},"variance":{"secs":0,"nanos":3901},"min":{"secs":0,"nanos":10742272},"max":{"secs":0,"nanos":18316800}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":498227},"median":{"secs":0,"nanos":498176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":497408},"max":{"secs":0,"nanos":499968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1681177},"median":{"secs":0,"nanos":1527040},"variance":{"secs":0,"nanos":113},"min":{"secs":0,"nanos":1404416},"max":{"secs":0,"nanos":2494208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3201024},"median":{"secs":0,"nanos":3132928},"variance":{"secs":0,"nanos":265},"min":{"secs":0,"nanos":2769664},"max":{"secs":0,"nanos":4104704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12033484},"median":{"secs":0,"nanos":12039936},"variance":{"secs":0,"nanos":151},"min":{"secs":0,"nanos":11570944},"max":{"secs":0,"nanos":13067776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21854208},"median":{"secs":0,"nanos":15919360},"variance":{"secs":0,"nanos":181237},"min":{"secs":0,"nanos":8983552},"max":{"secs":0,"nanos":40704512}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1534643},"median":{"secs":0,"nanos":1439488},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":1384448},"max":{"secs":0,"nanos":1799936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1964569},"median":{"secs":0,"nanos":1844480},"variance":{"secs":0,"nanos":45},"min":{"secs":0,"nanos":1843968},"max":{"secs":0,"nanos":2473728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5438182},"median":{"secs":0,"nanos":5225728},"variance":{"secs":0,"nanos":475},"min":{"secs":0,"nanos":4580096},"max":{"secs":0,"nanos":7188224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":35342156},"median":{"secs":0,"nanos":29753600},"variance":{"secs":0,"nanos":294078},"min":{"secs":0,"nanos":27257344},"max":{"secs":0,"nanos":86686208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":34319206},"median":{"secs":0,"nanos":34471424},"variance":{"secs":0,"nanos":236},"min":{"secs":0,"nanos":33640704},"max":{"secs":0,"nanos":35090176}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":17967257},"median":{"secs":0,"nanos":14934784},"variance":{"secs":0,"nanos":26716},"min":{"secs":0,"nanos":12687872},"max":{"secs":0,"nanos":25114368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13458688},"median":{"secs":0,"nanos":15065344},"variance":{"secs":0,"nanos":6803},"min":{"secs":0,"nanos":10482176},"max":{"secs":0,"nanos":16441856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":16014259},"median":{"secs":0,"nanos":16199424},"variance":{"secs":0,"nanos":603},"min":{"secs":0,"nanos":14998528},"max":{"secs":0,"nanos":17384960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":38067},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":36864},"max":{"secs":0,"nanos":40448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":104166},"median":{"secs":0,"nanos":102912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102144},"max":{"secs":0,"nanos":113408}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":39731},"median":{"secs":0,"nanos":29184},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28672},"max":{"secs":0,"nanos":69632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":219136},"median":{"secs":0,"nanos":211712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":198912},"max":{"secs":0,"nanos":240896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":25548},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":37376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":72064},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":54016},"max":{"secs":0,"nanos":104448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":179891},"median":{"secs":0,"nanos":175104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":172032},"max":{"secs":0,"nanos":212736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":433177},"median":{"secs":0,"nanos":299008},"variance":{"secs":0,"nanos":145},"min":{"secs":0,"nanos":294912},"max":{"secs":0,"nanos":1574656}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31001},"median":{"secs":0,"nanos":29696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":48128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":167782},"median":{"secs":0,"nanos":164864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":151040},"max":{"secs":0,"nanos":197376}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":94336},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90880},"max":{"secs":0,"nanos":100352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":198169},"median":{"secs":0,"nanos":187904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":183808},"max":{"secs":0,"nanos":236800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":319001},"median":{"secs":0,"nanos":323840},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":177152},"max":{"secs":0,"nanos":377856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":618905},"median":{"secs":0,"nanos":555520},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":551936},"max":{"secs":0,"nanos":1112064}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":191206},"median":{"secs":0,"nanos":186112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":180224},"max":{"secs":0,"nanos":229376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":340300},"median":{"secs":0,"nanos":319232},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":317440},"max":{"secs":0,"nanos":398592}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1438643},"median":{"secs":0,"nanos":1330176},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":1251840},"max":{"secs":0,"nanos":1670912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5608217},"median":{"secs":0,"nanos":5342720},"variance":{"secs":0,"nanos":2626},"min":{"secs":0,"nanos":3538176},"max":{"secs":0,"nanos":8026368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6190105},"median":{"secs":0,"nanos":6143232},"variance":{"secs":0,"nanos":96},"min":{"secs":0,"nanos":5804544},"max":{"secs":0,"nanos":6816768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6371609},"median":{"secs":0,"nanos":6192640},"variance":{"secs":0,"nanos":3512},"min":{"secs":0,"nanos":4190208},"max":{"secs":0,"nanos":9163264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21803366},"median":{"secs":0,"nanos":6904576},"variance":{"secs":0,"nanos":884883},"min":{"secs":0,"nanos":6030336},"max":{"secs":0,"nanos":94882048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":996121},"median":{"secs":0,"nanos":963584},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":942336},"max":{"secs":0,"nanos":1204736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1830374},"median":{"secs":0,"nanos":1594624},"variance":{"secs":0,"nanos":340},"min":{"secs":0,"nanos":1521664},"max":{"secs":0,"nanos":3479040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2140979},"median":{"secs":0,"nanos":1894656},"variance":{"secs":0,"nanos":393},"min":{"secs":0,"nanos":1652992},"max":{"secs":0,"nanos":3760128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":32768,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3693337},"median":{"secs":0,"nanos":3740928},"variance":{"secs":0,"nanos":67},"min":{"secs":0,"nanos":3436288},"max":{"secs":0,"nanos":4201472}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14573132},"median":{"secs":0,"nanos":16848128},"variance":{"secs":0,"nanos":18786},"min":{"secs":0,"nanos":9231872},"max":{"secs":0,"nanos":19716352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19431116},"median":{"secs":0,"nanos":18908672},"variance":{"secs":0,"nanos":7042},"min":{"secs":0,"nanos":16677888},"max":{"secs":0,"nanos":26026240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3669836},"median":{"secs":0,"nanos":3518464},"variance":{"secs":0,"nanos":609},"min":{"secs":0,"nanos":2843136},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4430361},"median":{"secs":0,"nanos":4444928},"variance":{"secs":0,"nanos":716},"min":{"secs":0,"nanos":3609088},"max":{"secs":0,"nanos":6687232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8967475},"median":{"secs":0,"nanos":9475328},"variance":{"secs":0,"nanos":3323},"min":{"secs":0,"nanos":6718720},"max":{"secs":0,"nanos":11831552}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6429081},"median":{"secs":0,"nanos":6463744},"variance":{"secs":0,"nanos":50},"min":{"secs":0,"nanos":6057472},"max":{"secs":0,"nanos":6874112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6845235},"median":{"secs":0,"nanos":6836224},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":6551040},"max":{"secs":0,"nanos":7118848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8046489},"median":{"secs":0,"nanos":8049664},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":7869952},"max":{"secs":0,"nanos":8253696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":83455462},"median":{"secs":0,"nanos":83596032},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":83105280},"max":{"secs":0,"nanos":83922432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":83787417},"median":{"secs":0,"nanos":83769856},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":83507968},"max":{"secs":0,"nanos":84268800}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3702656},"median":{"secs":0,"nanos":3708928},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":3543552},"max":{"secs":0,"nanos":3963648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":10744550},"median":{"secs":0,"nanos":10586624},"variance":{"secs":0,"nanos":120},"min":{"secs":0,"nanos":10247680},"max":{"secs":0,"nanos":11462400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":11976371},"median":{"secs":0,"nanos":12006912},"variance":{"secs":0,"nanos":185},"min":{"secs":0,"nanos":11264512},"max":{"secs":0,"nanos":12703232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":13132902},"median":{"secs":0,"nanos":13205504},"variance":{"secs":0,"nanos":150},"min":{"secs":0,"nanos":12390912},"max":{"secs":0,"nanos":13904384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18075724},"median":{"secs":0,"nanos":22734336},"variance":{"secs":0,"nanos":50955},"min":{"secs":0,"nanos":7036416},"max":{"secs":0,"nanos":23796480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":32768,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":831539},"median":{"secs":0,"nanos":836608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":814080},"max":{"secs":0,"nanos":845824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1084313},"median":{"secs":0,"nanos":1070336},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1068288},"max":{"secs":0,"nanos":1212672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1091737},"median":{"secs":0,"nanos":1077248},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1075712},"max":{"secs":0,"nanos":1217024}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":254950},"median":{"secs":0,"nanos":254464},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":251648},"max":{"secs":0,"nanos":263680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":307916},"median":{"secs":0,"nanos":307968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":307200},"max":{"secs":0,"nanos":308992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":318438},"median":{"secs":0,"nanos":318720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":316672},"max":{"secs":0,"nanos":320000}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2854348},"median":{"secs":0,"nanos":2741760},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":2739712},"max":{"secs":0,"nanos":3145472}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4107596},"median":{"secs":0,"nanos":4123904},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":4497152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6656665},"median":{"secs":0,"nanos":6635520},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":6530560},"max":{"secs":0,"nanos":6785024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":20676684},"median":{"secs":0,"nanos":20399104},"variance":{"secs":0,"nanos":317},"min":{"secs":0,"nanos":20233728},"max":{"secs":0,"nanos":21930240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20983936},"median":{"secs":0,"nanos":20862720},"variance":{"secs":0,"nanos":297},"min":{"secs":0,"nanos":20465152},"max":{"secs":0,"nanos":21934336}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2853990},"median":{"secs":0,"nanos":2767616},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":2766592},"max":{"secs":0,"nanos":3080960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3054105},"median":{"secs":0,"nanos":2935296},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":2909184},"max":{"secs":0,"nanos":3451648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3178214},"median":{"secs":0,"nanos":3114496},"variance":{"secs":0,"nanos":82},"min":{"secs":0,"nanos":2965760},"max":{"secs":0,"nanos":3795712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3324288},"median":{"secs":0,"nanos":3375872},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":3143424},"max":{"secs":0,"nanos":3499008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5457740},"median":{"secs":0,"nanos":5553664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":5283840},"max":{"secs":0,"nanos":5610240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2936627},"median":{"secs":0,"nanos":2848256},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":2842624},"max":{"secs":0,"nanos":3357696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3069926},"median":{"secs":0,"nanos":2890496},"variance":{"secs":0,"nanos":78},"min":{"secs":0,"nanos":2880512},"max":{"secs":0,"nanos":3554560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2979609},"median":{"secs":0,"nanos":3013888},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":2763776},"max":{"secs":0,"nanos":3126272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20880716},"median":{"secs":0,"nanos":25651968},"variance":{"secs":0,"nanos":75564},"min":{"secs":0,"nanos":7217664},"max":{"secs":0,"nanos":28026112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19703961},"median":{"secs":0,"nanos":26590464},"variance":{"secs":0,"nanos":110438},"min":{"secs":0,"nanos":4252672},"max":{"secs":0,"nanos":29412352}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":546841},"median":{"secs":0,"nanos":546560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":544768},"max":{"secs":0,"nanos":550912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":584857},"median":{"secs":0,"nanos":586240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":569856},"max":{"secs":0,"nanos":594944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2935526},"median":{"secs":0,"nanos":2813184},"variance":{"secs":0,"nanos":85},"min":{"secs":0,"nanos":2670080},"max":{"secs":0,"nanos":3495936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5875507},"median":{"secs":0,"nanos":5905152},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":5494272},"max":{"secs":0,"nanos":6170624}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19583360},"median":{"secs":0,"nanos":32427776},"variance":{"secs":0,"nanos":187140},"min":{"secs":0,"nanos":3954176},"max":{"secs":0,"nanos":33453056}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1592473},"median":{"secs":0,"nanos":1396736},"variance":{"secs":0,"nanos":324},"min":{"secs":0,"nanos":1381120},"max":{"secs":0,"nanos":3299584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5166720},"median":{"secs":0,"nanos":4754432},"variance":{"secs":0,"nanos":1522},"min":{"secs":0,"nanos":3846656},"max":{"secs":0,"nanos":7577856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5461350},"median":{"secs":0,"nanos":5367808},"variance":{"secs":0,"nanos":1295},"min":{"secs":0,"nanos":3847936},"max":{"secs":0,"nanos":7802368}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1848908},"median":{"secs":0,"nanos":1743360},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":1740544},"max":{"secs":0,"nanos":2204160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4710451},"median":{"secs":0,"nanos":4727296},"variance":{"secs":0,"nanos":46},"min":{"secs":0,"nanos":4426240},"max":{"secs":0,"nanos":5119744}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10421862},"median":{"secs":0,"nanos":10817280},"variance":{"secs":0,"nanos":413},"min":{"secs":0,"nanos":9217536},"max":{"secs":0,"nanos":11113984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":12941721},"median":{"secs":0,"nanos":12724736},"variance":{"secs":0,"nanos":1453},"min":{"secs":0,"nanos":11183872},"max":{"secs":0,"nanos":15297792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21671884},"median":{"secs":0,"nanos":23159552},"variance":{"secs":0,"nanos":29648},"min":{"secs":0,"nanos":6394624},"max":{"secs":0,"nanos":27064576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":178508},"median":{"secs":0,"nanos":178432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":176640},"max":{"secs":0,"nanos":181504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":645094},"median":{"secs":0,"nanos":591872},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":565504},"max":{"secs":0,"nanos":997120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":777164},"median":{"secs":0,"nanos":635136},"variance":{"secs":0,"nanos":142},"min":{"secs":0,"nanos":608768},"max":{"secs":0,"nanos":1900544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2245222},"median":{"secs":0,"nanos":1720576},"variance":{"secs":0,"nanos":1017},"min":{"secs":0,"nanos":1384448},"max":{"secs":0,"nanos":4108800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4201753},"median":{"secs":0,"nanos":4007680},"variance":{"secs":0,"nanos":289},"min":{"secs":0,"nanos":3652608},"max":{"secs":0,"nanos":5258752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4434969},"median":{"secs":0,"nanos":4115456},"variance":{"secs":0,"nanos":884},"min":{"secs":0,"nanos":3677440},"max":{"secs":0,"nanos":7000576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6815820},"median":{"secs":0,"nanos":6622976},"variance":{"secs":0,"nanos":225},"min":{"secs":0,"nanos":6160640},"max":{"secs":0,"nanos":7798272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6874035},"median":{"secs":0,"nanos":6903552},"variance":{"secs":0,"nanos":70},"min":{"secs":0,"nanos":6480128},"max":{"secs":0,"nanos":7405568}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4096,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1980774},"median":{"secs":0,"nanos":1670144},"variance":{"secs":0,"nanos":546},"min":{"secs":0,"nanos":1656064},"max":{"secs":0,"nanos":4151808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1922380},"median":{"secs":0,"nanos":1736704},"variance":{"secs":0,"nanos":217},"min":{"secs":0,"nanos":1591808},"max":{"secs":0,"nanos":3210496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2312601},"median":{"secs":0,"nanos":1955072},"variance":{"secs":0,"nanos":780},"min":{"secs":0,"nanos":1547520},"max":{"secs":0,"nanos":3899392}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":4096,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1996953},"median":{"secs":0,"nanos":1980672},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1978368},"max":{"secs":0,"nanos":2142208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3648460},"median":{"secs":0,"nanos":3424000},"variance":{"secs":0,"nanos":129},"min":{"secs":0,"nanos":3340800},"max":{"secs":0,"nanos":4158208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":9007923},"median":{"secs":0,"nanos":9748736},"variance":{"secs":0,"nanos":1579},"min":{"secs":0,"nanos":7247104},"max":{"secs":0,"nanos":10685696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10290124},"median":{"secs":0,"nanos":10428160},"variance":{"secs":0,"nanos":353},"min":{"secs":0,"nanos":8993792},"max":{"secs":0,"nanos":11058688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":17015910},"median":{"secs":0,"nanos":19732480},"variance":{"secs":0,"nanos":57324},"min":{"secs":0,"nanos":2707712},"max":{"secs":0,"nanos":27741440}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4096,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":357145},"median":{"secs":0,"nanos":356864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":354816},"max":{"secs":0,"nanos":360192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":487244},"median":{"secs":0,"nanos":486400},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":483840},"max":{"secs":0,"nanos":494080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":564531},"median":{"secs":0,"nanos":552192},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":548608},"max":{"secs":0,"nanos":673536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4096,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":632320},"median":{"secs":0,"nanos":604672},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":601856},"max":{"secs":0,"nanos":676608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2032332},"median":{"secs":0,"nanos":1042944},"variance":{"secs":0,"nanos":2576},"min":{"secs":0,"nanos":647680},"max":{"secs":0,"nanos":4772864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4475852},"median":{"secs":0,"nanos":4256256},"variance":{"secs":0,"nanos":673},"min":{"secs":0,"nanos":3849472},"max":{"secs":0,"nanos":6507008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5535641},"median":{"secs":0,"nanos":5656320},"variance":{"secs":0,"nanos":592},"min":{"secs":0,"nanos":4835840},"max":{"secs":0,"nanos":7040512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8077900},"median":{"secs":0,"nanos":8142336},"variance":{"secs":0,"nanos":759},"min":{"secs":0,"nanos":6707968},"max":{"secs":0,"nanos":10243072}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":4096,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":309401},"median":{"secs":0,"nanos":287232},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":278528},"max":{"secs":0,"nanos":349952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":321459},"median":{"secs":0,"nanos":364032},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":274432},"max":{"secs":0,"nanos":369920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3916595},"median":{"secs":0,"nanos":3640064},"variance":{"secs":0,"nanos":1871},"min":{"secs":0,"nanos":2874880},"max":{"secs":0,"nanos":7302144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3877196},"median":{"secs":0,"nanos":4154880},"variance":{"secs":0,"nanos":1798},"min":{"secs":0,"nanos":1967360},"max":{"secs":0,"nanos":6480384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":6518016},"median":{"secs":0,"nanos":6501376},"variance":{"secs":0,"nanos":2105},"min":{"secs":0,"nanos":4617472},"max":{"secs":0,"nanos":8945152}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":501017},"median":{"secs":0,"nanos":489984},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":471552},"max":{"secs":0,"nanos":598784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1788902},"median":{"secs":0,"nanos":1031680},"variance":{"secs":0,"nanos":1641},"min":{"secs":0,"nanos":977920},"max":{"secs":0,"nanos":4413696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2206438},"median":{"secs":0,"nanos":1664512},"variance":{"secs":0,"nanos":2587},"min":{"secs":0,"nanos":858880},"max":{"secs":0,"nanos":5134848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6208998},"median":{"secs":0,"nanos":5964800},"variance":{"secs":0,"nanos":1676},"min":{"secs":0,"nanos":4835072},"max":{"secs":0,"nanos":8681984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12472396},"median":{"secs":0,"nanos":12755968},"variance":{"secs":0,"nanos":2335},"min":{"secs":0,"nanos":9508352},"max":{"secs":0,"nanos":14672896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9887436},"median":{"secs":0,"nanos":10119680},"variance":{"secs":0,"nanos":3836},"min":{"secs":0,"nanos":6726400},"max":{"secs":0,"nanos":13103616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":14021836},"median":{"secs":0,"nanos":16350976},"variance":{"secs":0,"nanos":10050},"min":{"secs":0,"nanos":8959488},"max":{"secs":0,"nanos":17525248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":17226572},"median":{"secs":0,"nanos":17220864},"variance":{"secs":0,"nanos":490},"min":{"secs":0,"nanos":15859712},"max":{"secs":0,"nanos":18695168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":55099929},"median":{"secs":0,"nanos":51841280},"variance":{"secs":0,"nanos":35440},"min":{"secs":0,"nanos":49720320},"max":{"secs":0,"nanos":67577344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":55667020},"median":{"secs":0,"nanos":55696896},"variance":{"secs":0,"nanos":1014},"min":{"secs":0,"nanos":54422528},"max":{"secs":0,"nanos":58114048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1594931},"median":{"secs":0,"nanos":1610752},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":1460992},"max":{"secs":0,"nanos":1927168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1933875},"median":{"secs":0,"nanos":1934592},"variance":{"secs":0,"nanos":48},"min":{"secs":0,"nanos":1730816},"max":{"secs":0,"nanos":2314496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4725939},"median":{"secs":0,"nanos":4771328},"variance":{"secs":0,"nanos":358},"min":{"secs":0,"nanos":4123648},"max":{"secs":0,"nanos":5820416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":26991616},"median":{"secs":0,"nanos":26781952},"variance":{"secs":0,"nanos":330},"min":{"secs":0,"nanos":26334720},"max":{"secs":0,"nanos":28366592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":30336179},"median":{"secs":0,"nanos":27179008},"variance":{"secs":0,"nanos":141476},"min":{"secs":0,"nanos":24321536},"max":{"secs":0,"nanos":65809920}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6528},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5888},"max":{"secs":0,"nanos":7424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":19353},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16128},"max":{"secs":0,"nanos":21760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31616},"median":{"secs":0,"nanos":29184},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26368},"max":{"secs":0,"nanos":41984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":90803},"median":{"secs":0,"nanos":85248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81152},"max":{"secs":0,"nanos":118528}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7466035},"median":{"secs":0,"nanos":7106304},"variance":{"secs":0,"nanos":779},"min":{"secs":0,"nanos":6514688},"max":{"secs":0,"nanos":9399552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":6986649},"median":{"secs":0,"nanos":7123456},"variance":{"secs":0,"nanos":1133},"min":{"secs":0,"nanos":4427264},"max":{"secs":0,"nanos":8643840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":7558963},"median":{"secs":0,"nanos":7714304},"variance":{"secs":0,"nanos":1248},"min":{"secs":0,"nanos":5583360},"max":{"secs":0,"nanos":9086464}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":353408},"median":{"secs":0,"nanos":374272},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":299264},"max":{"secs":0,"nanos":513280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1010918},"median":{"secs":0,"nanos":660480},"variance":{"secs":0,"nanos":673},"min":{"secs":0,"nanos":554240},"max":{"secs":0,"nanos":3395840}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":14054},"median":{"secs":0,"nanos":13312},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12544},"max":{"secs":0,"nanos":21248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":98585},"median":{"secs":0,"nanos":97280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96256},"max":{"secs":0,"nanos":107008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6656},"median":{"secs":0,"nanos":6912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":7936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10086},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":32281},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30976},"max":{"secs":0,"nanos":34560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":54553},"median":{"secs":0,"nanos":54272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52480},"max":{"secs":0,"nanos":58880}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8524},"median":{"secs":0,"nanos":8192},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7936},"max":{"secs":0,"nanos":9984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28928},"median":{"secs":0,"nanos":28160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":32256}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":219187},"median":{"secs":0,"nanos":197888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":196864},"max":{"secs":0,"nanos":265984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":347622},"median":{"secs":0,"nanos":324608},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":321792},"max":{"secs":0,"nanos":388864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":416230},"median":{"secs":0,"nanos":344832},"variance":{"secs":0,"nanos":76},"min":{"secs":0,"nanos":291328},"max":{"secs":0,"nanos":1245696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":439833},"median":{"secs":0,"nanos":415744},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":367872},"max":{"secs":0,"nanos":824832}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":94284},"median":{"secs":0,"nanos":70656},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":68864},"max":{"secs":0,"nanos":137728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":186496},"median":{"secs":0,"nanos":176896},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175104},"max":{"secs":0,"nanos":220160}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":8192,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":377472},"median":{"secs":0,"nanos":377344},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":376576},"max":{"secs":0,"nanos":379392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1242726},"median":{"secs":0,"nanos":1003008},"variance":{"secs":0,"nanos":334},"min":{"secs":0,"nanos":994816},"max":{"secs":0,"nanos":2918144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1825433},"median":{"secs":0,"nanos":1972736},"variance":{"secs":0,"nanos":97},"min":{"secs":0,"nanos":1509376},"max":{"secs":0,"nanos":2467328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4146713},"median":{"secs":0,"nanos":2601728},"variance":{"secs":0,"nanos":10055},"min":{"secs":0,"nanos":2375424},"max":{"secs":0,"nanos":11112448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":13558323},"median":{"secs":0,"nanos":13963520},"variance":{"secs":0,"nanos":2354},"min":{"secs":0,"nanos":11239168},"max":{"secs":0,"nanos":15713280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":512,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":99737},"median":{"secs":0,"nanos":99072},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":97792},"max":{"secs":0,"nanos":108032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":938060},"median":{"secs":0,"nanos":828672},"variance":{"secs":0,"nanos":46},"min":{"secs":0,"nanos":814080},"max":{"secs":0,"nanos":1432576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":992742},"median":{"secs":0,"nanos":969984},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":950272},"max":{"secs":0,"nanos":1106944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":32,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":33100},"median":{"secs":0,"nanos":28928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28416},"max":{"secs":0,"nanos":49920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31360},"median":{"secs":0,"nanos":35072},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":41984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":52889},"median":{"secs":0,"nanos":42240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":40960},"max":{"secs":0,"nanos":84992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":105139},"median":{"secs":0,"nanos":97280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95744},"max":{"secs":0,"nanos":129792}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31564},"median":{"secs":0,"nanos":30208},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26368},"max":{"secs":0,"nanos":42240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99712},"median":{"secs":0,"nanos":96768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95488},"max":{"secs":0,"nanos":129536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":4096,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5944064},"median":{"secs":0,"nanos":5840896},"variance":{"secs":0,"nanos":93},"min":{"secs":0,"nanos":5469696},"max":{"secs":0,"nanos":6485248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6603494},"median":{"secs":0,"nanos":6872320},"variance":{"secs":0,"nanos":534},"min":{"secs":0,"nanos":5498624},"max":{"secs":0,"nanos":8165120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":16234009},"median":{"secs":0,"nanos":16456960},"variance":{"secs":0,"nanos":667},"min":{"secs":0,"nanos":14603776},"max":{"secs":0,"nanos":17638400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":8192,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1320601},"median":{"secs":0,"nanos":1246208},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":1210368},"max":{"secs":0,"nanos":1506048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1596083},"median":{"secs":0,"nanos":1565952},"variance":{"secs":0,"nanos":35},"min":{"secs":0,"nanos":1414656},"max":{"secs":0,"nanos":1984000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1805312},"median":{"secs":0,"nanos":1822208},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":1673216},"max":{"secs":0,"nanos":2065920}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10583398},"median":{"secs":0,"nanos":10593024},"variance":{"secs":0,"nanos":64},"min":{"secs":0,"nanos":10210048},"max":{"secs":0,"nanos":11016960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10488243},"median":{"secs":0,"nanos":10629632},"variance":{"secs":0,"nanos":149},"min":{"secs":0,"nanos":9640960},"max":{"secs":0,"nanos":10932992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13635814},"median":{"secs":0,"nanos":13641472},"variance":{"secs":0,"nanos":68},"min":{"secs":0,"nanos":13311488},"max":{"secs":0,"nanos":14107136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":168549657},"median":{"secs":0,"nanos":168432896},"variance":{"secs":0,"nanos":4346},"min":{"secs":0,"nanos":165645312},"max":{"secs":0,"nanos":173519104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":170876134},"median":{"secs":0,"nanos":170621184},"variance":{"secs":0,"nanos":1019},"min":{"secs":0,"nanos":169524224},"max":{"secs":0,"nanos":172238848}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3898854},"median":{"secs":0,"nanos":3859456},"variance":{"secs":0,"nanos":71},"min":{"secs":0,"nanos":3538944},"max":{"secs":0,"nanos":4324096}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":11619686},"median":{"secs":0,"nanos":11814912},"variance":{"secs":0,"nanos":133},"min":{"secs":0,"nanos":10893824},"max":{"secs":0,"nanos":12211712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":13804211},"median":{"secs":0,"nanos":21007360},"variance":{"secs":0,"nanos":77118},"min":{"secs":0,"nanos":3084288},"max":{"secs":0,"nanos":22594304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24392166},"median":{"secs":0,"nanos":24307200},"variance":{"secs":0,"nanos":566},"min":{"secs":0,"nanos":23443456},"max":{"secs":0,"nanos":25782784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":24703692},"median":{"secs":0,"nanos":24860928},"variance":{"secs":0,"nanos":1243},"min":{"secs":0,"nanos":23325696},"max":{"secs":0,"nanos":26831872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":32,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23040},"median":{"secs":0,"nanos":26368},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17408},"max":{"secs":0,"nanos":30464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":73728},"median":{"secs":0,"nanos":70912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":69632},"max":{"secs":0,"nanos":98304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":444108},"median":{"secs":0,"nanos":444416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":441600},"max":{"secs":0,"nanos":447488}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":450534},"median":{"secs":0,"nanos":450816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":446720},"max":{"secs":0,"nanos":452864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1650790},"median":{"secs":0,"nanos":1741824},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":1489920},"max":{"secs":0,"nanos":1839616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3340953},"median":{"secs":0,"nanos":2295808},"variance":{"secs":0,"nanos":3014},"min":{"secs":0,"nanos":1735424},"max":{"secs":0,"nanos":5984512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2999091},"median":{"secs":0,"nanos":2970112},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":2787840},"max":{"secs":0,"nanos":3270912}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4398028},"median":{"secs":0,"nanos":4089600},"variance":{"secs":0,"nanos":168},"min":{"secs":0,"nanos":4083968},"max":{"secs":0,"nanos":5124352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5667865},"median":{"secs":0,"nanos":5726208},"variance":{"secs":0,"nanos":1173},"min":{"secs":0,"nanos":4485888},"max":{"secs":0,"nanos":7638272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10582630},"median":{"secs":0,"nanos":9987840},"variance":{"secs":0,"nanos":2813},"min":{"secs":0,"nanos":8357632},"max":{"secs":0,"nanos":13517568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":13566361},"median":{"secs":0,"nanos":13590016},"variance":{"secs":0,"nanos":256},"min":{"secs":0,"nanos":12969984},"max":{"secs":0,"nanos":14556160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":31290444},"median":{"secs":0,"nanos":32386048},"variance":{"secs":0,"nanos":136350},"min":{"secs":0,"nanos":13623296},"max":{"secs":0,"nanos":54853120}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1060480},"median":{"secs":0,"nanos":1026816},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":1020416},"max":{"secs":0,"nanos":1239296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":12436761},"median":{"secs":0,"nanos":12412416},"variance":{"secs":0,"nanos":15727},"min":{"secs":0,"nanos":6176512},"max":{"secs":0,"nanos":19538688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":12518784},"median":{"secs":0,"nanos":15654912},"variance":{"secs":0,"nanos":13090},"min":{"secs":0,"nanos":7638272},"max":{"secs":0,"nanos":16456960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":8422374},"median":{"secs":0,"nanos":8476416},"variance":{"secs":0,"nanos":795},"min":{"secs":0,"nanos":7084032},"max":{"secs":0,"nanos":10167552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9214899},"median":{"secs":0,"nanos":8867328},"variance":{"secs":0,"nanos":3407},"min":{"secs":0,"nanos":7021056},"max":{"secs":0,"nanos":12228608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":12065228},"median":{"secs":0,"nanos":11575296},"variance":{"secs":0,"nanos":1358},"min":{"secs":0,"nanos":10624512},"max":{"secs":0,"nanos":13895680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17569177},"median":{"secs":0,"nanos":17550592},"variance":{"secs":0,"nanos":515},"min":{"secs":0,"nanos":16826368},"max":{"secs":0,"nanos":19045120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":28223001},"median":{"secs":0,"nanos":20060672},"variance":{"secs":0,"nanos":380845},"min":{"secs":0,"nanos":18967552},"max":{"secs":0,"nanos":83635456}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":283571},"median":{"secs":0,"nanos":280576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":278784},"max":{"secs":0,"nanos":309248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":308531},"median":{"secs":0,"nanos":305920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":304128},"max":{"secs":0,"nanos":332032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1280870},"median":{"secs":0,"nanos":1299200},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":1080320},"max":{"secs":0,"nanos":1415168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1554329},"median":{"secs":0,"nanos":1464064},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":1449728},"max":{"secs":0,"nanos":1927424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10144025},"median":{"secs":0,"nanos":10733056},"variance":{"secs":0,"nanos":1928},"min":{"secs":0,"nanos":7603712},"max":{"secs":0,"nanos":11856128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":733030},"median":{"secs":0,"nanos":704512},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":676096},"max":{"secs":0,"nanos":1043712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1856076},"median":{"secs":0,"nanos":1615616},"variance":{"secs":0,"nanos":167},"min":{"secs":0,"nanos":1518592},"max":{"secs":0,"nanos":2846208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1835392},"median":{"secs":0,"nanos":1686528},"variance":{"secs":0,"nanos":115},"min":{"secs":0,"nanos":1609984},"max":{"secs":0,"nanos":2669568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6193894},"median":{"secs":0,"nanos":6080000},"variance":{"secs":0,"nanos":203},"min":{"secs":0,"nanos":5879552},"max":{"secs":0,"nanos":7262720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":25185689},"median":{"secs":0,"nanos":32146176},"variance":{"secs":0,"nanos":280458},"min":{"secs":0,"nanos":4124160},"max":{"secs":0,"nanos":44143872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":27625369},"median":{"secs":0,"nanos":12870144},"variance":{"secs":0,"nanos":662035},"min":{"secs":0,"nanos":10686976},"max":{"secs":0,"nanos":77155072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14673280},"median":{"secs":0,"nanos":14449152},"variance":{"secs":0,"nanos":233},"min":{"secs":0,"nanos":14258432},"max":{"secs":0,"nanos":15777536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28581862},"median":{"secs":0,"nanos":28272128},"variance":{"secs":0,"nanos":65344},"min":{"secs":0,"nanos":11894528},"max":{"secs":0,"nanos":45217792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":190213785},"median":{"secs":0,"nanos":190499328},"variance":{"secs":0,"nanos":1831},"min":{"secs":0,"nanos":188051456},"max":{"secs":0,"nanos":192256000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":191472409},"median":{"secs":0,"nanos":191872000},"variance":{"secs":0,"nanos":2513},"min":{"secs":0,"nanos":188647680},"max":{"secs":0,"nanos":194107136}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5060556},"median":{"secs":0,"nanos":4890880},"variance":{"secs":0,"nanos":254},"min":{"secs":0,"nanos":4740608},"max":{"secs":0,"nanos":6115840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4969984},"median":{"secs":0,"nanos":4915456},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":4910336},"max":{"secs":0,"nanos":5110272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":7160985},"median":{"secs":0,"nanos":7138560},"variance":{"secs":0,"nanos":59},"min":{"secs":0,"nanos":6947840},"max":{"secs":0,"nanos":7730176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":89947238},"median":{"secs":0,"nanos":89821952},"variance":{"secs":0,"nanos":637},"min":{"secs":0,"nanos":88274688},"max":{"secs":0,"nanos":91324928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":90373708},"median":{"secs":0,"nanos":90107392},"variance":{"secs":0,"nanos":528},"min":{"secs":0,"nanos":89106176},"max":{"secs":0,"nanos":91360768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6348},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5632},"max":{"secs":0,"nanos":8448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8601},"median":{"secs":0,"nanos":8704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7936},"max":{"secs":0,"nanos":9216}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":69683},"median":{"secs":0,"nanos":69120},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":68352},"max":{"secs":0,"nanos":73984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":228582},"median":{"secs":0,"nanos":224768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":222720},"max":{"secs":0,"nanos":239872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4795904},"median":{"secs":0,"nanos":4828160},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":4636160},"max":{"secs":0,"nanos":5012480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":9209548},"median":{"secs":0,"nanos":9075712},"variance":{"secs":0,"nanos":582},"min":{"secs":0,"nanos":8417792},"max":{"secs":0,"nanos":11182336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":20920883},"median":{"secs":0,"nanos":28364800},"variance":{"secs":0,"nanos":129344},"min":{"secs":0,"nanos":3578624},"max":{"secs":0,"nanos":31237376}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":202214},"median":{"secs":0,"nanos":201984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":201216},"max":{"secs":0,"nanos":204288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":373196},"median":{"secs":0,"nanos":372736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":371968},"max":{"secs":0,"nanos":377088}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":177484},"median":{"secs":0,"nanos":177664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":176896},"max":{"secs":0,"nanos":178688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":663936},"median":{"secs":0,"nanos":654848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":652288},"max":{"secs":0,"nanos":752128}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58675},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56576},"max":{"secs":0,"nanos":61184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":70246},"median":{"secs":0,"nanos":66560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":64000},"max":{"secs":0,"nanos":105728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":455961},"median":{"secs":0,"nanos":450304},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":449024},"max":{"secs":0,"nanos":502272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":787123},"median":{"secs":0,"nanos":786688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":783104},"max":{"secs":0,"nanos":795648}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":133529},"median":{"secs":0,"nanos":134144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":132352},"max":{"secs":0,"nanos":134912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":298752},"median":{"secs":0,"nanos":291584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288256},"max":{"secs":0,"nanos":372736}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":996044},"median":{"secs":0,"nanos":994816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":989440},"max":{"secs":0,"nanos":1010176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1883494},"median":{"secs":0,"nanos":1731840},"variance":{"secs":0,"nanos":101},"min":{"secs":0,"nanos":1700352},"max":{"secs":0,"nanos":2742784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3879014},"median":{"secs":0,"nanos":3669504},"variance":{"secs":0,"nanos":291},"min":{"secs":0,"nanos":3352576},"max":{"secs":0,"nanos":4637952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4347084},"median":{"secs":0,"nanos":3688960},"variance":{"secs":0,"nanos":2630},"min":{"secs":0,"nanos":3312384},"max":{"secs":0,"nanos":9013504}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":512,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":393523},"median":{"secs":0,"nanos":383744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368384},"max":{"secs":0,"nanos":467456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":416153},"median":{"secs":0,"nanos":413952},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":382208},"max":{"secs":0,"nanos":498176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":16384,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1368371},"median":{"secs":0,"nanos":1353984},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1352960},"max":{"secs":0,"nanos":1499648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3309875},"median":{"secs":0,"nanos":2069504},"variance":{"secs":0,"nanos":13229},"min":{"secs":0,"nanos":1753600},"max":{"secs":0,"nanos":14150656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":9227801},"median":{"secs":0,"nanos":9332992},"variance":{"secs":0,"nanos":208},"min":{"secs":0,"nanos":8697600},"max":{"secs":0,"nanos":10364416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":9680716},"median":{"secs":0,"nanos":9670400},"variance":{"secs":0,"nanos":77},"min":{"secs":0,"nanos":9298944},"max":{"secs":0,"nanos":10192384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10995430},"median":{"secs":0,"nanos":9881600},"variance":{"secs":0,"nanos":6081},"min":{"secs":0,"nanos":9106944},"max":{"secs":0,"nanos":16126720}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":1024,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":309171},"median":{"secs":0,"nanos":308992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":307200},"max":{"secs":0,"nanos":310784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3570636},"median":{"secs":0,"nanos":3149312},"variance":{"secs":0,"nanos":1393},"min":{"secs":0,"nanos":2813696},"max":{"secs":0,"nanos":6971136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8573132},"median":{"secs":0,"nanos":8125952},"variance":{"secs":0,"nanos":3642},"min":{"secs":0,"nanos":7002624},"max":{"secs":0,"nanos":13848064}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":54732},"median":{"secs":0,"nanos":55040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":53248},"max":{"secs":0,"nanos":56320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":73702},"median":{"secs":0,"nanos":68096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":64768},"max":{"secs":0,"nanos":118784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":116275},"median":{"secs":0,"nanos":110080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":107008},"max":{"secs":0,"nanos":179712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":329164},"median":{"secs":0,"nanos":327680},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":325120},"max":{"secs":0,"nanos":337664}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":81920},"median":{"secs":0,"nanos":81408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":76544},"max":{"secs":0,"nanos":98304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":253619},"median":{"secs":0,"nanos":243200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":240128},"max":{"secs":0,"nanos":282112}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":8192,"n":16384,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":34587340},"median":{"secs":0,"nanos":20269824},"variance":{"secs":0,"nanos":825790},"min":{"secs":0,"nanos":19781632},"max":{"secs":0,"nanos":97835008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":23249817},"median":{"secs":0,"nanos":21177088},"variance":{"secs":0,"nanos":61775},"min":{"secs":0,"nanos":19234304},"max":{"secs":0,"nanos":46723328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":43139430},"median":{"secs":0,"nanos":30478848},"variance":{"secs":0,"nanos":610796},"min":{"secs":0,"nanos":28125696},"max":{"secs":0,"nanos":92635136}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":8192,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":7319731},"median":{"secs":0,"nanos":7243520},"variance":{"secs":0,"nanos":137},"min":{"secs":0,"nanos":7076864},"max":{"secs":0,"nanos":8253440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":26819891},"median":{"secs":0,"nanos":16675328},"variance":{"secs":0,"nanos":572450},"min":{"secs":0,"nanos":5268480},"max":{"secs":0,"nanos":59665664}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20993894},"median":{"secs":0,"nanos":20780800},"variance":{"secs":0,"nanos":1341},"min":{"secs":0,"nanos":19692544},"max":{"secs":0,"nanos":23414784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":33550003},"median":{"secs":0,"nanos":33283328},"variance":{"secs":0,"nanos":255},"min":{"secs":0,"nanos":33079040},"max":{"secs":0,"nanos":34582528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":36257612},"median":{"secs":0,"nanos":36604160},"variance":{"secs":0,"nanos":479},"min":{"secs":0,"nanos":35412992},"max":{"secs":0,"nanos":37237248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":50727347},"median":{"secs":0,"nanos":50851072},"variance":{"secs":0,"nanos":182},"min":{"secs":0,"nanos":50183424},"max":{"secs":0,"nanos":51690240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":642460160},"median":{"secs":0,"nanos":642402560},"variance":{"secs":0,"nanos":592},"min":{"secs":0,"nanos":641153280},"max":{"secs":0,"nanos":643976960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":671708262},"median":{"secs":0,"nanos":662825216},"variance":{"secs":0,"nanos":165563},"min":{"secs":0,"nanos":660317952},"max":{"secs":0,"nanos":695344896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":23449472},"median":{"secs":0,"nanos":27831296},"variance":{"secs":0,"nanos":59200},"min":{"secs":0,"nanos":8209152},"max":{"secs":0,"nanos":29090816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":26344755},"median":{"secs":0,"nanos":35233792},"variance":{"secs":0,"nanos":136625},"min":{"secs":0,"nanos":9345280},"max":{"secs":0,"nanos":36146432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":28113126},"median":{"secs":0,"nanos":36273152},"variance":{"secs":0,"nanos":300842},"min":{"secs":0,"nanos":6608128},"max":{"secs":0,"nanos":60301312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":90515404},"median":{"secs":0,"nanos":90031616},"variance":{"secs":0,"nanos":3509},"min":{"secs":0,"nanos":88845312},"max":{"secs":0,"nanos":95628288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":92449536},"median":{"secs":0,"nanos":92367104},"variance":{"secs":0,"nanos":353},"min":{"secs":0,"nanos":91471616},"max":{"secs":0,"nanos":93382144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":7961},"median":{"secs":0,"nanos":7680},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6912},"max":{"secs":0,"nanos":10752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28851},"median":{"secs":0,"nanos":28672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27904},"max":{"secs":0,"nanos":32768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1539456},"median":{"secs":0,"nanos":1495552},"variance":{"secs":0,"nanos":8},"min":{"secs":0,"nanos":1495040},"max":{"secs":0,"nanos":1766400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1985408},"median":{"secs":0,"nanos":1868800},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":1864448},"max":{"secs":0,"nanos":2261504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3578112},"median":{"secs":0,"nanos":3525120},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":3522816},"max":{"secs":0,"nanos":3709184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5450931},"median":{"secs":0,"nanos":5397248},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":5391104},"max":{"secs":0,"nanos":5612032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5551155},"median":{"secs":0,"nanos":5512960},"variance":{"secs":0,"nanos":90},"min":{"secs":0,"nanos":5335040},"max":{"secs":0,"nanos":6161408}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":860928},"median":{"secs":0,"nanos":809728},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":806144},"max":{"secs":0,"nanos":1004288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":810240},"median":{"secs":0,"nanos":812288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":803584},"max":{"secs":0,"nanos":816384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1420083},"median":{"secs":0,"nanos":1371904},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":1370880},"max":{"secs":0,"nanos":1713664}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1649228},"median":{"secs":0,"nanos":1631744},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1629952},"max":{"secs":0,"nanos":1804032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2674355},"median":{"secs":0,"nanos":2642944},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":2641152},"max":{"secs":0,"nanos":2817280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1522790},"median":{"secs":0,"nanos":1294336},"variance":{"secs":0,"nanos":220},"min":{"secs":0,"nanos":1245952},"max":{"secs":0,"nanos":2800384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1622502},"median":{"secs":0,"nanos":1295104},"variance":{"secs":0,"nanos":295},"min":{"secs":0,"nanos":1208320},"max":{"secs":0,"nanos":2735616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1985920},"median":{"secs":0,"nanos":1490688},"variance":{"secs":0,"nanos":747},"min":{"secs":0,"nanos":1305856},"max":{"secs":0,"nanos":3413248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12579660},"median":{"secs":0,"nanos":12660992},"variance":{"secs":0,"nanos":256},"min":{"secs":0,"nanos":11732736},"max":{"secs":0,"nanos":13472768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":25090841},"median":{"secs":0,"nanos":31987712},"variance":{"secs":0,"nanos":348455},"min":{"secs":0,"nanos":7110912},"max":{"secs":0,"nanos":58775040}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":7341952},"median":{"secs":0,"nanos":7218688},"variance":{"secs":0,"nanos":86},"min":{"secs":0,"nanos":7061248},"max":{"secs":0,"nanos":8057856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":17455104},"median":{"secs":0,"nanos":22214400},"variance":{"secs":0,"nanos":49989},"min":{"secs":0,"nanos":9715456},"max":{"secs":0,"nanos":27356160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19327820},"median":{"secs":0,"nanos":25230848},"variance":{"secs":0,"nanos":124496},"min":{"secs":0,"nanos":5667584},"max":{"secs":0,"nanos":33099520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":100815104},"median":{"secs":0,"nanos":99323904},"variance":{"secs":0,"nanos":12985},"min":{"secs":0,"nanos":98320640},"max":{"secs":0,"nanos":109970432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":108387046},"median":{"secs":0,"nanos":108420096},"variance":{"secs":0,"nanos":601},"min":{"secs":0,"nanos":106416896},"max":{"secs":0,"nanos":109629696}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":19621606},"median":{"secs":0,"nanos":19626496},"variance":{"secs":0,"nanos":197},"min":{"secs":0,"nanos":18626816},"max":{"secs":0,"nanos":20260608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":20664652},"median":{"secs":0,"nanos":20697600},"variance":{"secs":0,"nanos":1314},"min":{"secs":0,"nanos":19457792},"max":{"secs":0,"nanos":22861312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29978137},"median":{"secs":0,"nanos":29906176},"variance":{"secs":0,"nanos":359},"min":{"secs":0,"nanos":29204224},"max":{"secs":0,"nanos":31069184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":395244569},"median":{"secs":0,"nanos":395419136},"variance":{"secs":0,"nanos":1284},"min":{"secs":0,"nanos":393469184},"max":{"secs":0,"nanos":397208320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":396548531},"median":{"secs":0,"nanos":397008640},"variance":{"secs":0,"nanos":1437},"min":{"secs":0,"nanos":394344448},"max":{"secs":0,"nanos":398578944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":154931},"median":{"secs":0,"nanos":21504},"variance":{"secs":0,"nanos":158},"min":{"secs":0,"nanos":20480},"max":{"secs":0,"nanos":1348608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":30899},"median":{"secs":0,"nanos":28416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27648},"max":{"secs":0,"nanos":48128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":84096},"median":{"secs":0,"nanos":75520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":71680},"max":{"secs":0,"nanos":118016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":184217},"median":{"secs":0,"nanos":173824},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":171520},"max":{"secs":0,"nanos":234240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13669478},"median":{"secs":0,"nanos":14001152},"variance":{"secs":0,"nanos":881},"min":{"secs":0,"nanos":11030016},"max":{"secs":0,"nanos":14599680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":15486848},"median":{"secs":0,"nanos":15390976},"variance":{"secs":0,"nanos":306},"min":{"secs":0,"nanos":14690560},"max":{"secs":0,"nanos":16799232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":23064166},"median":{"secs":0,"nanos":20777216},"variance":{"secs":0,"nanos":18901},"min":{"secs":0,"nanos":19209728},"max":{"secs":0,"nanos":30651904}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":701465},"median":{"secs":0,"nanos":700672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":698112},"max":{"secs":0,"nanos":710144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1259084},"median":{"secs":0,"nanos":1213440},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":1208832},"max":{"secs":0,"nanos":1552128}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":183552},"median":{"secs":0,"nanos":182784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":182016},"max":{"secs":0,"nanos":193024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":692326},"median":{"secs":0,"nanos":692480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":690432},"max":{"secs":0,"nanos":694784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58470},"median":{"secs":0,"nanos":43008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":40704},"max":{"secs":0,"nanos":120576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":74675},"median":{"secs":0,"nanos":70144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":67840},"max":{"secs":0,"nanos":117504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1638528},"median":{"secs":0,"nanos":1525504},"variance":{"secs":0,"nanos":101},"min":{"secs":0,"nanos":1520128},"max":{"secs":0,"nanos":2593280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1839206},"median":{"secs":0,"nanos":2096640},"variance":{"secs":0,"nanos":1161},"min":{"secs":0,"nanos":805888},"max":{"secs":0,"nanos":3933184}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":154163},"median":{"secs":0,"nanos":140288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":136960},"max":{"secs":0,"nanos":220928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":247014},"median":{"secs":0,"nanos":246272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":244736},"max":{"secs":0,"nanos":252160}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1207731},"median":{"secs":0,"nanos":667648},"variance":{"secs":0,"nanos":783},"min":{"secs":0,"nanos":645632},"max":{"secs":0,"nanos":3216128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1774976},"median":{"secs":0,"nanos":1542912},"variance":{"secs":0,"nanos":862},"min":{"secs":0,"nanos":999424},"max":{"secs":0,"nanos":4422912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4582016},"median":{"secs":0,"nanos":4194048},"variance":{"secs":0,"nanos":6910},"min":{"secs":0,"nanos":1842944},"max":{"secs":0,"nanos":9022976}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6080588},"median":{"secs":0,"nanos":6535680},"variance":{"secs":0,"nanos":1704},"min":{"secs":0,"nanos":4262400},"max":{"secs":0,"nanos":8520448}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":512,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":652800},"median":{"secs":0,"nanos":655360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":638464},"max":{"secs":0,"nanos":673280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":803737},"median":{"secs":0,"nanos":684032},"variance":{"secs":0,"nanos":125},"min":{"secs":0,"nanos":680704},"max":{"secs":0,"nanos":1865216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":16384,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1397094},"median":{"secs":0,"nanos":1381376},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1380608},"max":{"secs":0,"nanos":1536512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7432908},"median":{"secs":0,"nanos":2596352},"variance":{"secs":0,"nanos":41840},"min":{"secs":0,"nanos":1843968},"max":{"secs":0,"nanos":16595200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":11426380},"median":{"secs":0,"nanos":11325440},"variance":{"secs":0,"nanos":1787},"min":{"secs":0,"nanos":9597440},"max":{"secs":0,"nanos":13233152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17443072},"median":{"secs":0,"nanos":17561600},"variance":{"secs":0,"nanos":630},"min":{"secs":0,"nanos":16227072},"max":{"secs":0,"nanos":18742272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":18718976},"median":{"secs":0,"nanos":19072512},"variance":{"secs":0,"nanos":804},"min":{"secs":0,"nanos":17550336},"max":{"secs":0,"nanos":20130304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":1024,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":530688},"median":{"secs":0,"nanos":530432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":528384},"max":{"secs":0,"nanos":532480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4611968},"median":{"secs":0,"nanos":4585984},"variance":{"secs":0,"nanos":193},"min":{"secs":0,"nanos":4104960},"max":{"secs":0,"nanos":5480960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13194214},"median":{"secs":0,"nanos":12532736},"variance":{"secs":0,"nanos":6031},"min":{"secs":0,"nanos":11334656},"max":{"secs":0,"nanos":20325888}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15206},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14336},"max":{"secs":0,"nanos":16640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":19020},"median":{"secs":0,"nanos":18944},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":20480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":31257},"median":{"secs":0,"nanos":30208},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28928},"max":{"secs":0,"nanos":34816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":59084},"median":{"secs":0,"nanos":58624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58112},"max":{"secs":0,"nanos":64256}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17715},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16896},"max":{"secs":0,"nanos":18944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53017},"median":{"secs":0,"nanos":52992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52480},"max":{"secs":0,"nanos":54016}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":8192,"n":32768,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":39097856},"median":{"secs":0,"nanos":38906624},"variance":{"secs":0,"nanos":1839},"min":{"secs":0,"nanos":37133568},"max":{"secs":0,"nanos":41883136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":43165900},"median":{"secs":0,"nanos":40313088},"variance":{"secs":0,"nanos":142955},"min":{"secs":0,"nanos":36641792},"max":{"secs":0,"nanos":78679552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58958924},"median":{"secs":0,"nanos":59155968},"variance":{"secs":0,"nanos":4539},"min":{"secs":0,"nanos":56209408},"max":{"secs":0,"nanos":62394880}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":16384,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9405363},"median":{"secs":0,"nanos":9394176},"variance":{"secs":0,"nanos":140},"min":{"secs":0,"nanos":8809984},"max":{"secs":0,"nanos":10365696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":25077350},"median":{"secs":0,"nanos":11303680},"variance":{"secs":0,"nanos":868540},"min":{"secs":0,"nanos":9610752},"max":{"secs":0,"nanos":95993344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13433548},"median":{"secs":0,"nanos":13357312},"variance":{"secs":0,"nanos":56},"min":{"secs":0,"nanos":13174016},"max":{"secs":0,"nanos":14104064}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":66881203},"median":{"secs":0,"nanos":67029504},"variance":{"secs":0,"nanos":183},"min":{"secs":0,"nanos":66223360},"max":{"secs":0,"nanos":67458048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":75219686},"median":{"secs":0,"nanos":75282944},"variance":{"secs":0,"nanos":93},"min":{"secs":0,"nanos":74766848},"max":{"secs":0,"nanos":75861248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":97410483},"median":{"secs":0,"nanos":97434112},"variance":{"secs":0,"nanos":599},"min":{"secs":0,"nanos":96761856},"max":{"secs":0,"nanos":99592448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":1,"nanos":296542310},"median":{"secs":1,"nanos":292398336},"variance":{"secs":0,"nanos":120246},"min":{"secs":1,"nanos":290953216},"max":{"secs":1,"nanos":328642816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":1,"nanos":303565132},"median":{"secs":1,"nanos":303598336},"variance":{"secs":0,"nanos":573},"min":{"secs":1,"nanos":302324480},"max":{"secs":1,"nanos":304818432}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":11762790},"median":{"secs":0,"nanos":11805184},"variance":{"secs":0,"nanos":93},"min":{"secs":0,"nanos":11357440},"max":{"secs":0,"nanos":12442368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":23266355},"median":{"secs":0,"nanos":14855424},"variance":{"secs":0,"nanos":181361},"min":{"secs":0,"nanos":14028544},"max":{"secs":0,"nanos":47611136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":15841868},"median":{"secs":0,"nanos":15803392},"variance":{"secs":0,"nanos":44},"min":{"secs":0,"nanos":15595264},"max":{"secs":0,"nanos":16211968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":182131507},"median":{"secs":0,"nanos":182181888},"variance":{"secs":0,"nanos":228},"min":{"secs":0,"nanos":181442304},"max":{"secs":0,"nanos":182945280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":183227366},"median":{"secs":0,"nanos":183218944},"variance":{"secs":0,"nanos":103},"min":{"secs":0,"nanos":182721792},"max":{"secs":0,"nanos":183704320}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32768,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10075468},"median":{"secs":0,"nanos":9958144},"variance":{"secs":0,"nanos":275},"min":{"secs":0,"nanos":9626624},"max":{"secs":0,"nanos":11315200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11126912},"median":{"secs":0,"nanos":10993152},"variance":{"secs":0,"nanos":129},"min":{"secs":0,"nanos":10815488},"max":{"secs":0,"nanos":12051712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14550451},"median":{"secs":0,"nanos":14388480},"variance":{"secs":0,"nanos":176},"min":{"secs":0,"nanos":14241280},"max":{"secs":0,"nanos":15465216}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2489548},"median":{"secs":0,"nanos":2359040},"variance":{"secs":0,"nanos":59},"min":{"secs":0,"nanos":2340096},"max":{"secs":0,"nanos":3044352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2969958},"median":{"secs":0,"nanos":2830080},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":2819584},"max":{"secs":0,"nanos":3471616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3371084},"median":{"secs":0,"nanos":3339008},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":3337728},"max":{"secs":0,"nanos":3503616}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20118809},"median":{"secs":0,"nanos":20053248},"variance":{"secs":0,"nanos":76},"min":{"secs":0,"nanos":19879936},"max":{"secs":0,"nanos":20648448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21586304},"median":{"secs":0,"nanos":21416448},"variance":{"secs":0,"nanos":903},"min":{"secs":0,"nanos":20334592},"max":{"secs":0,"nanos":23608832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":27420646},"median":{"secs":0,"nanos":27356672},"variance":{"secs":0,"nanos":202},"min":{"secs":0,"nanos":26802176},"max":{"secs":0,"nanos":28204544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":333673958},"median":{"secs":0,"nanos":330093568},"variance":{"secs":0,"nanos":47551},"min":{"secs":0,"nanos":326282240},"max":{"secs":0,"nanos":344820736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":331410483},"median":{"secs":0,"nanos":330222592},"variance":{"secs":0,"nanos":17040},"min":{"secs":0,"nanos":327560960},"max":{"secs":0,"nanos":341102336}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4566348},"median":{"secs":0,"nanos":4609024},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":4465664},"max":{"secs":0,"nanos":4799232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5213209},"median":{"secs":0,"nanos":5165568},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":5163520},"max":{"secs":0,"nanos":5337344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6680448},"median":{"secs":0,"nanos":6671872},"variance":{"secs":0,"nanos":26},"min":{"secs":0,"nanos":6521600},"max":{"secs":0,"nanos":7029248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":45682252},"median":{"secs":0,"nanos":45860352},"variance":{"secs":0,"nanos":437},"min":{"secs":0,"nanos":44809728},"max":{"secs":0,"nanos":46572032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":45648768},"median":{"secs":0,"nanos":45882368},"variance":{"secs":0,"nanos":179},"min":{"secs":0,"nanos":45052160},"max":{"secs":0,"nanos":46173952}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8345},"median":{"secs":0,"nanos":8448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7680},"max":{"secs":0,"nanos":8960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":47590},"median":{"secs":0,"nanos":47616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":47104},"max":{"secs":0,"nanos":48128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2776473},"median":{"secs":0,"nanos":2758656},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":2758144},"max":{"secs":0,"nanos":2931968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3639091},"median":{"secs":0,"nanos":3683584},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":3510528},"max":{"secs":0,"nanos":3893248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6614041},"median":{"secs":0,"nanos":6554624},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":6548992},"max":{"secs":0,"nanos":6723584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":10112537},"median":{"secs":0,"nanos":10032640},"variance":{"secs":0,"nanos":144},"min":{"secs":0,"nanos":9854464},"max":{"secs":0,"nanos":11218432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10229120},"median":{"secs":0,"nanos":10173440},"variance":{"secs":0,"nanos":65},"min":{"secs":0,"nanos":9989120},"max":{"secs":0,"nanos":10927104}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1692595},"median":{"secs":0,"nanos":1714432},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":1582592},"max":{"secs":0,"nanos":1856000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1779942},"median":{"secs":0,"nanos":1766656},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":1749760},"max":{"secs":0,"nanos":1945856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2856550},"median":{"secs":0,"nanos":2886144},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":2735616},"max":{"secs":0,"nanos":3049216}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5847910},"median":{"secs":0,"nanos":5871616},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":5700608},"max":{"secs":0,"nanos":6102528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21212211},"median":{"secs":0,"nanos":32975872},"variance":{"secs":0,"nanos":192801},"min":{"secs":0,"nanos":3871232},"max":{"secs":0,"nanos":33976832}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":16384,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":24622643},"median":{"secs":0,"nanos":10195712},"variance":{"secs":0,"nanos":515644},"min":{"secs":0,"nanos":8726784},"max":{"secs":0,"nanos":60785408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":11624780},"median":{"secs":0,"nanos":10851072},"variance":{"secs":0,"nanos":2401},"min":{"secs":0,"nanos":10371584},"max":{"secs":0,"nanos":15446016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29901004},"median":{"secs":0,"nanos":13478400},"variance":{"secs":0,"nanos":739985},"min":{"secs":0,"nanos":11907328},"max":{"secs":0,"nanos":80936704}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":16384,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":18542848},"median":{"secs":0,"nanos":12204800},"variance":{"secs":0,"nanos":167780},"min":{"secs":0,"nanos":10798592},"max":{"secs":0,"nanos":45121792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":30053376},"median":{"secs":0,"nanos":15074560},"variance":{"secs":0,"nanos":969154},"min":{"secs":0,"nanos":14083584},"max":{"secs":0,"nanos":106968832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":31793331},"median":{"secs":0,"nanos":47440896},"variance":{"secs":0,"nanos":386624},"min":{"secs":0,"nanos":12194304},"max":{"secs":0,"nanos":62134272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":196130995},"median":{"secs":0,"nanos":196281856},"variance":{"secs":0,"nanos":289},"min":{"secs":0,"nanos":195196160},"max":{"secs":0,"nanos":196919808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":214548736},"median":{"secs":0,"nanos":214851072},"variance":{"secs":0,"nanos":157},"min":{"secs":0,"nanos":213975552},"max":{"secs":0,"nanos":215061504}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2714393},"median":{"secs":0,"nanos":2695424},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":2688256},"max":{"secs":0,"nanos":2894336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2805248},"median":{"secs":0,"nanos":2764800},"variance":{"secs":0,"nanos":14},"min":{"secs":0,"nanos":2699776},"max":{"secs":0,"nanos":3042560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3033574},"median":{"secs":0,"nanos":2995712},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":2993408},"max":{"secs":0,"nanos":3192320}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":16384,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4451968},"median":{"secs":0,"nanos":4403712},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":4397568},"max":{"secs":0,"nanos":4577024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4612761},"median":{"secs":0,"nanos":4551680},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":4539136},"max":{"secs":0,"nanos":4724480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16014924},"median":{"secs":0,"nanos":18845696},"variance":{"secs":0,"nanos":15662},"min":{"secs":0,"nanos":9970432},"max":{"secs":0,"nanos":19577088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":49163136},"median":{"secs":0,"nanos":49278976},"variance":{"secs":0,"nanos":193},"min":{"secs":0,"nanos":48403968},"max":{"secs":0,"nanos":49881856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":56861644},"median":{"secs":0,"nanos":54326272},"variance":{"secs":0,"nanos":78724},"min":{"secs":0,"nanos":52920320},"max":{"secs":0,"nanos":83409664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":16384,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2278988},"median":{"secs":0,"nanos":2172928},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":2157568},"max":{"secs":0,"nanos":2714880}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2936320},"median":{"secs":0,"nanos":2573056},"variance":{"secs":0,"nanos":952},"min":{"secs":0,"nanos":2360576},"max":{"secs":0,"nanos":5687040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11628364},"median":{"secs":0,"nanos":11469824},"variance":{"secs":0,"nanos":448},"min":{"secs":0,"nanos":10952192},"max":{"secs":0,"nanos":13177088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":17520051},"median":{"secs":0,"nanos":14042624},"variance":{"secs":0,"nanos":28176},"min":{"secs":0,"nanos":13026560},"max":{"secs":0,"nanos":26324992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":24979353},"median":{"secs":0,"nanos":22965248},"variance":{"secs":0,"nanos":17990},"min":{"secs":0,"nanos":21043456},"max":{"secs":0,"nanos":31339008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":65536,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2302003},"median":{"secs":0,"nanos":2252544},"variance":{"secs":0,"nanos":79},"min":{"secs":0,"nanos":2094592},"max":{"secs":0,"nanos":3079680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3843200},"median":{"secs":0,"nanos":3458560},"variance":{"secs":0,"nanos":1117},"min":{"secs":0,"nanos":2763264},"max":{"secs":0,"nanos":6471168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3751142},"median":{"secs":0,"nanos":3730432},"variance":{"secs":0,"nanos":1361},"min":{"secs":0,"nanos":2666496},"max":{"secs":0,"nanos":6674688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24066867},"median":{"secs":0,"nanos":24174592},"variance":{"secs":0,"nanos":1158},"min":{"secs":0,"nanos":22679040},"max":{"secs":0,"nanos":25616640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":34274380},"median":{"secs":0,"nanos":43862528},"variance":{"secs":0,"nanos":393521},"min":{"secs":0,"nanos":14407680},"max":{"secs":0,"nanos":76725504}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":65536,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10263910},"median":{"secs":0,"nanos":10360576},"variance":{"secs":0,"nanos":175},"min":{"secs":0,"nanos":9667584},"max":{"secs":0,"nanos":11070464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":14121190},"median":{"secs":0,"nanos":11678976},"variance":{"secs":0,"nanos":57647},"min":{"secs":0,"nanos":10642176},"max":{"secs":0,"nanos":36846848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":15536000},"median":{"secs":0,"nanos":15544320},"variance":{"secs":0,"nanos":44},"min":{"secs":0,"nanos":15237120},"max":{"secs":0,"nanos":16051200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":195724032},"median":{"secs":0,"nanos":197421568},"variance":{"secs":0,"nanos":34847},"min":{"secs":0,"nanos":187659008},"max":{"secs":0,"nanos":202208512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":217635328},"median":{"secs":0,"nanos":217645312},"variance":{"secs":0,"nanos":581},"min":{"secs":0,"nanos":216596992},"max":{"secs":0,"nanos":218966784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":65536,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":36825753},"median":{"secs":0,"nanos":36587008},"variance":{"secs":0,"nanos":222},"min":{"secs":0,"nanos":36410112},"max":{"secs":0,"nanos":37509632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":40066022},"median":{"secs":0,"nanos":40570368},"variance":{"secs":0,"nanos":1568},"min":{"secs":0,"nanos":38662656},"max":{"secs":0,"nanos":42457600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":57501696},"median":{"secs":0,"nanos":57651712},"variance":{"secs":0,"nanos":219},"min":{"secs":0,"nanos":56863488},"max":{"secs":0,"nanos":58412800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":763701299},"median":{"secs":0,"nanos":762226432},"variance":{"secs":0,"nanos":27745},"min":{"secs":0,"nanos":759040768},"max":{"secs":0,"nanos":777991424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":775309260},"median":{"secs":0,"nanos":775662848},"variance":{"secs":0,"nanos":8530},"min":{"secs":0,"nanos":771348480},"max":{"secs":0,"nanos":780943360}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":19148},"median":{"secs":0,"nanos":19200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18688},"max":{"secs":0,"nanos":19712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":21145},"median":{"secs":0,"nanos":20992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":20480},"max":{"secs":0,"nanos":23040}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":155392},"median":{"secs":0,"nanos":153088},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":149760},"max":{"secs":0,"nanos":166912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":252160},"median":{"secs":0,"nanos":251904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":250112},"max":{"secs":0,"nanos":256512}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5091686},"median":{"secs":0,"nanos":5069312},"variance":{"secs":0,"nanos":50},"min":{"secs":0,"nanos":4834816},"max":{"secs":0,"nanos":5573632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":15749376},"median":{"secs":0,"nanos":18061312},"variance":{"secs":0,"nanos":12172},"min":{"secs":0,"nanos":9961984},"max":{"secs":0,"nanos":18496768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":23465011},"median":{"secs":0,"nanos":28047872},"variance":{"secs":0,"nanos":94204},"min":{"secs":0,"nanos":5083904},"max":{"secs":0,"nanos":33506048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":211430},"median":{"secs":0,"nanos":211456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":210432},"max":{"secs":0,"nanos":213760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":389043},"median":{"secs":0,"nanos":389120},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":388096},"max":{"secs":0,"nanos":389632}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":181939},"median":{"secs":0,"nanos":182016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":181248},"max":{"secs":0,"nanos":182784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1254579},"median":{"secs":0,"nanos":815360},"variance":{"secs":0,"nanos":716},"min":{"secs":0,"nanos":652288},"max":{"secs":0,"nanos":2613760}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":70348},"median":{"secs":0,"nanos":67840},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":66560},"max":{"secs":0,"nanos":86784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":80512},"median":{"secs":0,"nanos":74240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":73728},"max":{"secs":0,"nanos":127232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1925555},"median":{"secs":0,"nanos":1720320},"variance":{"secs":0,"nanos":175},"min":{"secs":0,"nanos":1717248},"max":{"secs":0,"nanos":2912256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3703040},"median":{"secs":0,"nanos":3053056},"variance":{"secs":0,"nanos":2593},"min":{"secs":0,"nanos":3036928},"max":{"secs":0,"nanos":8421376}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":457036},"median":{"secs":0,"nanos":454144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":453376},"max":{"secs":0,"nanos":481792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1140940},"median":{"secs":0,"nanos":1017088},"variance":{"secs":0,"nanos":98},"min":{"secs":0,"nanos":1015552},"max":{"secs":0,"nanos":2078208}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1007308},"median":{"secs":0,"nanos":978432},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":974080},"max":{"secs":0,"nanos":1186560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1955737},"median":{"secs":0,"nanos":1821952},"variance":{"secs":0,"nanos":126},"min":{"secs":0,"nanos":1796096},"max":{"secs":0,"nanos":3006976}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7800396},"median":{"secs":0,"nanos":7864320},"variance":{"secs":0,"nanos":1499},"min":{"secs":0,"nanos":6771712},"max":{"secs":0,"nanos":11117568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":13622809},"median":{"secs":0,"nanos":13362432},"variance":{"secs":0,"nanos":2177},"min":{"secs":0,"nanos":12054016},"max":{"secs":0,"nanos":17866752}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":660608},"median":{"secs":0,"nanos":568320},"variance":{"secs":0,"nanos":78},"min":{"secs":0,"nanos":554240},"max":{"secs":0,"nanos":1500160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2113382},"median":{"secs":0,"nanos":2508544},"variance":{"secs":0,"nanos":702},"min":{"secs":0,"nanos":1300224},"max":{"secs":0,"nanos":3218944}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":16384,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2609664},"median":{"secs":0,"nanos":2589952},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":2588416},"max":{"secs":0,"nanos":2789376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3724364},"median":{"secs":0,"nanos":3815424},"variance":{"secs":0,"nanos":94},"min":{"secs":0,"nanos":3409920},"max":{"secs":0,"nanos":4425984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5877990},"median":{"secs":0,"nanos":5948672},"variance":{"secs":0,"nanos":246},"min":{"secs":0,"nanos":5168384},"max":{"secs":0,"nanos":6910464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":32705049},"median":{"secs":0,"nanos":32809984},"variance":{"secs":0,"nanos":345},"min":{"secs":0,"nanos":31914496},"max":{"secs":0,"nanos":33743104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":33281484},"median":{"secs":0,"nanos":33365248},"variance":{"secs":0,"nanos":1492},"min":{"secs":0,"nanos":32194048},"max":{"secs":0,"nanos":36209664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":990950},"median":{"secs":0,"nanos":990720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":988672},"max":{"secs":0,"nanos":993536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7746099},"median":{"secs":0,"nanos":6530816},"variance":{"secs":0,"nanos":2400},"min":{"secs":0,"nanos":6475008},"max":{"secs":0,"nanos":10060288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":24751846},"median":{"secs":0,"nanos":25728512},"variance":{"secs":0,"nanos":21631},"min":{"secs":0,"nanos":19790592},"max":{"secs":0,"nanos":34345984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20147},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19712},"max":{"secs":0,"nanos":21504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":32281},"median":{"secs":0,"nanos":32512},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":32512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":53555},"median":{"secs":0,"nanos":53760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52992},"max":{"secs":0,"nanos":54016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":58726},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58368},"max":{"secs":0,"nanos":59136}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":30336},"median":{"secs":0,"nanos":29952},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":29696},"max":{"secs":0,"nanos":33280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99532},"median":{"secs":0,"nanos":99584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":98816},"max":{"secs":0,"nanos":100608}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":8192,"n":65536,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":75147084},"median":{"secs":0,"nanos":74754560},"variance":{"secs":0,"nanos":2244},"min":{"secs":0,"nanos":73533440},"max":{"secs":0,"nanos":78861312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":77232230},"median":{"secs":0,"nanos":77945344},"variance":{"secs":0,"nanos":4116},"min":{"secs":0,"nanos":74855936},"max":{"secs":0,"nanos":80513792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":119995673},"median":{"secs":0,"nanos":120341248},"variance":{"secs":0,"nanos":3534},"min":{"secs":0,"nanos":115180544},"max":{"secs":0,"nanos":122285824}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32768,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":19120051},"median":{"secs":0,"nanos":18932992},"variance":{"secs":0,"nanos":267},"min":{"secs":0,"nanos":18673664},"max":{"secs":0,"nanos":20447488}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21101644},"median":{"secs":0,"nanos":20842240},"variance":{"secs":0,"nanos":1110},"min":{"secs":0,"nanos":19567104},"max":{"secs":0,"nanos":23176192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":38809881},"median":{"secs":0,"nanos":41135616},"variance":{"secs":0,"nanos":133021},"min":{"secs":0,"nanos":28336384},"max":{"secs":0,"nanos":67972096}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":32768,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":134706611},"median":{"secs":0,"nanos":134307072},"variance":{"secs":0,"nanos":1620},"min":{"secs":0,"nanos":133505536},"max":{"secs":0,"nanos":137809152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":153446118},"median":{"secs":0,"nanos":153436416},"variance":{"secs":0,"nanos":507},"min":{"secs":0,"nanos":152008704},"max":{"secs":0,"nanos":154812672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":195839564},"median":{"secs":0,"nanos":195966720},"variance":{"secs":0,"nanos":651},"min":{"secs":0,"nanos":194478592},"max":{"secs":0,"nanos":197017856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":2,"nanos":591128908},"median":{"secs":2,"nanos":584579072},"variance":{"secs":0,"nanos":536935},"min":{"secs":2,"nanos":562596096},"max":{"secs":2,"nanos":623904512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":2,"nanos":755972070},"median":{"secs":2,"nanos":818241536},"variance":{"secs":0,"nanos":10718629},"min":{"secs":2,"nanos":603418624},"max":{"secs":2,"nanos":902828544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":32768,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":24206336},"median":{"secs":0,"nanos":24171520},"variance":{"secs":0,"nanos":57},"min":{"secs":0,"nanos":23859712},"max":{"secs":0,"nanos":24694016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":26875673},"median":{"secs":0,"nanos":26939904},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":26659584},"max":{"secs":0,"nanos":27093504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":31996800},"median":{"secs":0,"nanos":31997440},"variance":{"secs":0,"nanos":70},"min":{"secs":0,"nanos":31679488},"max":{"secs":0,"nanos":32541184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":366651468},"median":{"secs":0,"nanos":362670080},"variance":{"secs":0,"nanos":49445},"min":{"secs":0,"nanos":361556480},"max":{"secs":0,"nanos":381890048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":371509504},"median":{"secs":0,"nanos":366994944},"variance":{"secs":0,"nanos":69256},"min":{"secs":0,"nanos":364701440},"max":{"secs":0,"nanos":388809216}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":65536,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21417446},"median":{"secs":0,"nanos":21382656},"variance":{"secs":0,"nanos":415},"min":{"secs":0,"nanos":20710144},"max":{"secs":0,"nanos":22616064}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":22476416},"median":{"secs":0,"nanos":22470144},"variance":{"secs":0,"nanos":279},"min":{"secs":0,"nanos":21725696},"max":{"secs":0,"nanos":23371776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":28875366},"median":{"secs":0,"nanos":28708608},"variance":{"secs":0,"nanos":216},"min":{"secs":0,"nanos":28366080},"max":{"secs":0,"nanos":29736960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":32768,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4851123},"median":{"secs":0,"nanos":4648192},"variance":{"secs":0,"nanos":107},"min":{"secs":0,"nanos":4598528},"max":{"secs":0,"nanos":5548544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5934899},"median":{"secs":0,"nanos":5979648},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":5647360},"max":{"secs":0,"nanos":6553856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":6979353},"median":{"secs":0,"nanos":6851584},"variance":{"secs":0,"nanos":103},"min":{"secs":0,"nanos":6684416},"max":{"secs":0,"nanos":7546112}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":32768,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":40894259},"median":{"secs":0,"nanos":40931584},"variance":{"secs":0,"nanos":53},"min":{"secs":0,"nanos":40473856},"max":{"secs":0,"nanos":41259264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":42504857},"median":{"secs":0,"nanos":42556416},"variance":{"secs":0,"nanos":57},"min":{"secs":0,"nanos":42107392},"max":{"secs":0,"nanos":42871808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":53766092},"median":{"secs":0,"nanos":53843712},"variance":{"secs":0,"nanos":35},"min":{"secs":0,"nanos":53459968},"max":{"secs":0,"nanos":53998336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":670685849},"median":{"secs":0,"nanos":668935168},"variance":{"secs":0,"nanos":31423},"min":{"secs":0,"nanos":663690752},"max":{"secs":0,"nanos":679420160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":682268467},"median":{"secs":0,"nanos":673583360},"variance":{"secs":0,"nanos":215138},"min":{"secs":0,"nanos":671344384},"max":{"secs":0,"nanos":710997504}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":32768,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":9396249},"median":{"secs":0,"nanos":9459968},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":9106944},"max":{"secs":0,"nanos":9636608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":10463846},"median":{"secs":0,"nanos":10453760},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":10277376},"max":{"secs":0,"nanos":10681600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":30823808},"median":{"secs":0,"nanos":26791680},"variance":{"secs":0,"nanos":76451},"min":{"secs":0,"nanos":22859264},"max":{"secs":0,"nanos":48067840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":90009497},"median":{"secs":0,"nanos":90045440},"variance":{"secs":0,"nanos":125},"min":{"secs":0,"nanos":89118464},"max":{"secs":0,"nanos":90479104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":91021465},"median":{"secs":0,"nanos":91028480},"variance":{"secs":0,"nanos":79},"min":{"secs":0,"nanos":90744064},"max":{"secs":0,"nanos":91762432}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":64,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":11110},"median":{"secs":0,"nanos":11008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":10496},"max":{"secs":0,"nanos":12288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":89267},"median":{"secs":0,"nanos":88832},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88064},"max":{"secs":0,"nanos":94976}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":128,"k":32768,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5615948},"median":{"secs":0,"nanos":5667840},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":5507840},"max":{"secs":0,"nanos":5812992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":7254528},"median":{"secs":0,"nanos":7282176},"variance":{"secs":0,"nanos":14},"min":{"secs":0,"nanos":6994688},"max":{"secs":0,"nanos":7441408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13272448},"median":{"secs":0,"nanos":13286912},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":13062144},"max":{"secs":0,"nanos":13470464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":20118809},"median":{"secs":0,"nanos":19854080},"variance":{"secs":0,"nanos":273},"min":{"secs":0,"nanos":19691008},"max":{"secs":0,"nanos":21304320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20417459},"median":{"secs":0,"nanos":20252672},"variance":{"secs":0,"nanos":282},"min":{"secs":0,"nanos":20059648},"max":{"secs":0,"nanos":21484032}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":32768,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3423052},"median":{"secs":0,"nanos":3394048},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":3371008},"max":{"secs":0,"nanos":3558400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3545011},"median":{"secs":0,"nanos":3521280},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":3496704},"max":{"secs":0,"nanos":3688960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5641728},"median":{"secs":0,"nanos":5644288},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":5503232},"max":{"secs":0,"nanos":5911808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":22674764},"median":{"secs":0,"nanos":7842304},"variance":{"secs":0,"nanos":548095},"min":{"secs":0,"nanos":6949888},"max":{"secs":0,"nanos":64249856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11608755},"median":{"secs":0,"nanos":11658496},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":11399680},"max":{"secs":0,"nanos":11836160}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32768,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21633075},"median":{"secs":0,"nanos":11991296},"variance":{"secs":0,"nanos":640823},"min":{"secs":0,"nanos":10868992},"max":{"secs":0,"nanos":96092928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":31867084},"median":{"secs":0,"nanos":17044224},"variance":{"secs":0,"nanos":924218},"min":{"secs":0,"nanos":16225536},"max":{"secs":0,"nanos":102921216}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":31459737},"median":{"secs":0,"nanos":42833664},"variance":{"secs":0,"nanos":308046},"min":{"secs":0,"nanos":11073024},"max":{"secs":0,"nanos":59980288}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":32768,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":14512640},"median":{"secs":0,"nanos":14647040},"variance":{"secs":0,"nanos":141},"min":{"secs":0,"nanos":13665280},"max":{"secs":0,"nanos":15019520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":29992064},"median":{"secs":0,"nanos":16625408},"variance":{"secs":0,"nanos":429513},"min":{"secs":0,"nanos":16011008},"max":{"secs":0,"nanos":63046656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":36373888},"median":{"secs":0,"nanos":21307392},"variance":{"secs":0,"nanos":510622},"min":{"secs":0,"nanos":19935232},"max":{"secs":0,"nanos":75085056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":219579596},"median":{"secs":0,"nanos":219159808},"variance":{"secs":0,"nanos":1414},"min":{"secs":0,"nanos":218418944},"max":{"secs":0,"nanos":222172416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":232644940},"median":{"secs":0,"nanos":232518144},"variance":{"secs":0,"nanos":5205},"min":{"secs":0,"nanos":230202112},"max":{"secs":0,"nanos":237583872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":32768,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2766310},"median":{"secs":0,"nanos":2749696},"variance":{"secs":0,"nanos":8},"min":{"secs":0,"nanos":2705408},"max":{"secs":0,"nanos":3028992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3551334},"median":{"secs":0,"nanos":3440640},"variance":{"secs":0,"nanos":35},"min":{"secs":0,"nanos":3437824},"max":{"secs":0,"nanos":3925504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3906764},"median":{"secs":0,"nanos":3866880},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":3865856},"max":{"secs":0,"nanos":4080896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":32768,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5648665},"median":{"secs":0,"nanos":5705984},"variance":{"secs":0,"nanos":14},"min":{"secs":0,"nanos":5517056},"max":{"secs":0,"nanos":5837824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":6367129},"median":{"secs":0,"nanos":6298112},"variance":{"secs":0,"nanos":110},"min":{"secs":0,"nanos":6150400},"max":{"secs":0,"nanos":7306240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7330816},"median":{"secs":0,"nanos":7385088},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":7192320},"max":{"secs":0,"nanos":7547392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":56857088},"median":{"secs":0,"nanos":57193984},"variance":{"secs":0,"nanos":2265},"min":{"secs":0,"nanos":54332160},"max":{"secs":0,"nanos":59235072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":57981952},"median":{"secs":0,"nanos":57946368},"variance":{"secs":0,"nanos":441},"min":{"secs":0,"nanos":57174528},"max":{"secs":0,"nanos":59590400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":32768,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2118348},"median":{"secs":0,"nanos":2082048},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":2077184},"max":{"secs":0,"nanos":2296832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2401177},"median":{"secs":0,"nanos":2408192},"variance":{"secs":0,"nanos":43},"min":{"secs":0,"nanos":2184704},"max":{"secs":0,"nanos":2776320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7980390},"median":{"secs":0,"nanos":7995904},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":7832320},"max":{"secs":0,"nanos":8111360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13511526},"median":{"secs":0,"nanos":15219200},"variance":{"secs":0,"nanos":13307},"min":{"secs":0,"nanos":4081664},"max":{"secs":0,"nanos":15886080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":25210163},"median":{"secs":0,"nanos":35727360},"variance":{"secs":0,"nanos":222414},"min":{"secs":0,"nanos":5302016},"max":{"secs":0,"nanos":43770112}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":300928},"median":{"secs":0,"nanos":300544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":300032},"max":{"secs":0,"nanos":303104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1470771},"median":{"secs":0,"nanos":1324288},"variance":{"secs":0,"nanos":370},"min":{"secs":0,"nanos":868096},"max":{"secs":0,"nanos":2639616}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":257203},"median":{"secs":0,"nanos":183808},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":180480},"max":{"secs":0,"nanos":924160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":562841},"median":{"secs":0,"nanos":528384},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":526592},"max":{"secs":0,"nanos":872960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20275},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19456},"max":{"secs":0,"nanos":23808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":29747},"median":{"secs":0,"nanos":29440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28928},"max":{"secs":0,"nanos":32768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1254579},"median":{"secs":0,"nanos":1179136},"variance":{"secs":0,"nanos":35},"min":{"secs":0,"nanos":1166848},"max":{"secs":0,"nanos":1800192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3620172},"median":{"secs":0,"nanos":3551232},"variance":{"secs":0,"nanos":43},"min":{"secs":0,"nanos":3434496},"max":{"secs":0,"nanos":3972608}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":16,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"HighlyPermuted","matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8313702},"median":{"secs":0,"nanos":8901120},"variance":{"secs":0,"nanos":1223},"min":{"secs":0,"nanos":7066880},"max":{"secs":0,"nanos":9966336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":10508083},"median":{"secs":0,"nanos":10599168},"variance":{"secs":0,"nanos":7215},"min":{"secs":0,"nanos":7117056},"max":{"secs":0,"nanos":16107776}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":16,"k":128,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2057523},"median":{"secs":0,"nanos":2126592},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":1760000},"max":{"secs":0,"nanos":2305024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2409190},"median":{"secs":0,"nanos":2354688},"variance":{"secs":0,"nanos":89},"min":{"secs":0,"nanos":2120960},"max":{"secs":0,"nanos":3054080}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":481049},"median":{"secs":0,"nanos":480512},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":477952},"max":{"secs":0,"nanos":485632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3271321},"median":{"secs":0,"nanos":3102976},"variance":{"secs":0,"nanos":69},"min":{"secs":0,"nanos":3101696},"max":{"secs":0,"nanos":3828736}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2238259},"median":{"secs":0,"nanos":2165248},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":2161408},"max":{"secs":0,"nanos":2557440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8523955},"median":{"secs":0,"nanos":8820992},"variance":{"secs":0,"nanos":695},"min":{"secs":0,"nanos":6870784},"max":{"secs":0,"nanos":9652224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":13030},"median":{"secs":0,"nanos":13056},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12288},"max":{"secs":0,"nanos":14592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":50176},"median":{"secs":0,"nanos":50176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49152},"max":{"secs":0,"nanos":51968}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9856},"median":{"secs":0,"nanos":9472},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9216},"max":{"secs":0,"nanos":12544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16384},"median":{"secs":0,"nanos":16384},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":17408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":52249},"median":{"secs":0,"nanos":51712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50944},"max":{"secs":0,"nanos":57856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":88576},"median":{"secs":0,"nanos":88832},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":87296},"max":{"secs":0,"nanos":89600}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23244},"median":{"secs":0,"nanos":23040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22528},"max":{"secs":0,"nanos":24832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":447257},"median":{"secs":0,"nanos":393472},"variance":{"secs":0,"nanos":26},"min":{"secs":0,"nanos":391936},"max":{"secs":0,"nanos":931840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1030963},"median":{"secs":0,"nanos":882432},"variance":{"secs":0,"nanos":38},"min":{"secs":0,"nanos":873984},"max":{"secs":0,"nanos":1378816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8403737},"median":{"secs":0,"nanos":7565056},"variance":{"secs":0,"nanos":3139},"min":{"secs":0,"nanos":6970624},"max":{"secs":0,"nanos":13059328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":17757619},"median":{"secs":0,"nanos":21202432},"variance":{"secs":0,"nanos":19331},"min":{"secs":0,"nanos":12700416},"max":{"secs":0,"nanos":23019008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":21747584},"median":{"secs":0,"nanos":21529856},"variance":{"secs":0,"nanos":635},"min":{"secs":0,"nanos":20941824},"max":{"secs":0,"nanos":22815488}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":880051},"median":{"secs":0,"nanos":833792},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":831488},"max":{"secs":0,"nanos":1156608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4265472},"median":{"secs":0,"nanos":4232960},"variance":{"secs":0,"nanos":53},"min":{"secs":0,"nanos":4065536},"max":{"secs":0,"nanos":4748032}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1181798},"median":{"secs":0,"nanos":1018880},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":1015040},"max":{"secs":0,"nanos":1640192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3060940},"median":{"secs":0,"nanos":3099136},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":2771200},"max":{"secs":0,"nanos":3341312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5700761},"median":{"secs":0,"nanos":5619968},"variance":{"secs":0,"nanos":114},"min":{"secs":0,"nanos":5458688},"max":{"secs":0,"nanos":6657792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6021888},"median":{"secs":0,"nanos":5919744},"variance":{"secs":0,"nanos":265},"min":{"secs":0,"nanos":5434368},"max":{"secs":0,"nanos":7074560}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":907520},"median":{"secs":0,"nanos":837120},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":835840},"max":{"secs":0,"nanos":1338368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":5061452},"median":{"secs":0,"nanos":5076480},"variance":{"secs":0,"nanos":160},"min":{"secs":0,"nanos":4518912},"max":{"secs":0,"nanos":5577984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":280524},"median":{"secs":0,"nanos":280576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":279552},"max":{"secs":0,"nanos":283136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":867660},"median":{"secs":0,"nanos":785152},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":783872},"max":{"secs":0,"nanos":1090560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3206476},"median":{"secs":0,"nanos":3286784},"variance":{"secs":0,"nanos":76},"min":{"secs":0,"nanos":2921984},"max":{"secs":0,"nanos":3659520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5808256},"median":{"secs":0,"nanos":5945088},"variance":{"secs":0,"nanos":159},"min":{"secs":0,"nanos":5228288},"max":{"secs":0,"nanos":6368000}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":455782},"median":{"secs":0,"nanos":438272},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":436992},"max":{"secs":0,"nanos":613888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1301427},"median":{"secs":0,"nanos":1154560},"variance":{"secs":0,"nanos":41},"min":{"secs":0,"nanos":1151232},"max":{"secs":0,"nanos":1724160}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":16,"k":128,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1764172},"median":{"secs":0,"nanos":1688064},"variance":{"secs":0,"nanos":26},"min":{"secs":0,"nanos":1686016},"max":{"secs":0,"nanos":2189056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2888012},"median":{"secs":0,"nanos":2841088},"variance":{"secs":0,"nanos":56},"min":{"secs":0,"nanos":2691328},"max":{"secs":0,"nanos":3344896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16,"n":128,"k":128,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"HighlyPermuted","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1924096},"median":{"secs":0,"nanos":1908224},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1907456},"max":{"secs":0,"nanos":2066432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4443648},"median":{"secs":0,"nanos":4312576},"variance":{"secs":0,"nanos":114},"min":{"secs":0,"nanos":4123904},"max":{"secs":0,"nanos":5063680}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2,"n":64,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":311372},"median":{"secs":0,"nanos":296192},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":295680},"max":{"secs":0,"nanos":443136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":441036},"median":{"secs":0,"nanos":425728},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":425472},"max":{"secs":0,"nanos":576768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":64,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":434022},"median":{"secs":0,"nanos":433664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":433408},"max":{"secs":0,"nanos":435712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":611328},"median":{"secs":0,"nanos":594176},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":592640},"max":{"secs":0,"nanos":762624}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":4,"n":64,"k":256,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":11340},"median":{"secs":0,"nanos":11264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":10240},"max":{"secs":0,"nanos":13056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":88550},"median":{"secs":0,"nanos":88576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88064},"max":{"secs":0,"nanos":89088}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13235},"median":{"secs":0,"nanos":13312},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12800},"max":{"secs":0,"nanos":14080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":26777},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23040},"max":{"secs":0,"nanos":53248}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":18176},"median":{"secs":0,"nanos":16384},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":26368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":35532},"median":{"secs":0,"nanos":35072},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":34304},"max":{"secs":0,"nanos":41728}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":25702},"median":{"secs":0,"nanos":25600},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":25088},"max":{"secs":0,"nanos":27136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":38758},"median":{"secs":0,"nanos":35072},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":34304},"max":{"secs":0,"nanos":72448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":108774},"median":{"secs":0,"nanos":108544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":107264},"max":{"secs":0,"nanos":113664}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":273587},"median":{"secs":0,"nanos":189696},"variance":{"secs":0,"nanos":28},"min":{"secs":0,"nanos":187648},"max":{"secs":0,"nanos":635392}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1267916},"median":{"secs":0,"nanos":804608},"variance":{"secs":0,"nanos":560},"min":{"secs":0,"nanos":752640},"max":{"secs":0,"nanos":2548224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1080243},"median":{"secs":0,"nanos":1013248},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":1011712},"max":{"secs":0,"nanos":1677824}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4,"n":64,"k":512,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":95436},"median":{"secs":0,"nanos":95232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":94976},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":370227},"median":{"secs":0,"nanos":368896},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368384},"max":{"secs":0,"nanos":380928}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":19046},"median":{"secs":0,"nanos":19200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18432},"max":{"secs":0,"nanos":19968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":33152},"median":{"secs":0,"nanos":28672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":75008}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":27929},"median":{"secs":0,"nanos":26624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":25856},"max":{"secs":0,"nanos":41728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":56396},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":55296},"max":{"secs":0,"nanos":59136}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":64,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":25267},"median":{"secs":0,"nanos":20992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":20480},"max":{"secs":0,"nanos":63744}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":37811},"median":{"secs":0,"nanos":33792},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":76800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":312012},"median":{"secs":0,"nanos":275200},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":274176},"max":{"secs":0,"nanos":619520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":878182},"median":{"secs":0,"nanos":839424},"variance":{"secs":0,"nanos":209},"min":{"secs":0,"nanos":484352},"max":{"secs":0,"nanos":1681664}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":527718},"median":{"secs":0,"nanos":528128},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":526592},"max":{"secs":0,"nanos":529408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1220992},"median":{"secs":0,"nanos":942592},"variance":{"secs":0,"nanos":315},"min":{"secs":0,"nanos":941056},"max":{"secs":0,"nanos":2703616}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4,"n":64,"k":1024,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":196019},"median":{"secs":0,"nanos":173568},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":173056},"max":{"secs":0,"nanos":375040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":326272},"median":{"secs":0,"nanos":326144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":325632},"max":{"secs":0,"nanos":327424}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":637696},"median":{"secs":0,"nanos":573184},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":570112},"max":{"secs":0,"nanos":1220608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8800460},"median":{"secs":0,"nanos":8792576},"variance":{"secs":0,"nanos":11716},"min":{"secs":0,"nanos":3417600},"max":{"secs":0,"nanos":14553088}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":361523},"median":{"secs":0,"nanos":334080},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":331520},"max":{"secs":0,"nanos":603904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2037862},"median":{"secs":0,"nanos":2078464},"variance":{"secs":0,"nanos":150},"min":{"secs":0,"nanos":1640448},"max":{"secs":0,"nanos":2858752}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14259},"median":{"secs":0,"nanos":13568},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12800},"max":{"secs":0,"nanos":20736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20377},"median":{"secs":0,"nanos":19456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18432},"max":{"secs":0,"nanos":26368}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3702886},"median":{"secs":0,"nanos":3502080},"variance":{"secs":0,"nanos":271},"min":{"secs":0,"nanos":3212032},"max":{"secs":0,"nanos":4715520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":34691174},"median":{"secs":0,"nanos":27917568},"variance":{"secs":0,"nanos":128925},"min":{"secs":0,"nanos":23955968},"max":{"secs":0,"nanos":53144576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":32,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"HighlyPermuted","matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5863731},"median":{"secs":0,"nanos":5678080},"variance":{"secs":0,"nanos":575},"min":{"secs":0,"nanos":4977664},"max":{"secs":0,"nanos":7100416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15950796},"median":{"secs":0,"nanos":15961856},"variance":{"secs":0,"nanos":171},"min":{"secs":0,"nanos":15139072},"max":{"secs":0,"nanos":16654080}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":128,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8247091},"median":{"secs":0,"nanos":8182784},"variance":{"secs":0,"nanos":320},"min":{"secs":0,"nanos":7327232},"max":{"secs":0,"nanos":9269248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":18019865},"median":{"secs":0,"nanos":18150400},"variance":{"secs":0,"nanos":374},"min":{"secs":0,"nanos":16984832},"max":{"secs":0,"nanos":19268352}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":14141004},"median":{"secs":0,"nanos":14137856},"variance":{"secs":0,"nanos":206},"min":{"secs":0,"nanos":13589248},"max":{"secs":0,"nanos":15090688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14630323},"median":{"secs":0,"nanos":14756864},"variance":{"secs":0,"nanos":529},"min":{"secs":0,"nanos":13082368},"max":{"secs":0,"nanos":15559424}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13937664},"median":{"secs":0,"nanos":14202368},"variance":{"secs":0,"nanos":351},"min":{"secs":0,"nanos":12603904},"max":{"secs":0,"nanos":14743040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14219366},"median":{"secs":0,"nanos":14302208},"variance":{"secs":0,"nanos":524},"min":{"secs":0,"nanos":12675840},"max":{"secs":0,"nanos":15406592}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":90086},"median":{"secs":0,"nanos":90112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88832},"max":{"secs":0,"nanos":91648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":152857},"median":{"secs":0,"nanos":120832},"variance":{"secs":0,"nanos":8},"min":{"secs":0,"nanos":120320},"max":{"secs":0,"nanos":433152}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":256230},"median":{"secs":0,"nanos":245760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":245248},"max":{"secs":0,"nanos":350976}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":573440},"median":{"secs":0,"nanos":533504},"variance":{"secs":0,"nanos":8},"min":{"secs":0,"nanos":529920},"max":{"secs":0,"nanos":827648}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":36377},"median":{"secs":0,"nanos":36608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35584},"max":{"secs":0,"nanos":37120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":254438},"median":{"secs":0,"nanos":38400},"variance":{"secs":0,"nanos":401},"min":{"secs":0,"nanos":37120},"max":{"secs":0,"nanos":2154752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1699814},"median":{"secs":0,"nanos":1096448},"variance":{"secs":0,"nanos":891},"min":{"secs":0,"nanos":1092608},"max":{"secs":0,"nanos":3728896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2904243},"median":{"secs":0,"nanos":2898688},"variance":{"secs":0,"nanos":1174},"min":{"secs":0,"nanos":1952000},"max":{"secs":0,"nanos":5277952}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":376524},"median":{"secs":0,"nanos":326400},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":324864},"max":{"secs":0,"nanos":756480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1576371},"median":{"secs":0,"nanos":1115648},"variance":{"secs":0,"nanos":1135},"min":{"secs":0,"nanos":766464},"max":{"secs":0,"nanos":4256512}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":222464},"median":{"secs":0,"nanos":214272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":212736},"max":{"secs":0,"nanos":298496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":871219},"median":{"secs":0,"nanos":606208},"variance":{"secs":0,"nanos":497},"min":{"secs":0,"nanos":599552},"max":{"secs":0,"nanos":2983424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3363788},"median":{"secs":0,"nanos":3365632},"variance":{"secs":0,"nanos":2243},"min":{"secs":0,"nanos":1609216},"max":{"secs":0,"nanos":6225920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3259392},"median":{"secs":0,"nanos":3986176},"variance":{"secs":0,"nanos":1436},"min":{"secs":0,"nanos":1761280},"max":{"secs":0,"nanos":5388032}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":512,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":920985},"median":{"secs":0,"nanos":358912},"variance":{"secs":0,"nanos":1159},"min":{"secs":0,"nanos":304896},"max":{"secs":0,"nanos":3177216}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":536755},"median":{"secs":0,"nanos":391168},"variance":{"secs":0,"nanos":171},"min":{"secs":0,"nanos":387840},"max":{"secs":0,"nanos":1778432}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":193324006},"median":{"secs":0,"nanos":192765440},"variance":{"secs":0,"nanos":11645},"min":{"secs":0,"nanos":186570752},"max":{"secs":0,"nanos":199012864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":195429785},"median":{"secs":0,"nanos":196873472},"variance":{"secs":0,"nanos":4553},"min":{"secs":0,"nanos":192331776},"max":{"secs":0,"nanos":197699328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":237430681},"median":{"secs":0,"nanos":229453824},"variance":{"secs":0,"nanos":242403},"min":{"secs":0,"nanos":225567744},"max":{"secs":0,"nanos":275296000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":256045952},"median":{"secs":0,"nanos":262268928},"variance":{"secs":0,"nanos":147885},"min":{"secs":0,"nanos":231919360},"max":{"secs":0,"nanos":269164800}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":522252108},"median":{"secs":0,"nanos":517136128},"variance":{"secs":0,"nanos":3408557},"min":{"secs":0,"nanos":438504704},"max":{"secs":0,"nanos":628865024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":507548876},"median":{"secs":0,"nanos":542834432},"variance":{"secs":0,"nanos":4224402},"min":{"secs":0,"nanos":433224960},"max":{"secs":0,"nanos":627158528}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":81811584},"median":{"secs":0,"nanos":82436864},"variance":{"secs":0,"nanos":3186},"min":{"secs":0,"nanos":78460160},"max":{"secs":0,"nanos":84643840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":114938393},"median":{"secs":0,"nanos":114999552},"variance":{"secs":0,"nanos":1694},"min":{"secs":0,"nanos":113550848},"max":{"secs":0,"nanos":117731840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":164960588},"median":{"secs":0,"nanos":165351424},"variance":{"secs":0,"nanos":1023},"min":{"secs":0,"nanos":163256832},"max":{"secs":0,"nanos":166547456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":171058329},"median":{"secs":0,"nanos":171093760},"variance":{"secs":0,"nanos":906},"min":{"secs":0,"nanos":169534208},"max":{"secs":0,"nanos":173021952}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":1024,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":447235814},"median":{"secs":0,"nanos":419709440},"variance":{"secs":0,"nanos":3099054},"min":{"secs":0,"nanos":414582528},"max":{"secs":0,"nanos":586163456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":436852070},"median":{"secs":0,"nanos":438107392},"variance":{"secs":0,"nanos":136849},"min":{"secs":0,"nanos":417614592},"max":{"secs":0,"nanos":462312960}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":44713907},"median":{"secs":0,"nanos":44297984},"variance":{"secs":0,"nanos":414},"min":{"secs":0,"nanos":44028160},"max":{"secs":0,"nanos":45654016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":45902566},"median":{"secs":0,"nanos":46460416},"variance":{"secs":0,"nanos":474},"min":{"secs":0,"nanos":44981760},"max":{"secs":0,"nanos":46656256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53784371},"median":{"secs":0,"nanos":54295296},"variance":{"secs":0,"nanos":3289},"min":{"secs":0,"nanos":51363072},"max":{"secs":0,"nanos":56457984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":83514752},"median":{"secs":0,"nanos":83474944},"variance":{"secs":0,"nanos":610},"min":{"secs":0,"nanos":81813248},"max":{"secs":0,"nanos":85226496}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5306572},"median":{"secs":0,"nanos":5280768},"variance":{"secs":0,"nanos":119},"min":{"secs":0,"nanos":5109760},"max":{"secs":0,"nanos":6308096}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":67307673},"median":{"secs":0,"nanos":67592192},"variance":{"secs":0,"nanos":321},"min":{"secs":0,"nanos":66212096},"max":{"secs":0,"nanos":67816192}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":128,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"HighlyPermuted"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":83667763},"median":{"secs":0,"nanos":84257536},"variance":{"secs":0,"nanos":1651},"min":{"secs":0,"nanos":81016576},"max":{"secs":0,"nanos":85917952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":164889292},"median":{"secs":0,"nanos":164764928},"variance":{"secs":0,"nanos":464},"min":{"secs":0,"nanos":163904256},"max":{"secs":0,"nanos":166212864}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":128,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"HighlyPermuted","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":172074572},"median":{"secs":0,"nanos":169855488},"variance":{"secs":0,"nanos":32032},"min":{"secs":0,"nanos":167344640},"max":{"secs":0,"nanos":183512064}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":256206336},"median":{"secs":0,"nanos":256176640},"variance":{"secs":0,"nanos":124907},"min":{"secs":0,"nanos":239291392},"max":{"secs":0,"nanos":277885440}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2,"n":256,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1584153},"median":{"secs":0,"nanos":1554688},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":1553920},"max":{"secs":0,"nanos":1702656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2268825},"median":{"secs":0,"nanos":2252288},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":2251264},"max":{"secs":0,"nanos":2418944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1714201},"median":{"secs":0,"nanos":1582080},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":1579008},"max":{"secs":0,"nanos":2125312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3977779},"median":{"secs":0,"nanos":3927808},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":3920384},"max":{"secs":0,"nanos":4119296}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":4,"n":256,"k":512,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":89881},"median":{"secs":0,"nanos":88832},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88064},"max":{"secs":0,"nanos":100352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":163916},"median":{"secs":0,"nanos":163840},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":162816},"max":{"secs":0,"nanos":165376}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":7756},"median":{"secs":0,"nanos":7680},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6912},"max":{"secs":0,"nanos":9216}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":26854},"median":{"secs":0,"nanos":24320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23552},"max":{"secs":0,"nanos":48640}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":55705},"median":{"secs":0,"nanos":52736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52736},"max":{"secs":0,"nanos":63488}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":100659},"median":{"secs":0,"nanos":100608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":99840},"max":{"secs":0,"nanos":101376}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":749440},"median":{"secs":0,"nanos":659200},"variance":{"secs":0,"nanos":69},"min":{"secs":0,"nanos":656896},"max":{"secs":0,"nanos":1542656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":912716},"median":{"secs":0,"nanos":691712},"variance":{"secs":0,"nanos":171},"min":{"secs":0,"nanos":688128},"max":{"secs":0,"nanos":1798912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1005465},"median":{"secs":0,"nanos":872448},"variance":{"secs":0,"nanos":110},"min":{"secs":0,"nanos":871168},"max":{"secs":0,"nanos":1992192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1357772},"median":{"secs":0,"nanos":1250560},"variance":{"secs":0,"nanos":81},"min":{"secs":0,"nanos":1248256},"max":{"secs":0,"nanos":2208000}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":512,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":164633},"median":{"secs":0,"nanos":152320},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":151552},"max":{"secs":0,"nanos":273920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":257996},"median":{"secs":0,"nanos":252416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":251136},"max":{"secs":0,"nanos":309248}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4,"n":256,"k":64,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":7142},"median":{"secs":0,"nanos":7168},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6656},"max":{"secs":0,"nanos":7680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":26393},"median":{"secs":0,"nanos":26624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":25856},"max":{"secs":0,"nanos":26880}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6476},"median":{"secs":0,"nanos":6400},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6144},"max":{"secs":0,"nanos":7424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9011},"median":{"secs":0,"nanos":8960},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8192},"max":{"secs":0,"nanos":11008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":49971},"median":{"secs":0,"nanos":49920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49152},"max":{"secs":0,"nanos":50432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":93465},"median":{"secs":0,"nanos":92928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":92160},"max":{"secs":0,"nanos":100608}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":92851},"median":{"secs":0,"nanos":91904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":91392},"max":{"secs":0,"nanos":101888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":406425},"median":{"secs":0,"nanos":163584},"variance":{"secs":0,"nanos":233},"min":{"secs":0,"nanos":162304},"max":{"secs":0,"nanos":1381888}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":9523},"median":{"secs":0,"nanos":9216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":14080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9446},"median":{"secs":0,"nanos":9472},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":10496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20377},"median":{"secs":0,"nanos":20224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19968},"max":{"secs":0,"nanos":21504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":32051},"median":{"secs":0,"nanos":31744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30976},"max":{"secs":0,"nanos":36352}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":12364},"median":{"secs":0,"nanos":11520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11008},"max":{"secs":0,"nanos":20992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18150},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17152},"max":{"secs":0,"nanos":22784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":32179},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":32768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":32844},"median":{"secs":0,"nanos":32768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":33280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":233395},"median":{"secs":0,"nanos":95232},"variance":{"secs":0,"nanos":168},"min":{"secs":0,"nanos":94720},"max":{"secs":0,"nanos":1466624}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":292505},"median":{"secs":0,"nanos":171264},"variance":{"secs":0,"nanos":128},"min":{"secs":0,"nanos":170496},"max":{"secs":0,"nanos":1366784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":512,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14080},"median":{"secs":0,"nanos":13056},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12544},"max":{"secs":0,"nanos":22272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":15872},"median":{"secs":0,"nanos":15872},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":16384}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4,"n":256,"k":32,"lhs_pow2_factor":2,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8601},"median":{"secs":0,"nanos":8448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8192},"max":{"secs":0,"nanos":9984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13542},"median":{"secs":0,"nanos":13568},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13056},"max":{"secs":0,"nanos":14080}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6988},"median":{"secs":0,"nanos":6912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6400},"max":{"secs":0,"nanos":8192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9113},"median":{"secs":0,"nanos":9216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":9728}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1469158},"median":{"secs":0,"nanos":1439488},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":1437696},"max":{"secs":0,"nanos":1594368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":7512908},"median":{"secs":0,"nanos":7635200},"variance":{"secs":0,"nanos":71},"min":{"secs":0,"nanos":7135232},"max":{"secs":0,"nanos":7846656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":14462361},"median":{"secs":0,"nanos":14370816},"variance":{"secs":0,"nanos":766},"min":{"secs":0,"nanos":13274624},"max":{"secs":0,"nanos":15973376}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":53094},"median":{"secs":0,"nanos":53248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52736},"max":{"secs":0,"nanos":53760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":100736},"median":{"secs":0,"nanos":100864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":100096},"max":{"secs":0,"nanos":102400}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":194406},"median":{"secs":0,"nanos":193792},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":193280},"max":{"secs":0,"nanos":200704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":349952},"median":{"secs":0,"nanos":349952},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":349440},"max":{"secs":0,"nanos":351232}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28979},"median":{"secs":0,"nanos":27904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27136},"max":{"secs":0,"nanos":34304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":32486},"median":{"secs":0,"nanos":32256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":34560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":342041},"median":{"secs":0,"nanos":339200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":335360},"max":{"secs":0,"nanos":374784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":639385},"median":{"secs":0,"nanos":593664},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":591616},"max":{"secs":0,"nanos":1053952}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":339174},"median":{"secs":0,"nanos":112384},"variance":{"secs":0,"nanos":304},"min":{"secs":0,"nanos":111104},"max":{"secs":0,"nanos":1984512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":278169},"median":{"secs":0,"nanos":188416},"variance":{"secs":0,"nanos":65},"min":{"secs":0,"nanos":185600},"max":{"secs":0,"nanos":1044992}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1609036},"median":{"secs":0,"nanos":1379840},"variance":{"secs":0,"nanos":177},"min":{"secs":0,"nanos":1374976},"max":{"secs":0,"nanos":2508800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1509657},"median":{"secs":0,"nanos":1407232},"variance":{"secs":0,"nanos":77},"min":{"secs":0,"nanos":1404672},"max":{"secs":0,"nanos":2342912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1970764},"median":{"secs":0,"nanos":1612800},"variance":{"secs":0,"nanos":407},"min":{"secs":0,"nanos":1473024},"max":{"secs":0,"nanos":3504896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4733977},"median":{"secs":0,"nanos":5005312},"variance":{"secs":0,"nanos":987},"min":{"secs":0,"nanos":3126272},"max":{"secs":0,"nanos":6421504}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":1024,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":240256},"median":{"secs":0,"nanos":238848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":237824},"max":{"secs":0,"nanos":251392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":256640},"median":{"secs":0,"nanos":251904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":251136},"max":{"secs":0,"nanos":296960}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":8192,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":257945},"median":{"secs":0,"nanos":238592},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":238336},"max":{"secs":0,"nanos":429568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1874560},"median":{"secs":0,"nanos":1713408},"variance":{"secs":0,"nanos":74},"min":{"secs":0,"nanos":1679616},"max":{"secs":0,"nanos":2567936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2659712},"median":{"secs":0,"nanos":2149376},"variance":{"secs":0,"nanos":995},"min":{"secs":0,"nanos":1846528},"max":{"secs":0,"nanos":4287232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4412595},"median":{"secs":0,"nanos":4145152},"variance":{"secs":0,"nanos":4061},"min":{"secs":0,"nanos":2831360},"max":{"secs":0,"nanos":9852672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":11127987},"median":{"secs":0,"nanos":14293760},"variance":{"secs":0,"nanos":58094},"min":{"secs":0,"nanos":1970944},"max":{"secs":0,"nanos":25492224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":71961},"median":{"secs":0,"nanos":72448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":68608},"max":{"secs":0,"nanos":74240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":125235},"median":{"secs":0,"nanos":125184},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":124416},"max":{"secs":0,"nanos":126976}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":135756},"median":{"secs":0,"nanos":135424},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":133120},"max":{"secs":0,"nanos":140288}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2,"n":64,"k":64,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":10444},"median":{"secs":0,"nanos":9984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":14848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28390},"median":{"secs":0,"nanos":28416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27904},"max":{"secs":0,"nanos":29440}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":156979},"median":{"secs":0,"nanos":156928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":155648},"max":{"secs":0,"nanos":158208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":487065},"median":{"secs":0,"nanos":432128},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":389888},"max":{"secs":0,"nanos":685824}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":191001},"median":{"secs":0,"nanos":190976},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":189952},"max":{"secs":0,"nanos":192768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":429235},"median":{"secs":0,"nanos":325120},"variance":{"secs":0,"nanos":95},"min":{"secs":0,"nanos":322048},"max":{"secs":0,"nanos":1356544}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":900761},"median":{"secs":0,"nanos":755968},"variance":{"secs":0,"nanos":104},"min":{"secs":0,"nanos":754176},"max":{"secs":0,"nanos":1806848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1784396},"median":{"secs":0,"nanos":1731328},"variance":{"secs":0,"nanos":327},"min":{"secs":0,"nanos":1311744},"max":{"secs":0,"nanos":3205632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2264652},"median":{"secs":0,"nanos":1776640},"variance":{"secs":0,"nanos":1263},"min":{"secs":0,"nanos":1353984},"max":{"secs":0,"nanos":4249088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1888640},"median":{"secs":0,"nanos":1898752},"variance":{"secs":0,"nanos":294},"min":{"secs":0,"nanos":1290496},"max":{"secs":0,"nanos":3131904}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":1024,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":445004},"median":{"secs":0,"nanos":192000},"variance":{"secs":0,"nanos":263},"min":{"secs":0,"nanos":177664},"max":{"secs":0,"nanos":1538560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":217318},"median":{"secs":0,"nanos":216576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":213760},"max":{"secs":0,"nanos":226560}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1366195},"median":{"secs":0,"nanos":719616},"variance":{"secs":0,"nanos":887},"min":{"secs":0,"nanos":666368},"max":{"secs":0,"nanos":2929408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":2978355},"median":{"secs":0,"nanos":3037184},"variance":{"secs":0,"nanos":812},"min":{"secs":0,"nanos":1812480},"max":{"secs":0,"nanos":4557824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6664268},"median":{"secs":0,"nanos":6144512},"variance":{"secs":0,"nanos":10518},"min":{"secs":0,"nanos":4059648},"max":{"secs":0,"nanos":15723520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":8256819},"median":{"secs":0,"nanos":8261632},"variance":{"secs":0,"nanos":1837},"min":{"secs":0,"nanos":6775808},"max":{"secs":0,"nanos":10547200}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":229555},"median":{"secs":0,"nanos":206336},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":205824},"max":{"secs":0,"nanos":435456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4188953},"median":{"secs":0,"nanos":4186368},"variance":{"secs":0,"nanos":3047},"min":{"secs":0,"nanos":2387456},"max":{"secs":0,"nanos":7819008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":7091},"median":{"secs":0,"nanos":6912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6656},"max":{"secs":0,"nanos":8448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9318},"median":{"secs":0,"nanos":9472},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":10240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4377369},"median":{"secs":0,"nanos":4132864},"variance":{"secs":0,"nanos":447},"min":{"secs":0,"nanos":3801600},"max":{"secs":0,"nanos":5751552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10895385},"median":{"secs":0,"nanos":10715904},"variance":{"secs":0,"nanos":500},"min":{"secs":0,"nanos":10106368},"max":{"secs":0,"nanos":12155904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":12840678},"median":{"secs":0,"nanos":13660416},"variance":{"secs":0,"nanos":2294},"min":{"secs":0,"nanos":8742912},"max":{"secs":0,"nanos":14158592}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":201062},"median":{"secs":0,"nanos":200704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":200192},"max":{"secs":0,"nanos":204544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":441958},"median":{"secs":0,"nanos":353792},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":353024},"max":{"secs":0,"nanos":843776}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":345011},"median":{"secs":0,"nanos":345088},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":344576},"max":{"secs":0,"nanos":345856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":675251},"median":{"secs":0,"nanos":647424},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":646656},"max":{"secs":0,"nanos":916224}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":19174},"median":{"secs":0,"nanos":18944},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":22272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58137},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":55296},"max":{"secs":0,"nanos":72448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":413798},"median":{"secs":0,"nanos":381440},"variance":{"secs":0,"nanos":9},"min":{"secs":0,"nanos":377088},"max":{"secs":0,"nanos":707584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2888115},"median":{"secs":0,"nanos":2582528},"variance":{"secs":0,"nanos":2050},"min":{"secs":0,"nanos":1874944},"max":{"secs":0,"nanos":6274048}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":738022},"median":{"secs":0,"nanos":355584},"variance":{"secs":0,"nanos":767},"min":{"secs":0,"nanos":240128},"max":{"secs":0,"nanos":2658304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":467584},"median":{"secs":0,"nanos":461312},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":460544},"max":{"secs":0,"nanos":525568}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":898585},"median":{"secs":0,"nanos":850432},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":848896},"max":{"secs":0,"nanos":1323264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5767731},"median":{"secs":0,"nanos":5364992},"variance":{"secs":0,"nanos":6461},"min":{"secs":0,"nanos":4299264},"max":{"secs":0,"nanos":13132800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4886604},"median":{"secs":0,"nanos":5378816},"variance":{"secs":0,"nanos":722},"min":{"secs":0,"nanos":3438080},"max":{"secs":0,"nanos":6117376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5372800},"median":{"secs":0,"nanos":6169600},"variance":{"secs":0,"nanos":775},"min":{"secs":0,"nanos":4413440},"max":{"secs":0,"nanos":6370304}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":1024,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":415411},"median":{"secs":0,"nanos":369920},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":347904},"max":{"secs":0,"nanos":857856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":816614},"median":{"secs":0,"nanos":704768},"variance":{"secs":0,"nanos":78},"min":{"secs":0,"nanos":683776},"max":{"secs":0,"nanos":1615872}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":8192,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":490035},"median":{"secs":0,"nanos":474112},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":472064},"max":{"secs":0,"nanos":635392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4933273},"median":{"secs":0,"nanos":4777216},"variance":{"secs":0,"nanos":1771},"min":{"secs":0,"nanos":3718400},"max":{"secs":0,"nanos":8094720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6624716},"median":{"secs":0,"nanos":5591040},"variance":{"secs":0,"nanos":5327},"min":{"secs":0,"nanos":4741120},"max":{"secs":0,"nanos":12463360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5738240},"median":{"secs":0,"nanos":5627648},"variance":{"secs":0,"nanos":118},"min":{"secs":0,"nanos":5485312},"max":{"secs":0,"nanos":6439936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":19584179},"median":{"secs":0,"nanos":7202816},"variance":{"secs":0,"nanos":284865},"min":{"secs":0,"nanos":5497088},"max":{"secs":0,"nanos":47223808}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":512,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":194969},"median":{"secs":0,"nanos":174336},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":173568},"max":{"secs":0,"nanos":381184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":362316},"median":{"secs":0,"nanos":346624},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":345600},"max":{"secs":0,"nanos":499456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":421734},"median":{"secs":0,"nanos":421888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":420352},"max":{"secs":0,"nanos":423680}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2,"n":64,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20761},"median":{"secs":0,"nanos":20736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":20224},"max":{"secs":0,"nanos":21248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":109977},"median":{"secs":0,"nanos":110080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":108800},"max":{"secs":0,"nanos":112384}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":64,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":5862},"median":{"secs":0,"nanos":5888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":6400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8448},"median":{"secs":0,"nanos":8448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7936},"max":{"secs":0,"nanos":9472}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1511808},"median":{"secs":0,"nanos":1481472},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":1479168},"max":{"secs":0,"nanos":1647872}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1481036},"median":{"secs":0,"nanos":1507328},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":1334272},"max":{"secs":0,"nanos":1657088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2590540},"median":{"secs":0,"nanos":2559232},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":2558464},"max":{"secs":0,"nanos":2728960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":99660},"median":{"secs":0,"nanos":99840},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":99328},"max":{"secs":0,"nanos":100096}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":170496},"median":{"secs":0,"nanos":170496},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":169984},"max":{"secs":0,"nanos":171008}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":4,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":176998},"median":{"secs":0,"nanos":177152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":176128},"max":{"secs":0,"nanos":177408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":317849},"median":{"secs":0,"nanos":317952},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":317184},"max":{"secs":0,"nanos":318208}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":1024,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8704},"median":{"secs":0,"nanos":8704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8192},"max":{"secs":0,"nanos":8960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":18713},"median":{"secs":0,"nanos":18944},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":19456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":50457},"median":{"secs":0,"nanos":50432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49920},"max":{"secs":0,"nanos":51200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":89600},"median":{"secs":0,"nanos":89600},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":89088},"max":{"secs":0,"nanos":90624}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":41190},"median":{"secs":0,"nanos":41216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":40704},"max":{"secs":0,"nanos":41728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49075},"median":{"secs":0,"nanos":49152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48384},"max":{"secs":0,"nanos":49664}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":810342},"median":{"secs":0,"nanos":735744},"variance":{"secs":0,"nanos":51},"min":{"secs":0,"nanos":731392},"max":{"secs":0,"nanos":1488896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":827724},"median":{"secs":0,"nanos":827392},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":812032},"max":{"secs":0,"nanos":845056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1321651},"median":{"secs":0,"nanos":1241344},"variance":{"secs":0,"nanos":52},"min":{"secs":0,"nanos":1237504},"max":{"secs":0,"nanos":2006272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2152985},"median":{"secs":0,"nanos":2029312},"variance":{"secs":0,"nanos":61},"min":{"secs":0,"nanos":2007552},"max":{"secs":0,"nanos":2692096}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":1024,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":200371},"median":{"secs":0,"nanos":200704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":197888},"max":{"secs":0,"nanos":201984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":501145},"median":{"secs":0,"nanos":276736},"variance":{"secs":0,"nanos":466},"min":{"secs":0,"nanos":263424},"max":{"secs":0,"nanos":2548736}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":8192,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":565964},"median":{"secs":0,"nanos":525056},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":524544},"max":{"secs":0,"nanos":758272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1149670},"median":{"secs":0,"nanos":1094144},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":1091328},"max":{"secs":0,"nanos":1282560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4403097},"median":{"secs":0,"nanos":1857792},"variance":{"secs":0,"nanos":13441},"min":{"secs":0,"nanos":1285632},"max":{"secs":0,"nanos":9518592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4500531},"median":{"secs":0,"nanos":3624960},"variance":{"secs":0,"nanos":2592},"min":{"secs":0,"nanos":3318784},"max":{"secs":0,"nanos":8309760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6400844},"median":{"secs":0,"nanos":5255680},"variance":{"secs":0,"nanos":6008},"min":{"secs":0,"nanos":4948736},"max":{"secs":0,"nanos":13252352}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":512,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":85017},"median":{"secs":0,"nanos":84736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":83200},"max":{"secs":0,"nanos":87296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":127744},"median":{"secs":0,"nanos":128000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126464},"max":{"secs":0,"nanos":129280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":804096},"median":{"secs":0,"nanos":780544},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":758272},"max":{"secs":0,"nanos":1008896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2,"n":64,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":18713},"median":{"secs":0,"nanos":18432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17408},"max":{"secs":0,"nanos":20480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":47180},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46080},"max":{"secs":0,"nanos":47616}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":72780},"median":{"secs":0,"nanos":72960},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":70656},"max":{"secs":0,"nanos":73728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":260096},"median":{"secs":0,"nanos":259840},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":258816},"max":{"secs":0,"nanos":261888}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":52480},"median":{"secs":0,"nanos":51712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50688},"max":{"secs":0,"nanos":60160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":166169},"median":{"secs":0,"nanos":166144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":164864},"max":{"secs":0,"nanos":167936}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":258380},"median":{"secs":0,"nanos":258560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":257024},"max":{"secs":0,"nanos":260096}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1545497},"median":{"secs":0,"nanos":1528320},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1526016},"max":{"secs":0,"nanos":1705984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":21811},"median":{"secs":0,"nanos":21504},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":20992},"max":{"secs":0,"nanos":23808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":35200},"median":{"secs":0,"nanos":33280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":31744},"max":{"secs":0,"nanos":55040}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":2048,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2849689},"median":{"secs":0,"nanos":2656512},"variance":{"secs":0,"nanos":114},"min":{"secs":0,"nanos":2651648},"max":{"secs":0,"nanos":3533824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3954201},"median":{"secs":0,"nanos":3727616},"variance":{"secs":0,"nanos":198},"min":{"secs":0,"nanos":3624448},"max":{"secs":0,"nanos":4849152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3986688},"median":{"secs":0,"nanos":4617984},"variance":{"secs":0,"nanos":877},"min":{"secs":0,"nanos":2835456},"max":{"secs":0,"nanos":5079552}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3105177},"median":{"secs":0,"nanos":3274240},"variance":{"secs":0,"nanos":115},"min":{"secs":0,"nanos":2417664},"max":{"secs":0,"nanos":3278592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4347392},"median":{"secs":0,"nanos":4080128},"variance":{"secs":0,"nanos":1268},"min":{"secs":0,"nanos":3310848},"max":{"secs":0,"nanos":6438656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6220339},"median":{"secs":0,"nanos":6220288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6216192},"max":{"secs":0,"nanos":6224384}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":2048,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":85683},"median":{"secs":0,"nanos":85760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":83456},"max":{"secs":0,"nanos":87808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":94387},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":92416},"max":{"secs":0,"nanos":99072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":101427},"median":{"secs":0,"nanos":98560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95488},"max":{"secs":0,"nanos":131328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4212556},"median":{"secs":0,"nanos":3438336},"variance":{"secs":0,"nanos":1853},"min":{"secs":0,"nanos":3429120},"max":{"secs":0,"nanos":7359744}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7298790},"median":{"secs":0,"nanos":6139904},"variance":{"secs":0,"nanos":11919},"min":{"secs":0,"nanos":6130688},"max":{"secs":0,"nanos":17655808}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":135884},"median":{"secs":0,"nanos":127232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":125952},"max":{"secs":0,"nanos":214784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":248012},"median":{"secs":0,"nanos":241152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":240384},"max":{"secs":0,"nanos":310272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":490086},"median":{"secs":0,"nanos":489984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":486912},"max":{"secs":0,"nanos":494336}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":407321},"median":{"secs":0,"nanos":407296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":406528},"max":{"secs":0,"nanos":410624}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2904217},"median":{"secs":0,"nanos":2718720},"variance":{"secs":0,"nanos":74},"min":{"secs":0,"nanos":2708224},"max":{"secs":0,"nanos":3532032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5874611},"median":{"secs":0,"nanos":6564864},"variance":{"secs":0,"nanos":1363},"min":{"secs":0,"nanos":4520448},"max":{"secs":0,"nanos":7822336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7697510},"median":{"secs":0,"nanos":6986496},"variance":{"secs":0,"nanos":1070},"min":{"secs":0,"nanos":6795264},"max":{"secs":0,"nanos":9365248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":17884595},"median":{"secs":0,"nanos":21118208},"variance":{"secs":0,"nanos":77743},"min":{"secs":0,"nanos":3594240},"max":{"secs":0,"nanos":33643008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":2048,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":110028},"median":{"secs":0,"nanos":110080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":107008},"max":{"secs":0,"nanos":111616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":226355},"median":{"secs":0,"nanos":226560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":225024},"max":{"secs":0,"nanos":228608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":253696},"median":{"secs":0,"nanos":253696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":252672},"max":{"secs":0,"nanos":255488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":297932},"median":{"secs":0,"nanos":297984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":296960},"max":{"secs":0,"nanos":299264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1526272},"median":{"secs":0,"nanos":1526272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":1524480},"max":{"secs":0,"nanos":1528832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3271475},"median":{"secs":0,"nanos":3272192},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":3262976},"max":{"secs":0,"nanos":3281152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":8712448},"median":{"secs":0,"nanos":9938432},"variance":{"secs":0,"nanos":4919},"min":{"secs":0,"nanos":5591808},"max":{"secs":0,"nanos":11915264}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":289280},"median":{"secs":0,"nanos":289280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288512},"max":{"secs":0,"nanos":290048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":3829606},"median":{"secs":0,"nanos":3730944},"variance":{"secs":0,"nanos":87},"min":{"secs":0,"nanos":3727616},"max":{"secs":0,"nanos":4717056}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":2,"n":128,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":271539},"median":{"secs":0,"nanos":271616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":270848},"max":{"secs":0,"nanos":272640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":358656},"median":{"secs":0,"nanos":358912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":357888},"max":{"secs":0,"nanos":359168}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":2,"n":128,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":10649},"median":{"secs":0,"nanos":10496},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":12288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":59545},"median":{"secs":0,"nanos":59392},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58880},"max":{"secs":0,"nanos":62208}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":277120},"median":{"secs":0,"nanos":276992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":276480},"max":{"secs":0,"nanos":278528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":628633},"median":{"secs":0,"nanos":628480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":627712},"max":{"secs":0,"nanos":631296}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":32,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":58777},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":57856},"max":{"secs":0,"nanos":59904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":73907},"median":{"secs":0,"nanos":73728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":73216},"max":{"secs":0,"nanos":75264}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":false,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28544},"median":{"secs":0,"nanos":28416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28160},"max":{"secs":0,"nanos":29184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":47820},"median":{"secs":0,"nanos":47616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":47104},"max":{"secs":0,"nanos":49920}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":74624},"median":{"secs":0,"nanos":74496},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":73984},"max":{"secs":0,"nanos":76288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":125772},"median":{"secs":0,"nanos":125440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":124928},"max":{"secs":0,"nanos":127744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":85248},"median":{"secs":0,"nanos":88064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":77312},"max":{"secs":0,"nanos":90624}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":243609},"median":{"secs":0,"nanos":243456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":242688},"max":{"secs":0,"nanos":245760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":133606},"median":{"secs":0,"nanos":133376},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":133120},"max":{"secs":0,"nanos":135168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":214553},"median":{"secs":0,"nanos":214784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":212992},"max":{"secs":0,"nanos":216576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":2,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6528},"median":{"secs":0,"nanos":6400},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6144},"max":{"secs":0,"nanos":7424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9267},"median":{"secs":0,"nanos":9216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":9728}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":860492},"median":{"secs":0,"nanos":860160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":858112},"max":{"secs":0,"nanos":865024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2807936},"median":{"secs":0,"nanos":2527232},"variance":{"secs":0,"nanos":293},"min":{"secs":0,"nanos":2510080},"max":{"secs":0,"nanos":3912192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4809395},"median":{"secs":0,"nanos":4556032},"variance":{"secs":0,"nanos":277},"min":{"secs":0,"nanos":4470016},"max":{"secs":0,"nanos":5967360}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":158848},"median":{"secs":0,"nanos":158720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":157952},"max":{"secs":0,"nanos":160768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":312883},"median":{"secs":0,"nanos":312576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":311808},"max":{"secs":0,"nanos":315136}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1572096},"median":{"secs":0,"nanos":896256},"variance":{"secs":0,"nanos":2300},"min":{"secs":0,"nanos":791808},"max":{"secs":0,"nanos":5739264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1016320},"median":{"secs":0,"nanos":1016832},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":1012480},"max":{"secs":0,"nanos":1019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1140864},"median":{"secs":0,"nanos":1018624},"variance":{"secs":0,"nanos":127},"min":{"secs":0,"nanos":1015808},"max":{"secs":0,"nanos":2210048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1048115},"median":{"secs":0,"nanos":1046528},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":1045248},"max":{"secs":0,"nanos":1063680}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":256,"n":512,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":221209},"median":{"secs":0,"nanos":214272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":214016},"max":{"secs":0,"nanos":282880}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":494387},"median":{"secs":0,"nanos":401920},"variance":{"secs":0,"nanos":74},"min":{"secs":0,"nanos":399616},"max":{"secs":0,"nanos":1312000}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":748441},"median":{"secs":0,"nanos":743936},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":739072},"max":{"secs":0,"nanos":775680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":938828},"median":{"secs":0,"nanos":936192},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":925440},"max":{"secs":0,"nanos":973056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1668710},"median":{"secs":0,"nanos":1636608},"variance":{"secs":0,"nanos":8},"min":{"secs":0,"nanos":1615104},"max":{"secs":0,"nanos":1944576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7832806},"median":{"secs":0,"nanos":7175680},"variance":{"secs":0,"nanos":1900},"min":{"secs":0,"nanos":6930688},"max":{"secs":0,"nanos":10790912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10845977},"median":{"secs":0,"nanos":10492416},"variance":{"secs":0,"nanos":7862},"min":{"secs":0,"nanos":7048448},"max":{"secs":0,"nanos":16683008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":453452},"median":{"secs":0,"nanos":450816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":446720},"max":{"secs":0,"nanos":468224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":518681},"median":{"secs":0,"nanos":521472},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":502528},"max":{"secs":0,"nanos":538368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":644147},"median":{"secs":0,"nanos":604672},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":542208},"max":{"secs":0,"nanos":1003520}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":333900},"median":{"secs":0,"nanos":313600},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":307456},"max":{"secs":0,"nanos":509696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":670976},"median":{"secs":0,"nanos":669696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":667904},"max":{"secs":0,"nanos":680704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5077836},"median":{"secs":0,"nanos":4787712},"variance":{"secs":0,"nanos":1168},"min":{"secs":0,"nanos":4308480},"max":{"secs":0,"nanos":8066560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":9272499},"median":{"secs":0,"nanos":8961792},"variance":{"secs":0,"nanos":880},"min":{"secs":0,"nanos":8420096},"max":{"secs":0,"nanos":11757568}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":769561},"median":{"secs":0,"nanos":769536},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":765696},"max":{"secs":0,"nanos":772864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":1814016},"median":{"secs":0,"nanos":1806336},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1771520},"max":{"secs":0,"nanos":1937408}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":514432},"median":{"secs":0,"nanos":512256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":509952},"max":{"secs":0,"nanos":521984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":645785},"median":{"secs":0,"nanos":645376},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":643072},"max":{"secs":0,"nanos":652544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2415795},"median":{"secs":0,"nanos":2407424},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":2404864},"max":{"secs":0,"nanos":2450688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4591257},"median":{"secs":0,"nanos":4217856},"variance":{"secs":0,"nanos":726},"min":{"secs":0,"nanos":4210688},"max":{"secs":0,"nanos":6996480}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":259840},"median":{"secs":0,"nanos":238848},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":238080},"max":{"secs":0,"nanos":449024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":367539},"median":{"secs":0,"nanos":367616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":366080},"max":{"secs":0,"nanos":370688}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":85760},"median":{"secs":0,"nanos":85760},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":84736},"max":{"secs":0,"nanos":87296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":123136},"median":{"secs":0,"nanos":123136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":122624},"max":{"secs":0,"nanos":124928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":791936},"median":{"secs":0,"nanos":775936},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":773120},"max":{"secs":0,"nanos":936448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1359974},"median":{"secs":0,"nanos":1360128},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":1358080},"max":{"secs":0,"nanos":1361408}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":220492},"median":{"secs":0,"nanos":220416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":218112},"max":{"secs":0,"nanos":223488}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":238003},"median":{"secs":0,"nanos":237824},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":237056},"max":{"secs":0,"nanos":239616}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":2,"n":32,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":true}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":92518},"median":{"secs":0,"nanos":92672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":91648},"max":{"secs":0,"nanos":93696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":228659},"median":{"secs":0,"nanos":228352},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":228096},"max":{"secs":0,"nanos":230656}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":2,"n":32,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9113},"median":{"secs":0,"nanos":8960},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8192},"max":{"secs":0,"nanos":12288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":51456},"median":{"secs":0,"nanos":51456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50944},"max":{"secs":0,"nanos":52224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":32,"k":128,"lhs_pow2_factor":1,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":198476},"median":{"secs":0,"nanos":198144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":196608},"max":{"secs":0,"nanos":200960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":236339},"median":{"secs":0,"nanos":236288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":235776},"max":{"secs":0,"nanos":237312}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
