{"key":{"key":{"definition":{"m":16384,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":78924},"median":{"secs":0,"nanos":78848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":77824},"max":{"secs":0,"nanos":79872}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84838},"median":{"secs":0,"nanos":83200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81920},"max":{"secs":0,"nanos":89856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":127513},"median":{"secs":0,"nanos":127744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126208},"max":{"secs":0,"nanos":128256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1813376},"median":{"secs":0,"nanos":2008576},"variance":{"secs":0,"nanos":277},"min":{"secs":0,"nanos":776448},"max":{"secs":0,"nanos":2255104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3172121},"median":{"secs":0,"nanos":3086592},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":2596352},"max":{"secs":0,"nanos":4029696}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":47513},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46592},"max":{"secs":0,"nanos":49152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48640},"median":{"secs":0,"nanos":48640},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":87270},"median":{"secs":0,"nanos":87040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":86016},"max":{"secs":0,"nanos":88576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":369971},"median":{"secs":0,"nanos":370176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368896},"max":{"secs":0,"nanos":371456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1455641},"median":{"secs":0,"nanos":1934592},"variance":{"secs":0,"nanos":425},"min":{"secs":0,"nanos":657920},"max":{"secs":0,"nanos":2091264}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241100},"median":{"secs":0,"nanos":241152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":239360},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":258713},"median":{"secs":0,"nanos":258816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":257792},"max":{"secs":0,"nanos":260352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":926848},"median":{"secs":0,"nanos":446976},"variance":{"secs":0,"nanos":566},"min":{"secs":0,"nanos":443904},"max":{"secs":0,"nanos":2368512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":9396428},"median":{"secs":0,"nanos":9568256},"variance":{"secs":0,"nanos":253},"min":{"secs":0,"nanos":8057856},"max":{"secs":0,"nanos":10051072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12117401},"median":{"secs":0,"nanos":12157696},"variance":{"secs":0,"nanos":48},"min":{"secs":0,"nanos":11742720},"max":{"secs":0,"nanos":12550144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":131865},"median":{"secs":0,"nanos":132096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":133888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":157004},"median":{"secs":0,"nanos":156672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":156416},"max":{"secs":0,"nanos":159232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":299315},"median":{"secs":0,"nanos":299264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":298240},"max":{"secs":0,"nanos":300800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4648089},"median":{"secs":0,"nanos":4743168},"variance":{"secs":0,"nanos":330},"min":{"secs":0,"nanos":3098624},"max":{"secs":0,"nanos":5299200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5799168},"median":{"secs":0,"nanos":5455104},"variance":{"secs":0,"nanos":367},"min":{"secs":0,"nanos":5152512},"max":{"secs":0,"nanos":6633984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1475686},"median":{"secs":0,"nanos":1986304},"variance":{"secs":0,"nanos":559},"min":{"secs":0,"nanos":703744},"max":{"secs":0,"nanos":2598400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1749657},"median":{"secs":0,"nanos":2099712},"variance":{"secs":0,"nanos":401},"min":{"secs":0,"nanos":787200},"max":{"secs":0,"nanos":2355456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1802905},"median":{"secs":0,"nanos":2217728},"variance":{"secs":0,"nanos":454},"min":{"secs":0,"nanos":980992},"max":{"secs":0,"nanos":2553344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":30080076},"median":{"secs":0,"nanos":29821184},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":29470208},"max":{"secs":0,"nanos":31312896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":36225868},"median":{"secs":0,"nanos":36247552},"variance":{"secs":0,"nanos":72},"min":{"secs":0,"nanos":35538688},"max":{"secs":0,"nanos":36578304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":589824},"median":{"secs":0,"nanos":445184},"variance":{"secs":0,"nanos":188},"min":{"secs":0,"nanos":442880},"max":{"secs":0,"nanos":1893376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1064960},"median":{"secs":0,"nanos":496640},"variance":{"secs":0,"nanos":504},"min":{"secs":0,"nanos":493568},"max":{"secs":0,"nanos":2270720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1264742},"median":{"secs":0,"nanos":1919232},"variance":{"secs":0,"nanos":545},"min":{"secs":0,"nanos":525312},"max":{"secs":0,"nanos":2094592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":14605619},"median":{"secs":0,"nanos":14573568},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":14064128},"max":{"secs":0,"nanos":15123456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17460556},"median":{"secs":0,"nanos":17382912},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":16971264},"max":{"secs":0,"nanos":18785792}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1322649},"median":{"secs":0,"nanos":1814784},"variance":{"secs":0,"nanos":495},"min":{"secs":0,"nanos":621056},"max":{"secs":0,"nanos":2107136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1333196},"median":{"secs":0,"nanos":1818880},"variance":{"secs":0,"nanos":491},"min":{"secs":0,"nanos":636416},"max":{"secs":0,"nanos":2146304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2547737},"median":{"secs":0,"nanos":2468608},"variance":{"secs":0,"nanos":90},"min":{"secs":0,"nanos":2291200},"max":{"secs":0,"nanos":3327744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27955},"median":{"secs":0,"nanos":28160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":28672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99865},"median":{"secs":0,"nanos":101888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":93696},"max":{"secs":0,"nanos":103168}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":13798},"median":{"secs":0,"nanos":14080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53222},"median":{"secs":0,"nanos":54784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49920},"max":{"secs":0,"nanos":56576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6297},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":7680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11571},"median":{"secs":0,"nanos":12032},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":13824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20326},"median":{"secs":0,"nanos":20736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":22016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":32256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9600},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":11008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":29875},"median":{"secs":0,"nanos":30720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27136},"max":{"secs":0,"nanos":32256}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20070},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18944},"max":{"secs":0,"nanos":21760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":33587},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30208},"max":{"secs":0,"nanos":35328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":54860},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50432},"max":{"secs":0,"nanos":58368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":62438},"median":{"secs":0,"nanos":63232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":57856},"max":{"secs":0,"nanos":65024}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23859},"median":{"secs":0,"nanos":23808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23296},"max":{"secs":0,"nanos":25088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":32588},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":29696},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84659},"median":{"secs":0,"nanos":84480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":83456},"max":{"secs":0,"nanos":87040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":95820},"median":{"secs":0,"nanos":96000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":94976},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":106035},"median":{"secs":0,"nanos":105728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104960},"max":{"secs":0,"nanos":108288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1168512},"median":{"secs":0,"nanos":590592},"variance":{"secs":0,"nanos":513},"min":{"secs":0,"nanos":585472},"max":{"secs":0,"nanos":2264576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1024358},"median":{"secs":0,"nanos":652544},"variance":{"secs":0,"nanos":404},"min":{"secs":0,"nanos":580096},"max":{"secs":0,"nanos":2198784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":33049},"median":{"secs":0,"nanos":33280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":33792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49536},"median":{"secs":0,"nanos":49664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":51456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":60697},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":68608}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3159936},"median":{"secs":0,"nanos":2837504},"variance":{"secs":0,"nanos":549},"min":{"secs":0,"nanos":2586368},"max":{"secs":0,"nanos":4791296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3912883},"median":{"secs":0,"nanos":4368384},"variance":{"secs":0,"nanos":620},"min":{"secs":0,"nanos":2815744},"max":{"secs":0,"nanos":4789504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4364313},"median":{"secs":0,"nanos":4728320},"variance":{"secs":0,"nanos":418},"min":{"secs":0,"nanos":3090688},"max":{"secs":0,"nanos":4894208}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":663577},"median":{"secs":0,"nanos":396800},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":390656},"max":{"secs":0,"nanos":1881088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":477593},"median":{"secs":0,"nanos":477440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":475904},"max":{"secs":0,"nanos":481792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1051648},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":491776},"max":{"secs":0,"nanos":1971712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4388812},"median":{"secs":0,"nanos":4470272},"variance":{"secs":0,"nanos":105},"min":{"secs":0,"nanos":3484416},"max":{"secs":0,"nanos":4717056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4677811},"median":{"secs":0,"nanos":4743936},"variance":{"secs":0,"nanos":87},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5408179},"median":{"secs":0,"nanos":5401088},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":5086720},"max":{"secs":0,"nanos":5629952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":89216230},"median":{"secs":0,"nanos":89440256},"variance":{"secs":0,"nanos":360},"min":{"secs":0,"nanos":88281600},"max":{"secs":0,"nanos":89995776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":90050073},"median":{"secs":0,"nanos":90768640},"variance":{"secs":0,"nanos":3784},"min":{"secs":0,"nanos":85283840},"max":{"secs":0,"nanos":92812544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2846592},"median":{"secs":0,"nanos":2911744},"variance":{"secs":0,"nanos":38},"min":{"secs":0,"nanos":2603776},"max":{"secs":0,"nanos":3213824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3134438},"median":{"secs":0,"nanos":3144704},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":2890240},"max":{"secs":0,"nanos":3370752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4747571},"median":{"secs":0,"nanos":4781824},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":4491008},"max":{"secs":0,"nanos":5148672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12969395},"median":{"secs":0,"nanos":12839424},"variance":{"secs":0,"nanos":482},"min":{"secs":0,"nanos":11920640},"max":{"secs":0,"nanos":14241536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":12890009},"median":{"secs":0,"nanos":12916736},"variance":{"secs":0,"nanos":402},"min":{"secs":0,"nanos":12069888},"max":{"secs":0,"nanos":14033664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":457395},"median":{"secs":0,"nanos":459008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":448256},"max":{"secs":0,"nanos":466432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1108249},"median":{"secs":0,"nanos":567808},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":556032},"max":{"secs":0,"nanos":2232320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1257881},"median":{"secs":0,"nanos":1859072},"variance":{"secs":0,"nanos":500},"min":{"secs":0,"nanos":551168},"max":{"secs":0,"nanos":2175488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":154393},"median":{"secs":0,"nanos":154112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152576},"max":{"secs":0,"nanos":157184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":171008},"median":{"secs":0,"nanos":170752},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":169984},"max":{"secs":0,"nanos":174080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":176844},"median":{"secs":0,"nanos":177152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175104},"max":{"secs":0,"nanos":178944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2747673},"median":{"secs":0,"nanos":2727424},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":2589952},"max":{"secs":0,"nanos":2972928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3355545},"median":{"secs":0,"nanos":3318784},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":3174144},"max":{"secs":0,"nanos":3715584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4769228},"median":{"secs":0,"nanos":4724992},"variance":{"secs":0,"nanos":23},"min":{"secs":0,"nanos":4530176},"max":{"secs":0,"nanos":5030144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":22860083},"median":{"secs":0,"nanos":22610432},"variance":{"secs":0,"nanos":692},"min":{"secs":0,"nanos":22076928},"max":{"secs":0,"nanos":24517888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":23029324},"median":{"secs":0,"nanos":23006208},"variance":{"secs":0,"nanos":351},"min":{"secs":0,"nanos":22123776},"max":{"secs":0,"nanos":23974144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2768409},"median":{"secs":0,"nanos":2835200},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2596096},"max":{"secs":0,"nanos":2918912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2956953},"median":{"secs":0,"nanos":2889984},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":2803456},"max":{"secs":0,"nanos":3351040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3377254},"median":{"secs":0,"nanos":3007232},"variance":{"secs":0,"nanos":408},"min":{"secs":0,"nanos":2758400},"max":{"secs":0,"nanos":4370176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3361459},"median":{"secs":0,"nanos":3155200},"variance":{"secs":0,"nanos":386},"min":{"secs":0,"nanos":2711552},"max":{"secs":0,"nanos":4545024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4098585},"median":{"secs":0,"nanos":4202496},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":3863552},"max":{"secs":0,"nanos":4319232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2723609},"median":{"secs":0,"nanos":2778112},"variance":{"secs":0,"nanos":9},"min":{"secs":0,"nanos":2591744},"max":{"secs":0,"nanos":2860032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2994636},"median":{"secs":0,"nanos":2810368},"variance":{"secs":0,"nanos":242},"min":{"secs":0,"nanos":2573824},"max":{"secs":0,"nanos":4035584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3095347},"median":{"secs":0,"nanos":2846720},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":2555904},"max":{"secs":0,"nanos":4527360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2918297},"median":{"secs":0,"nanos":3033088},"variance":{"secs":0,"nanos":178},"min":{"secs":0,"nanos":1715712},"max":{"secs":0,"nanos":3332352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4619648},"median":{"secs":0,"nanos":4735744},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":4385024},"max":{"secs":0,"nanos":4851712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29465},"median":{"secs":0,"nanos":31744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19968},"max":{"secs":0,"nanos":32256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":35584},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":39680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":48486},"median":{"secs":0,"nanos":50176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":41984},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":151884},"median":{"secs":0,"nanos":151808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":148992},"max":{"secs":0,"nanos":156160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":265267},"median":{"secs":0,"nanos":265984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":262912},"max":{"secs":0,"nanos":267008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":67865},"median":{"secs":0,"nanos":68096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":67072},"max":{"secs":0,"nanos":68608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":85760},"median":{"secs":0,"nanos":86016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":84992},"max":{"secs":0,"nanos":86528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":153395},"median":{"secs":0,"nanos":153600},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152320},"max":{"secs":0,"nanos":154112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1614310},"median":{"secs":0,"nanos":1999104},"variance":{"secs":0,"nanos":523},"min":{"secs":0,"nanos":746496},"max":{"secs":0,"nanos":2496768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2211968},"median":{"secs":0,"nanos":2457856},"variance":{"secs":0,"nanos":411},"min":{"secs":0,"nanos":977152},"max":{"secs":0,"nanos":2748928}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":171699},"median":{"secs":0,"nanos":171520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":170752},"max":{"secs":0,"nanos":173568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":203161},"median":{"secs":0,"nanos":203264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":201984},"max":{"secs":0,"nanos":204032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":265164},"median":{"secs":0,"nanos":265216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":264704},"max":{"secs":0,"nanos":265728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5617945},"median":{"secs":0,"nanos":5809408},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":4139264},"max":{"secs":0,"nanos":6855680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6383744},"median":{"secs":0,"nanos":7002368},"variance":{"secs":0,"nanos":2283},"min":{"secs":0,"nanos":3175936},"max":{"secs":0,"nanos":8116736}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":854041},"median":{"secs":0,"nanos":588032},"variance":{"secs":0,"nanos":272},"min":{"secs":0,"nanos":586240},"max":{"secs":0,"nanos":2317056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1432345},"median":{"secs":0,"nanos":1067520},"variance":{"secs":0,"nanos":481},"min":{"secs":0,"nanos":1064960},"max":{"secs":0,"nanos":2841600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1408563},"median":{"secs":0,"nanos":1902848},"variance":{"secs":0,"nanos":627},"min":{"secs":0,"nanos":624896},"max":{"secs":0,"nanos":2342400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17843},"median":{"secs":0,"nanos":19456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":20736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":96358},"median":{"secs":0,"nanos":99584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90624},"max":{"secs":0,"nanos":99840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":11161},"median":{"secs":0,"nanos":12544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":13312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49433},"median":{"secs":0,"nanos":50688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":52224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4761},"median":{"secs":0,"nanos":4864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":3840},"max":{"secs":0,"nanos":5376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10342},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11392},"median":{"secs":0,"nanos":11520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11008},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":15718},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12032},"max":{"secs":0,"nanos":18176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9779},"median":{"secs":0,"nanos":10240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":10496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16512},"median":{"secs":0,"nanos":16128},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":18688}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":12211},"median":{"secs":0,"nanos":12288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11264},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":16307},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":17920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24755},"median":{"secs":0,"nanos":23296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":28928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":56883},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52992},"max":{"secs":0,"nanos":59392}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15078},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14592},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18764},"median":{"secs":0,"nanos":18432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17152},"max":{"secs":0,"nanos":20480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":81382},"median":{"secs":0,"nanos":81664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":80384},"max":{"secs":0,"nanos":81920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":96102},"median":{"secs":0,"nanos":96256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95232},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":103552},"median":{"secs":0,"nanos":103424},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102912},"max":{"secs":0,"nanos":104192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":230092},"median":{"secs":0,"nanos":230144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":229120},"max":{"secs":0,"nanos":230912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":235468},"median":{"secs":0,"nanos":235520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":234496},"max":{"secs":0,"nanos":236288}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":27008},"median":{"secs":0,"nanos":27136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26112},"max":{"secs":0,"nanos":27648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":35788},"median":{"secs":0,"nanos":35584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35328},"max":{"secs":0,"nanos":36608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48716},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46336},"max":{"secs":0,"nanos":53760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1744870},"median":{"secs":0,"nanos":2263040},"variance":{"secs":0,"nanos":781},"min":{"secs":0,"nanos":699648},"max":{"secs":0,"nanos":2929152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2141081},"median":{"secs":0,"nanos":2369280},"variance":{"secs":0,"nanos":456},"min":{"secs":0,"nanos":866304},"max":{"secs":0,"nanos":2792192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2445542},"median":{"secs":0,"nanos":2424320},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":2179584},"max":{"secs":0,"nanos":2655232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4096,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":287923},"median":{"secs":0,"nanos":287744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":286208},"max":{"secs":0,"nanos":291840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":292531},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288512},"max":{"secs":0,"nanos":293888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":314700},"median":{"secs":0,"nanos":316160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":297728},"max":{"secs":0,"nanos":322048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2858675},"median":{"secs":0,"nanos":2860032},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":2631424},"max":{"secs":0,"nanos":3207936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3077504},"median":{"secs":0,"nanos":3134976},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":2892032},"max":{"secs":0,"nanos":3203840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3393510},"median":{"secs":0,"nanos":3485696},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":3169280},"max":{"secs":0,"nanos":3584768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":46301209},"median":{"secs":0,"nanos":46567680},"variance":{"secs":0,"nanos":917},"min":{"secs":0,"nanos":45009152},"max":{"secs":0,"nanos":47776256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":48641536},"median":{"secs":0,"nanos":49084928},"variance":{"secs":0,"nanos":897},"min":{"secs":0,"nanos":47056896},"max":{"secs":0,"nanos":49919488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1511398},"median":{"secs":0,"nanos":2010112},"variance":{"secs":0,"nanos":498},"min":{"secs":0,"nanos":811264},"max":{"secs":0,"nanos":2388224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1580953},"median":{"secs":0,"nanos":2100736},"variance":{"secs":0,"nanos":484},"min":{"secs":0,"nanos":894208},"max":{"secs":0,"nanos":2541312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2823014},"median":{"secs":0,"nanos":2794240},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":2614272},"max":{"secs":0,"nanos":3104768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7104921},"median":{"secs":0,"nanos":7248640},"variance":{"secs":0,"nanos":527},"min":{"secs":0,"nanos":5942784},"max":{"secs":0,"nanos":8421632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":7807820},"median":{"secs":0,"nanos":7682304},"variance":{"secs":0,"nanos":329},"min":{"secs":0,"nanos":7330048},"max":{"secs":0,"nanos":9185536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241228},"median":{"secs":0,"nanos":242176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":237056},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":284313},"median":{"secs":0,"nanos":284160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":283136},"max":{"secs":0,"nanos":286464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":291609},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":287232},"max":{"secs":0,"nanos":294144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4096,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":123417},"median":{"secs":0,"nanos":123136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":121600},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":131097},"median":{"secs":0,"nanos":130816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":132352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":135321},"median":{"secs":0,"nanos":138240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126976},"max":{"secs":0,"nanos":139776}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1374924},"median":{"secs":0,"nanos":1930240},"variance":{"secs":0,"nanos":447},"min":{"secs":0,"nanos":709888},"max":{"secs":0,"nanos":2182400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1608448},"median":{"secs":0,"nanos":2105088},"variance":{"secs":0,"nanos":487},"min":{"secs":0,"nanos":913408},"max":{"secs":0,"nanos":2474240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3066265},"median":{"secs":0,"nanos":3121664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":2834688},"max":{"secs":0,"nanos":3227904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":11702323},"median":{"secs":0,"nanos":12060928},"variance":{"secs":0,"nanos":740},"min":{"secs":0,"nanos":10090240},"max":{"secs":0,"nanos":12796416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12374656},"median":{"secs":0,"nanos":12242688},"variance":{"secs":0,"nanos":279},"min":{"secs":0,"nanos":11604992},"max":{"secs":0,"nanos":13507584}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1454131},"median":{"secs":0,"nanos":1996544},"variance":{"secs":0,"nanos":430},"min":{"secs":0,"nanos":798208},"max":{"secs":0,"nanos":2208256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1685990},"median":{"secs":0,"nanos":2030848},"variance":{"secs":0,"nanos":497},"min":{"secs":0,"nanos":817920},"max":{"secs":0,"nanos":2525184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1548697},"median":{"secs":0,"nanos":2036480},"variance":{"secs":0,"nanos":489},"min":{"secs":0,"nanos":856064},"max":{"secs":0,"nanos":2447104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2235340},"median":{"secs":0,"nanos":2341376},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":879104},"max":{"secs":0,"nanos":2958848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2489856},"median":{"secs":0,"nanos":2488320},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":2264832},"max":{"secs":0,"nanos":2931456}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1403494},"median":{"secs":0,"nanos":1913600},"variance":{"secs":0,"nanos":493},"min":{"secs":0,"nanos":714496},"max":{"secs":0,"nanos":2437888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1716454},"median":{"secs":0,"nanos":2025472},"variance":{"secs":0,"nanos":460},"min":{"secs":0,"nanos":704000},"max":{"secs":0,"nanos":2404608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1742336},"median":{"secs":0,"nanos":2047488},"variance":{"secs":0,"nanos":520},"min":{"secs":0,"nanos":684544},"max":{"secs":0,"nanos":2651904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1674419},"median":{"secs":0,"nanos":2202112},"variance":{"secs":0,"nanos":669},"min":{"secs":0,"nanos":866560},"max":{"secs":0,"nanos":2809344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3120486},"median":{"secs":0,"nanos":3109120},"variance":{"secs":0,"nanos":63},"min":{"secs":0,"nanos":2810624},"max":{"secs":0,"nanos":3551744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":141696},"median":{"secs":0,"nanos":148224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":149760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":149785},"median":{"secs":0,"nanos":156928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":137984},"max":{"secs":0,"nanos":157696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1397478},"median":{"secs":0,"nanos":1858304},"variance":{"secs":0,"nanos":412},"min":{"secs":0,"nanos":762368},"max":{"secs":0,"nanos":2256640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1442457},"median":{"secs":0,"nanos":1890048},"variance":{"secs":0,"nanos":446},"min":{"secs":0,"nanos":793856},"max":{"secs":0,"nanos":2423808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2390169},"median":{"secs":0,"nanos":2469632},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":2235136},"max":{"secs":0,"nanos":2723328}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
