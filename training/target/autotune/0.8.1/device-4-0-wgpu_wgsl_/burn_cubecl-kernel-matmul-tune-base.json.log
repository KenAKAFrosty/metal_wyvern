{"key":{"key":{"definition":{"m":16384,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":78924},"median":{"secs":0,"nanos":78848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":77824},"max":{"secs":0,"nanos":79872}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84838},"median":{"secs":0,"nanos":83200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81920},"max":{"secs":0,"nanos":89856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":127513},"median":{"secs":0,"nanos":127744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126208},"max":{"secs":0,"nanos":128256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1813376},"median":{"secs":0,"nanos":2008576},"variance":{"secs":0,"nanos":277},"min":{"secs":0,"nanos":776448},"max":{"secs":0,"nanos":2255104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3172121},"median":{"secs":0,"nanos":3086592},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":2596352},"max":{"secs":0,"nanos":4029696}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":47513},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46592},"max":{"secs":0,"nanos":49152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48640},"median":{"secs":0,"nanos":48640},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":87270},"median":{"secs":0,"nanos":87040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":86016},"max":{"secs":0,"nanos":88576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":369971},"median":{"secs":0,"nanos":370176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368896},"max":{"secs":0,"nanos":371456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1455641},"median":{"secs":0,"nanos":1934592},"variance":{"secs":0,"nanos":425},"min":{"secs":0,"nanos":657920},"max":{"secs":0,"nanos":2091264}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241100},"median":{"secs":0,"nanos":241152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":239360},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":258713},"median":{"secs":0,"nanos":258816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":257792},"max":{"secs":0,"nanos":260352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":926848},"median":{"secs":0,"nanos":446976},"variance":{"secs":0,"nanos":566},"min":{"secs":0,"nanos":443904},"max":{"secs":0,"nanos":2368512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":9396428},"median":{"secs":0,"nanos":9568256},"variance":{"secs":0,"nanos":253},"min":{"secs":0,"nanos":8057856},"max":{"secs":0,"nanos":10051072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12117401},"median":{"secs":0,"nanos":12157696},"variance":{"secs":0,"nanos":48},"min":{"secs":0,"nanos":11742720},"max":{"secs":0,"nanos":12550144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":131865},"median":{"secs":0,"nanos":132096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":133888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":157004},"median":{"secs":0,"nanos":156672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":156416},"max":{"secs":0,"nanos":159232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":299315},"median":{"secs":0,"nanos":299264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":298240},"max":{"secs":0,"nanos":300800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4648089},"median":{"secs":0,"nanos":4743168},"variance":{"secs":0,"nanos":330},"min":{"secs":0,"nanos":3098624},"max":{"secs":0,"nanos":5299200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5799168},"median":{"secs":0,"nanos":5455104},"variance":{"secs":0,"nanos":367},"min":{"secs":0,"nanos":5152512},"max":{"secs":0,"nanos":6633984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1475686},"median":{"secs":0,"nanos":1986304},"variance":{"secs":0,"nanos":559},"min":{"secs":0,"nanos":703744},"max":{"secs":0,"nanos":2598400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1749657},"median":{"secs":0,"nanos":2099712},"variance":{"secs":0,"nanos":401},"min":{"secs":0,"nanos":787200},"max":{"secs":0,"nanos":2355456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1802905},"median":{"secs":0,"nanos":2217728},"variance":{"secs":0,"nanos":454},"min":{"secs":0,"nanos":980992},"max":{"secs":0,"nanos":2553344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":30080076},"median":{"secs":0,"nanos":29821184},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":29470208},"max":{"secs":0,"nanos":31312896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":36225868},"median":{"secs":0,"nanos":36247552},"variance":{"secs":0,"nanos":72},"min":{"secs":0,"nanos":35538688},"max":{"secs":0,"nanos":36578304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":589824},"median":{"secs":0,"nanos":445184},"variance":{"secs":0,"nanos":188},"min":{"secs":0,"nanos":442880},"max":{"secs":0,"nanos":1893376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1064960},"median":{"secs":0,"nanos":496640},"variance":{"secs":0,"nanos":504},"min":{"secs":0,"nanos":493568},"max":{"secs":0,"nanos":2270720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1264742},"median":{"secs":0,"nanos":1919232},"variance":{"secs":0,"nanos":545},"min":{"secs":0,"nanos":525312},"max":{"secs":0,"nanos":2094592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":14605619},"median":{"secs":0,"nanos":14573568},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":14064128},"max":{"secs":0,"nanos":15123456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17460556},"median":{"secs":0,"nanos":17382912},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":16971264},"max":{"secs":0,"nanos":18785792}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1322649},"median":{"secs":0,"nanos":1814784},"variance":{"secs":0,"nanos":495},"min":{"secs":0,"nanos":621056},"max":{"secs":0,"nanos":2107136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1333196},"median":{"secs":0,"nanos":1818880},"variance":{"secs":0,"nanos":491},"min":{"secs":0,"nanos":636416},"max":{"secs":0,"nanos":2146304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2547737},"median":{"secs":0,"nanos":2468608},"variance":{"secs":0,"nanos":90},"min":{"secs":0,"nanos":2291200},"max":{"secs":0,"nanos":3327744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27955},"median":{"secs":0,"nanos":28160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":28672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99865},"median":{"secs":0,"nanos":101888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":93696},"max":{"secs":0,"nanos":103168}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":13798},"median":{"secs":0,"nanos":14080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53222},"median":{"secs":0,"nanos":54784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49920},"max":{"secs":0,"nanos":56576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6297},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":7680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11571},"median":{"secs":0,"nanos":12032},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":13824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20326},"median":{"secs":0,"nanos":20736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":22016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":32256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9600},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":11008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":29875},"median":{"secs":0,"nanos":30720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27136},"max":{"secs":0,"nanos":32256}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20070},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18944},"max":{"secs":0,"nanos":21760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":33587},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30208},"max":{"secs":0,"nanos":35328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":54860},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50432},"max":{"secs":0,"nanos":58368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":62438},"median":{"secs":0,"nanos":63232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":57856},"max":{"secs":0,"nanos":65024}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23859},"median":{"secs":0,"nanos":23808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23296},"max":{"secs":0,"nanos":25088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":32588},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":29696},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84659},"median":{"secs":0,"nanos":84480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":83456},"max":{"secs":0,"nanos":87040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":95820},"median":{"secs":0,"nanos":96000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":94976},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":106035},"median":{"secs":0,"nanos":105728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104960},"max":{"secs":0,"nanos":108288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1168512},"median":{"secs":0,"nanos":590592},"variance":{"secs":0,"nanos":513},"min":{"secs":0,"nanos":585472},"max":{"secs":0,"nanos":2264576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1024358},"median":{"secs":0,"nanos":652544},"variance":{"secs":0,"nanos":404},"min":{"secs":0,"nanos":580096},"max":{"secs":0,"nanos":2198784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":33049},"median":{"secs":0,"nanos":33280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":33792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49536},"median":{"secs":0,"nanos":49664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":51456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":60697},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":68608}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3159936},"median":{"secs":0,"nanos":2837504},"variance":{"secs":0,"nanos":549},"min":{"secs":0,"nanos":2586368},"max":{"secs":0,"nanos":4791296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3912883},"median":{"secs":0,"nanos":4368384},"variance":{"secs":0,"nanos":620},"min":{"secs":0,"nanos":2815744},"max":{"secs":0,"nanos":4789504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4364313},"median":{"secs":0,"nanos":4728320},"variance":{"secs":0,"nanos":418},"min":{"secs":0,"nanos":3090688},"max":{"secs":0,"nanos":4894208}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":663577},"median":{"secs":0,"nanos":396800},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":390656},"max":{"secs":0,"nanos":1881088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":477593},"median":{"secs":0,"nanos":477440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":475904},"max":{"secs":0,"nanos":481792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1051648},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":491776},"max":{"secs":0,"nanos":1971712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4388812},"median":{"secs":0,"nanos":4470272},"variance":{"secs":0,"nanos":105},"min":{"secs":0,"nanos":3484416},"max":{"secs":0,"nanos":4717056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4677811},"median":{"secs":0,"nanos":4743936},"variance":{"secs":0,"nanos":87},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5408179},"median":{"secs":0,"nanos":5401088},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":5086720},"max":{"secs":0,"nanos":5629952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":89216230},"median":{"secs":0,"nanos":89440256},"variance":{"secs":0,"nanos":360},"min":{"secs":0,"nanos":88281600},"max":{"secs":0,"nanos":89995776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":90050073},"median":{"secs":0,"nanos":90768640},"variance":{"secs":0,"nanos":3784},"min":{"secs":0,"nanos":85283840},"max":{"secs":0,"nanos":92812544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2846592},"median":{"secs":0,"nanos":2911744},"variance":{"secs":0,"nanos":38},"min":{"secs":0,"nanos":2603776},"max":{"secs":0,"nanos":3213824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3134438},"median":{"secs":0,"nanos":3144704},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":2890240},"max":{"secs":0,"nanos":3370752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4747571},"median":{"secs":0,"nanos":4781824},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":4491008},"max":{"secs":0,"nanos":5148672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12969395},"median":{"secs":0,"nanos":12839424},"variance":{"secs":0,"nanos":482},"min":{"secs":0,"nanos":11920640},"max":{"secs":0,"nanos":14241536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":12890009},"median":{"secs":0,"nanos":12916736},"variance":{"secs":0,"nanos":402},"min":{"secs":0,"nanos":12069888},"max":{"secs":0,"nanos":14033664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":457395},"median":{"secs":0,"nanos":459008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":448256},"max":{"secs":0,"nanos":466432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1108249},"median":{"secs":0,"nanos":567808},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":556032},"max":{"secs":0,"nanos":2232320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1257881},"median":{"secs":0,"nanos":1859072},"variance":{"secs":0,"nanos":500},"min":{"secs":0,"nanos":551168},"max":{"secs":0,"nanos":2175488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":154393},"median":{"secs":0,"nanos":154112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152576},"max":{"secs":0,"nanos":157184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":171008},"median":{"secs":0,"nanos":170752},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":169984},"max":{"secs":0,"nanos":174080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":176844},"median":{"secs":0,"nanos":177152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175104},"max":{"secs":0,"nanos":178944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2747673},"median":{"secs":0,"nanos":2727424},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":2589952},"max":{"secs":0,"nanos":2972928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3355545},"median":{"secs":0,"nanos":3318784},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":3174144},"max":{"secs":0,"nanos":3715584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4769228},"median":{"secs":0,"nanos":4724992},"variance":{"secs":0,"nanos":23},"min":{"secs":0,"nanos":4530176},"max":{"secs":0,"nanos":5030144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":22860083},"median":{"secs":0,"nanos":22610432},"variance":{"secs":0,"nanos":692},"min":{"secs":0,"nanos":22076928},"max":{"secs":0,"nanos":24517888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":23029324},"median":{"secs":0,"nanos":23006208},"variance":{"secs":0,"nanos":351},"min":{"secs":0,"nanos":22123776},"max":{"secs":0,"nanos":23974144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2768409},"median":{"secs":0,"nanos":2835200},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2596096},"max":{"secs":0,"nanos":2918912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2956953},"median":{"secs":0,"nanos":2889984},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":2803456},"max":{"secs":0,"nanos":3351040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3377254},"median":{"secs":0,"nanos":3007232},"variance":{"secs":0,"nanos":408},"min":{"secs":0,"nanos":2758400},"max":{"secs":0,"nanos":4370176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3361459},"median":{"secs":0,"nanos":3155200},"variance":{"secs":0,"nanos":386},"min":{"secs":0,"nanos":2711552},"max":{"secs":0,"nanos":4545024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4098585},"median":{"secs":0,"nanos":4202496},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":3863552},"max":{"secs":0,"nanos":4319232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2723609},"median":{"secs":0,"nanos":2778112},"variance":{"secs":0,"nanos":9},"min":{"secs":0,"nanos":2591744},"max":{"secs":0,"nanos":2860032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2994636},"median":{"secs":0,"nanos":2810368},"variance":{"secs":0,"nanos":242},"min":{"secs":0,"nanos":2573824},"max":{"secs":0,"nanos":4035584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3095347},"median":{"secs":0,"nanos":2846720},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":2555904},"max":{"secs":0,"nanos":4527360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2918297},"median":{"secs":0,"nanos":3033088},"variance":{"secs":0,"nanos":178},"min":{"secs":0,"nanos":1715712},"max":{"secs":0,"nanos":3332352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4619648},"median":{"secs":0,"nanos":4735744},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":4385024},"max":{"secs":0,"nanos":4851712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29465},"median":{"secs":0,"nanos":31744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19968},"max":{"secs":0,"nanos":32256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":35584},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":39680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":48486},"median":{"secs":0,"nanos":50176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":41984},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":151884},"median":{"secs":0,"nanos":151808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":148992},"max":{"secs":0,"nanos":156160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":265267},"median":{"secs":0,"nanos":265984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":262912},"max":{"secs":0,"nanos":267008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":67865},"median":{"secs":0,"nanos":68096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":67072},"max":{"secs":0,"nanos":68608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":85760},"median":{"secs":0,"nanos":86016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":84992},"max":{"secs":0,"nanos":86528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":153395},"median":{"secs":0,"nanos":153600},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152320},"max":{"secs":0,"nanos":154112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1614310},"median":{"secs":0,"nanos":1999104},"variance":{"secs":0,"nanos":523},"min":{"secs":0,"nanos":746496},"max":{"secs":0,"nanos":2496768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2211968},"median":{"secs":0,"nanos":2457856},"variance":{"secs":0,"nanos":411},"min":{"secs":0,"nanos":977152},"max":{"secs":0,"nanos":2748928}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":171699},"median":{"secs":0,"nanos":171520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":170752},"max":{"secs":0,"nanos":173568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":203161},"median":{"secs":0,"nanos":203264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":201984},"max":{"secs":0,"nanos":204032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":265164},"median":{"secs":0,"nanos":265216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":264704},"max":{"secs":0,"nanos":265728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5617945},"median":{"secs":0,"nanos":5809408},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":4139264},"max":{"secs":0,"nanos":6855680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6383744},"median":{"secs":0,"nanos":7002368},"variance":{"secs":0,"nanos":2283},"min":{"secs":0,"nanos":3175936},"max":{"secs":0,"nanos":8116736}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":854041},"median":{"secs":0,"nanos":588032},"variance":{"secs":0,"nanos":272},"min":{"secs":0,"nanos":586240},"max":{"secs":0,"nanos":2317056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1432345},"median":{"secs":0,"nanos":1067520},"variance":{"secs":0,"nanos":481},"min":{"secs":0,"nanos":1064960},"max":{"secs":0,"nanos":2841600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1408563},"median":{"secs":0,"nanos":1902848},"variance":{"secs":0,"nanos":627},"min":{"secs":0,"nanos":624896},"max":{"secs":0,"nanos":2342400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17843},"median":{"secs":0,"nanos":19456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":20736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":96358},"median":{"secs":0,"nanos":99584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90624},"max":{"secs":0,"nanos":99840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":11161},"median":{"secs":0,"nanos":12544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":13312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49433},"median":{"secs":0,"nanos":50688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":52224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4761},"median":{"secs":0,"nanos":4864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":3840},"max":{"secs":0,"nanos":5376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10342},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11392},"median":{"secs":0,"nanos":11520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11008},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":15718},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12032},"max":{"secs":0,"nanos":18176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9779},"median":{"secs":0,"nanos":10240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":10496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16512},"median":{"secs":0,"nanos":16128},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":18688}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":12211},"median":{"secs":0,"nanos":12288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11264},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":16307},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":17920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24755},"median":{"secs":0,"nanos":23296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":28928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":56883},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52992},"max":{"secs":0,"nanos":59392}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15078},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14592},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18764},"median":{"secs":0,"nanos":18432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17152},"max":{"secs":0,"nanos":20480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":81382},"median":{"secs":0,"nanos":81664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":80384},"max":{"secs":0,"nanos":81920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":96102},"median":{"secs":0,"nanos":96256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95232},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":103552},"median":{"secs":0,"nanos":103424},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102912},"max":{"secs":0,"nanos":104192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":230092},"median":{"secs":0,"nanos":230144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":229120},"max":{"secs":0,"nanos":230912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":235468},"median":{"secs":0,"nanos":235520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":234496},"max":{"secs":0,"nanos":236288}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":27008},"median":{"secs":0,"nanos":27136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26112},"max":{"secs":0,"nanos":27648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":35788},"median":{"secs":0,"nanos":35584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35328},"max":{"secs":0,"nanos":36608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48716},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46336},"max":{"secs":0,"nanos":53760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1744870},"median":{"secs":0,"nanos":2263040},"variance":{"secs":0,"nanos":781},"min":{"secs":0,"nanos":699648},"max":{"secs":0,"nanos":2929152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2141081},"median":{"secs":0,"nanos":2369280},"variance":{"secs":0,"nanos":456},"min":{"secs":0,"nanos":866304},"max":{"secs":0,"nanos":2792192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2445542},"median":{"secs":0,"nanos":2424320},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":2179584},"max":{"secs":0,"nanos":2655232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4096,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":287923},"median":{"secs":0,"nanos":287744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":286208},"max":{"secs":0,"nanos":291840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":292531},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288512},"max":{"secs":0,"nanos":293888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":314700},"median":{"secs":0,"nanos":316160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":297728},"max":{"secs":0,"nanos":322048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2858675},"median":{"secs":0,"nanos":2860032},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":2631424},"max":{"secs":0,"nanos":3207936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3077504},"median":{"secs":0,"nanos":3134976},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":2892032},"max":{"secs":0,"nanos":3203840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3393510},"median":{"secs":0,"nanos":3485696},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":3169280},"max":{"secs":0,"nanos":3584768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":46301209},"median":{"secs":0,"nanos":46567680},"variance":{"secs":0,"nanos":917},"min":{"secs":0,"nanos":45009152},"max":{"secs":0,"nanos":47776256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":48641536},"median":{"secs":0,"nanos":49084928},"variance":{"secs":0,"nanos":897},"min":{"secs":0,"nanos":47056896},"max":{"secs":0,"nanos":49919488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1511398},"median":{"secs":0,"nanos":2010112},"variance":{"secs":0,"nanos":498},"min":{"secs":0,"nanos":811264},"max":{"secs":0,"nanos":2388224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1580953},"median":{"secs":0,"nanos":2100736},"variance":{"secs":0,"nanos":484},"min":{"secs":0,"nanos":894208},"max":{"secs":0,"nanos":2541312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2823014},"median":{"secs":0,"nanos":2794240},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":2614272},"max":{"secs":0,"nanos":3104768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7104921},"median":{"secs":0,"nanos":7248640},"variance":{"secs":0,"nanos":527},"min":{"secs":0,"nanos":5942784},"max":{"secs":0,"nanos":8421632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":7807820},"median":{"secs":0,"nanos":7682304},"variance":{"secs":0,"nanos":329},"min":{"secs":0,"nanos":7330048},"max":{"secs":0,"nanos":9185536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241228},"median":{"secs":0,"nanos":242176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":237056},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":284313},"median":{"secs":0,"nanos":284160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":283136},"max":{"secs":0,"nanos":286464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":291609},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":287232},"max":{"secs":0,"nanos":294144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4096,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":123417},"median":{"secs":0,"nanos":123136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":121600},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":131097},"median":{"secs":0,"nanos":130816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":132352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":135321},"median":{"secs":0,"nanos":138240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126976},"max":{"secs":0,"nanos":139776}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1374924},"median":{"secs":0,"nanos":1930240},"variance":{"secs":0,"nanos":447},"min":{"secs":0,"nanos":709888},"max":{"secs":0,"nanos":2182400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1608448},"median":{"secs":0,"nanos":2105088},"variance":{"secs":0,"nanos":487},"min":{"secs":0,"nanos":913408},"max":{"secs":0,"nanos":2474240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3066265},"median":{"secs":0,"nanos":3121664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":2834688},"max":{"secs":0,"nanos":3227904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":11702323},"median":{"secs":0,"nanos":12060928},"variance":{"secs":0,"nanos":740},"min":{"secs":0,"nanos":10090240},"max":{"secs":0,"nanos":12796416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12374656},"median":{"secs":0,"nanos":12242688},"variance":{"secs":0,"nanos":279},"min":{"secs":0,"nanos":11604992},"max":{"secs":0,"nanos":13507584}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1454131},"median":{"secs":0,"nanos":1996544},"variance":{"secs":0,"nanos":430},"min":{"secs":0,"nanos":798208},"max":{"secs":0,"nanos":2208256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1685990},"median":{"secs":0,"nanos":2030848},"variance":{"secs":0,"nanos":497},"min":{"secs":0,"nanos":817920},"max":{"secs":0,"nanos":2525184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1548697},"median":{"secs":0,"nanos":2036480},"variance":{"secs":0,"nanos":489},"min":{"secs":0,"nanos":856064},"max":{"secs":0,"nanos":2447104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2235340},"median":{"secs":0,"nanos":2341376},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":879104},"max":{"secs":0,"nanos":2958848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2489856},"median":{"secs":0,"nanos":2488320},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":2264832},"max":{"secs":0,"nanos":2931456}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1403494},"median":{"secs":0,"nanos":1913600},"variance":{"secs":0,"nanos":493},"min":{"secs":0,"nanos":714496},"max":{"secs":0,"nanos":2437888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1716454},"median":{"secs":0,"nanos":2025472},"variance":{"secs":0,"nanos":460},"min":{"secs":0,"nanos":704000},"max":{"secs":0,"nanos":2404608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1742336},"median":{"secs":0,"nanos":2047488},"variance":{"secs":0,"nanos":520},"min":{"secs":0,"nanos":684544},"max":{"secs":0,"nanos":2651904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1674419},"median":{"secs":0,"nanos":2202112},"variance":{"secs":0,"nanos":669},"min":{"secs":0,"nanos":866560},"max":{"secs":0,"nanos":2809344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3120486},"median":{"secs":0,"nanos":3109120},"variance":{"secs":0,"nanos":63},"min":{"secs":0,"nanos":2810624},"max":{"secs":0,"nanos":3551744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":141696},"median":{"secs":0,"nanos":148224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":149760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":149785},"median":{"secs":0,"nanos":156928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":137984},"max":{"secs":0,"nanos":157696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1397478},"median":{"secs":0,"nanos":1858304},"variance":{"secs":0,"nanos":412},"min":{"secs":0,"nanos":762368},"max":{"secs":0,"nanos":2256640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1442457},"median":{"secs":0,"nanos":1890048},"variance":{"secs":0,"nanos":446},"min":{"secs":0,"nanos":793856},"max":{"secs":0,"nanos":2423808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2390169},"median":{"secs":0,"nanos":2469632},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":2235136},"max":{"secs":0,"nanos":2723328}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4303820},"median":{"secs":0,"nanos":4352768},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":4071936},"max":{"secs":0,"nanos":4536832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4471091},"median":{"secs":0,"nanos":4539904},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":4215040},"max":{"secs":0,"nanos":4889600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6750694},"median":{"secs":0,"nanos":6797312},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":6456576},"max":{"secs":0,"nanos":6977280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":452070},"median":{"secs":0,"nanos":334592},"variance":{"secs":0,"nanos":131},"min":{"secs":0,"nanos":309760},"max":{"secs":0,"nanos":1537280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":530636},"median":{"secs":0,"nanos":394240},"variance":{"secs":0,"nanos":167},"min":{"secs":0,"nanos":393472},"max":{"secs":0,"nanos":1756416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":931174},"median":{"secs":0,"nanos":505856},"variance":{"secs":0,"nanos":352},"min":{"secs":0,"nanos":500992},"max":{"secs":0,"nanos":1889280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6408576},"median":{"secs":0,"nanos":6811136},"variance":{"secs":0,"nanos":486},"min":{"secs":0,"nanos":5229568},"max":{"secs":0,"nanos":7111424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6496025},"median":{"secs":0,"nanos":6904064},"variance":{"secs":0,"nanos":505},"min":{"secs":0,"nanos":5234176},"max":{"secs":0,"nanos":7331840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116812},"median":{"secs":0,"nanos":116736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":114176},"max":{"secs":0,"nanos":120832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":209689},"median":{"secs":0,"nanos":209408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":207872},"max":{"secs":0,"nanos":214016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":222873},"median":{"secs":0,"nanos":223232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":221184},"max":{"secs":0,"nanos":224768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4199270},"median":{"secs":0,"nanos":4310016},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":3977728},"max":{"secs":0,"nanos":4468992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4398438},"median":{"secs":0,"nanos":4438784},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":4174080},"max":{"secs":0,"nanos":4658176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6667136},"median":{"secs":0,"nanos":6691328},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":6347264},"max":{"secs":0,"nanos":7027968}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":308454},"median":{"secs":0,"nanos":303872},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":296192},"max":{"secs":0,"nanos":331520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":507852},"median":{"secs":0,"nanos":388864},"variance":{"secs":0,"nanos":127},"min":{"secs":0,"nanos":387840},"max":{"secs":0,"nanos":1579520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":862924},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":319},"min":{"secs":0,"nanos":492032},"max":{"secs":0,"nanos":1786368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2512588},"median":{"secs":0,"nanos":2562560},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2298368},"max":{"secs":0,"nanos":2671104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2504243},"median":{"secs":0,"nanos":2598912},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":2280704},"max":{"secs":0,"nanos":2753024}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":94284},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88320},"max":{"secs":0,"nanos":109312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":140672},"median":{"secs":0,"nanos":139520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":138752},"max":{"secs":0,"nanos":153344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":144153},"median":{"secs":0,"nanos":143616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":142592},"max":{"secs":0,"nanos":149248}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1093657},"median":{"secs":0,"nanos":742912},"variance":{"secs":0,"nanos":576},"min":{"secs":0,"nanos":686336},"max":{"secs":0,"nanos":3087616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1415398},"median":{"secs":0,"nanos":1443584},"variance":{"secs":0,"nanos":149},"min":{"secs":0,"nanos":911872},"max":{"secs":0,"nanos":2127360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1982233},"median":{"secs":0,"nanos":2361344},"variance":{"secs":0,"nanos":998},"min":{"secs":0,"nanos":629760},"max":{"secs":0,"nanos":3198464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10076851},"median":{"secs":0,"nanos":9892864},"variance":{"secs":0,"nanos":2699},"min":{"secs":0,"nanos":7765760},"max":{"secs":0,"nanos":12916224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":14860697},"median":{"secs":0,"nanos":15601664},"variance":{"secs":0,"nanos":3901},"min":{"secs":0,"nanos":10742272},"max":{"secs":0,"nanos":18316800}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":498227},"median":{"secs":0,"nanos":498176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":497408},"max":{"secs":0,"nanos":499968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1681177},"median":{"secs":0,"nanos":1527040},"variance":{"secs":0,"nanos":113},"min":{"secs":0,"nanos":1404416},"max":{"secs":0,"nanos":2494208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3201024},"median":{"secs":0,"nanos":3132928},"variance":{"secs":0,"nanos":265},"min":{"secs":0,"nanos":2769664},"max":{"secs":0,"nanos":4104704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12033484},"median":{"secs":0,"nanos":12039936},"variance":{"secs":0,"nanos":151},"min":{"secs":0,"nanos":11570944},"max":{"secs":0,"nanos":13067776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21854208},"median":{"secs":0,"nanos":15919360},"variance":{"secs":0,"nanos":181237},"min":{"secs":0,"nanos":8983552},"max":{"secs":0,"nanos":40704512}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1534643},"median":{"secs":0,"nanos":1439488},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":1384448},"max":{"secs":0,"nanos":1799936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1964569},"median":{"secs":0,"nanos":1844480},"variance":{"secs":0,"nanos":45},"min":{"secs":0,"nanos":1843968},"max":{"secs":0,"nanos":2473728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5438182},"median":{"secs":0,"nanos":5225728},"variance":{"secs":0,"nanos":475},"min":{"secs":0,"nanos":4580096},"max":{"secs":0,"nanos":7188224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":35342156},"median":{"secs":0,"nanos":29753600},"variance":{"secs":0,"nanos":294078},"min":{"secs":0,"nanos":27257344},"max":{"secs":0,"nanos":86686208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":34319206},"median":{"secs":0,"nanos":34471424},"variance":{"secs":0,"nanos":236},"min":{"secs":0,"nanos":33640704},"max":{"secs":0,"nanos":35090176}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":17967257},"median":{"secs":0,"nanos":14934784},"variance":{"secs":0,"nanos":26716},"min":{"secs":0,"nanos":12687872},"max":{"secs":0,"nanos":25114368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13458688},"median":{"secs":0,"nanos":15065344},"variance":{"secs":0,"nanos":6803},"min":{"secs":0,"nanos":10482176},"max":{"secs":0,"nanos":16441856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":16014259},"median":{"secs":0,"nanos":16199424},"variance":{"secs":0,"nanos":603},"min":{"secs":0,"nanos":14998528},"max":{"secs":0,"nanos":17384960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":38067},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":36864},"max":{"secs":0,"nanos":40448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":104166},"median":{"secs":0,"nanos":102912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102144},"max":{"secs":0,"nanos":113408}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":39731},"median":{"secs":0,"nanos":29184},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28672},"max":{"secs":0,"nanos":69632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":219136},"median":{"secs":0,"nanos":211712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":198912},"max":{"secs":0,"nanos":240896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":25548},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":37376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":72064},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":54016},"max":{"secs":0,"nanos":104448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":179891},"median":{"secs":0,"nanos":175104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":172032},"max":{"secs":0,"nanos":212736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":433177},"median":{"secs":0,"nanos":299008},"variance":{"secs":0,"nanos":145},"min":{"secs":0,"nanos":294912},"max":{"secs":0,"nanos":1574656}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31001},"median":{"secs":0,"nanos":29696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":48128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":167782},"median":{"secs":0,"nanos":164864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":151040},"max":{"secs":0,"nanos":197376}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":94336},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90880},"max":{"secs":0,"nanos":100352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":198169},"median":{"secs":0,"nanos":187904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":183808},"max":{"secs":0,"nanos":236800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":319001},"median":{"secs":0,"nanos":323840},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":177152},"max":{"secs":0,"nanos":377856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":618905},"median":{"secs":0,"nanos":555520},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":551936},"max":{"secs":0,"nanos":1112064}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":191206},"median":{"secs":0,"nanos":186112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":180224},"max":{"secs":0,"nanos":229376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":340300},"median":{"secs":0,"nanos":319232},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":317440},"max":{"secs":0,"nanos":398592}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1438643},"median":{"secs":0,"nanos":1330176},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":1251840},"max":{"secs":0,"nanos":1670912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5608217},"median":{"secs":0,"nanos":5342720},"variance":{"secs":0,"nanos":2626},"min":{"secs":0,"nanos":3538176},"max":{"secs":0,"nanos":8026368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6190105},"median":{"secs":0,"nanos":6143232},"variance":{"secs":0,"nanos":96},"min":{"secs":0,"nanos":5804544},"max":{"secs":0,"nanos":6816768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6371609},"median":{"secs":0,"nanos":6192640},"variance":{"secs":0,"nanos":3512},"min":{"secs":0,"nanos":4190208},"max":{"secs":0,"nanos":9163264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21803366},"median":{"secs":0,"nanos":6904576},"variance":{"secs":0,"nanos":884883},"min":{"secs":0,"nanos":6030336},"max":{"secs":0,"nanos":94882048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":996121},"median":{"secs":0,"nanos":963584},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":942336},"max":{"secs":0,"nanos":1204736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1830374},"median":{"secs":0,"nanos":1594624},"variance":{"secs":0,"nanos":340},"min":{"secs":0,"nanos":1521664},"max":{"secs":0,"nanos":3479040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2140979},"median":{"secs":0,"nanos":1894656},"variance":{"secs":0,"nanos":393},"min":{"secs":0,"nanos":1652992},"max":{"secs":0,"nanos":3760128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":32768,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3693337},"median":{"secs":0,"nanos":3740928},"variance":{"secs":0,"nanos":67},"min":{"secs":0,"nanos":3436288},"max":{"secs":0,"nanos":4201472}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14573132},"median":{"secs":0,"nanos":16848128},"variance":{"secs":0,"nanos":18786},"min":{"secs":0,"nanos":9231872},"max":{"secs":0,"nanos":19716352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19431116},"median":{"secs":0,"nanos":18908672},"variance":{"secs":0,"nanos":7042},"min":{"secs":0,"nanos":16677888},"max":{"secs":0,"nanos":26026240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3669836},"median":{"secs":0,"nanos":3518464},"variance":{"secs":0,"nanos":609},"min":{"secs":0,"nanos":2843136},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4430361},"median":{"secs":0,"nanos":4444928},"variance":{"secs":0,"nanos":716},"min":{"secs":0,"nanos":3609088},"max":{"secs":0,"nanos":6687232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8967475},"median":{"secs":0,"nanos":9475328},"variance":{"secs":0,"nanos":3323},"min":{"secs":0,"nanos":6718720},"max":{"secs":0,"nanos":11831552}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6429081},"median":{"secs":0,"nanos":6463744},"variance":{"secs":0,"nanos":50},"min":{"secs":0,"nanos":6057472},"max":{"secs":0,"nanos":6874112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6845235},"median":{"secs":0,"nanos":6836224},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":6551040},"max":{"secs":0,"nanos":7118848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8046489},"median":{"secs":0,"nanos":8049664},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":7869952},"max":{"secs":0,"nanos":8253696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":83455462},"median":{"secs":0,"nanos":83596032},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":83105280},"max":{"secs":0,"nanos":83922432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":83787417},"median":{"secs":0,"nanos":83769856},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":83507968},"max":{"secs":0,"nanos":84268800}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3702656},"median":{"secs":0,"nanos":3708928},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":3543552},"max":{"secs":0,"nanos":3963648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":10744550},"median":{"secs":0,"nanos":10586624},"variance":{"secs":0,"nanos":120},"min":{"secs":0,"nanos":10247680},"max":{"secs":0,"nanos":11462400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":11976371},"median":{"secs":0,"nanos":12006912},"variance":{"secs":0,"nanos":185},"min":{"secs":0,"nanos":11264512},"max":{"secs":0,"nanos":12703232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":13132902},"median":{"secs":0,"nanos":13205504},"variance":{"secs":0,"nanos":150},"min":{"secs":0,"nanos":12390912},"max":{"secs":0,"nanos":13904384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18075724},"median":{"secs":0,"nanos":22734336},"variance":{"secs":0,"nanos":50955},"min":{"secs":0,"nanos":7036416},"max":{"secs":0,"nanos":23796480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":32768,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":831539},"median":{"secs":0,"nanos":836608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":814080},"max":{"secs":0,"nanos":845824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1084313},"median":{"secs":0,"nanos":1070336},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1068288},"max":{"secs":0,"nanos":1212672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1091737},"median":{"secs":0,"nanos":1077248},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1075712},"max":{"secs":0,"nanos":1217024}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":254950},"median":{"secs":0,"nanos":254464},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":251648},"max":{"secs":0,"nanos":263680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":307916},"median":{"secs":0,"nanos":307968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":307200},"max":{"secs":0,"nanos":308992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":318438},"median":{"secs":0,"nanos":318720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":316672},"max":{"secs":0,"nanos":320000}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2854348},"median":{"secs":0,"nanos":2741760},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":2739712},"max":{"secs":0,"nanos":3145472}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4107596},"median":{"secs":0,"nanos":4123904},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":4497152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6656665},"median":{"secs":0,"nanos":6635520},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":6530560},"max":{"secs":0,"nanos":6785024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":20676684},"median":{"secs":0,"nanos":20399104},"variance":{"secs":0,"nanos":317},"min":{"secs":0,"nanos":20233728},"max":{"secs":0,"nanos":21930240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20983936},"median":{"secs":0,"nanos":20862720},"variance":{"secs":0,"nanos":297},"min":{"secs":0,"nanos":20465152},"max":{"secs":0,"nanos":21934336}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2853990},"median":{"secs":0,"nanos":2767616},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":2766592},"max":{"secs":0,"nanos":3080960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3054105},"median":{"secs":0,"nanos":2935296},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":2909184},"max":{"secs":0,"nanos":3451648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3178214},"median":{"secs":0,"nanos":3114496},"variance":{"secs":0,"nanos":82},"min":{"secs":0,"nanos":2965760},"max":{"secs":0,"nanos":3795712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3324288},"median":{"secs":0,"nanos":3375872},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":3143424},"max":{"secs":0,"nanos":3499008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5457740},"median":{"secs":0,"nanos":5553664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":5283840},"max":{"secs":0,"nanos":5610240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2936627},"median":{"secs":0,"nanos":2848256},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":2842624},"max":{"secs":0,"nanos":3357696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3069926},"median":{"secs":0,"nanos":2890496},"variance":{"secs":0,"nanos":78},"min":{"secs":0,"nanos":2880512},"max":{"secs":0,"nanos":3554560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2979609},"median":{"secs":0,"nanos":3013888},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":2763776},"max":{"secs":0,"nanos":3126272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20880716},"median":{"secs":0,"nanos":25651968},"variance":{"secs":0,"nanos":75564},"min":{"secs":0,"nanos":7217664},"max":{"secs":0,"nanos":28026112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19703961},"median":{"secs":0,"nanos":26590464},"variance":{"secs":0,"nanos":110438},"min":{"secs":0,"nanos":4252672},"max":{"secs":0,"nanos":29412352}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":546841},"median":{"secs":0,"nanos":546560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":544768},"max":{"secs":0,"nanos":550912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":584857},"median":{"secs":0,"nanos":586240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":569856},"max":{"secs":0,"nanos":594944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2935526},"median":{"secs":0,"nanos":2813184},"variance":{"secs":0,"nanos":85},"min":{"secs":0,"nanos":2670080},"max":{"secs":0,"nanos":3495936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5875507},"median":{"secs":0,"nanos":5905152},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":5494272},"max":{"secs":0,"nanos":6170624}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19583360},"median":{"secs":0,"nanos":32427776},"variance":{"secs":0,"nanos":187140},"min":{"secs":0,"nanos":3954176},"max":{"secs":0,"nanos":33453056}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1592473},"median":{"secs":0,"nanos":1396736},"variance":{"secs":0,"nanos":324},"min":{"secs":0,"nanos":1381120},"max":{"secs":0,"nanos":3299584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5166720},"median":{"secs":0,"nanos":4754432},"variance":{"secs":0,"nanos":1522},"min":{"secs":0,"nanos":3846656},"max":{"secs":0,"nanos":7577856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5461350},"median":{"secs":0,"nanos":5367808},"variance":{"secs":0,"nanos":1295},"min":{"secs":0,"nanos":3847936},"max":{"secs":0,"nanos":7802368}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1848908},"median":{"secs":0,"nanos":1743360},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":1740544},"max":{"secs":0,"nanos":2204160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4710451},"median":{"secs":0,"nanos":4727296},"variance":{"secs":0,"nanos":46},"min":{"secs":0,"nanos":4426240},"max":{"secs":0,"nanos":5119744}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10421862},"median":{"secs":0,"nanos":10817280},"variance":{"secs":0,"nanos":413},"min":{"secs":0,"nanos":9217536},"max":{"secs":0,"nanos":11113984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":12941721},"median":{"secs":0,"nanos":12724736},"variance":{"secs":0,"nanos":1453},"min":{"secs":0,"nanos":11183872},"max":{"secs":0,"nanos":15297792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21671884},"median":{"secs":0,"nanos":23159552},"variance":{"secs":0,"nanos":29648},"min":{"secs":0,"nanos":6394624},"max":{"secs":0,"nanos":27064576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":178508},"median":{"secs":0,"nanos":178432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":176640},"max":{"secs":0,"nanos":181504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":645094},"median":{"secs":0,"nanos":591872},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":565504},"max":{"secs":0,"nanos":997120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":777164},"median":{"secs":0,"nanos":635136},"variance":{"secs":0,"nanos":142},"min":{"secs":0,"nanos":608768},"max":{"secs":0,"nanos":1900544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2245222},"median":{"secs":0,"nanos":1720576},"variance":{"secs":0,"nanos":1017},"min":{"secs":0,"nanos":1384448},"max":{"secs":0,"nanos":4108800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4201753},"median":{"secs":0,"nanos":4007680},"variance":{"secs":0,"nanos":289},"min":{"secs":0,"nanos":3652608},"max":{"secs":0,"nanos":5258752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4434969},"median":{"secs":0,"nanos":4115456},"variance":{"secs":0,"nanos":884},"min":{"secs":0,"nanos":3677440},"max":{"secs":0,"nanos":7000576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6815820},"median":{"secs":0,"nanos":6622976},"variance":{"secs":0,"nanos":225},"min":{"secs":0,"nanos":6160640},"max":{"secs":0,"nanos":7798272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6874035},"median":{"secs":0,"nanos":6903552},"variance":{"secs":0,"nanos":70},"min":{"secs":0,"nanos":6480128},"max":{"secs":0,"nanos":7405568}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4096,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1980774},"median":{"secs":0,"nanos":1670144},"variance":{"secs":0,"nanos":546},"min":{"secs":0,"nanos":1656064},"max":{"secs":0,"nanos":4151808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1922380},"median":{"secs":0,"nanos":1736704},"variance":{"secs":0,"nanos":217},"min":{"secs":0,"nanos":1591808},"max":{"secs":0,"nanos":3210496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2312601},"median":{"secs":0,"nanos":1955072},"variance":{"secs":0,"nanos":780},"min":{"secs":0,"nanos":1547520},"max":{"secs":0,"nanos":3899392}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":4096,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1996953},"median":{"secs":0,"nanos":1980672},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1978368},"max":{"secs":0,"nanos":2142208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3648460},"median":{"secs":0,"nanos":3424000},"variance":{"secs":0,"nanos":129},"min":{"secs":0,"nanos":3340800},"max":{"secs":0,"nanos":4158208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":9007923},"median":{"secs":0,"nanos":9748736},"variance":{"secs":0,"nanos":1579},"min":{"secs":0,"nanos":7247104},"max":{"secs":0,"nanos":10685696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10290124},"median":{"secs":0,"nanos":10428160},"variance":{"secs":0,"nanos":353},"min":{"secs":0,"nanos":8993792},"max":{"secs":0,"nanos":11058688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":17015910},"median":{"secs":0,"nanos":19732480},"variance":{"secs":0,"nanos":57324},"min":{"secs":0,"nanos":2707712},"max":{"secs":0,"nanos":27741440}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4096,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":357145},"median":{"secs":0,"nanos":356864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":354816},"max":{"secs":0,"nanos":360192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":487244},"median":{"secs":0,"nanos":486400},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":483840},"max":{"secs":0,"nanos":494080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":564531},"median":{"secs":0,"nanos":552192},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":548608},"max":{"secs":0,"nanos":673536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4096,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":632320},"median":{"secs":0,"nanos":604672},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":601856},"max":{"secs":0,"nanos":676608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2032332},"median":{"secs":0,"nanos":1042944},"variance":{"secs":0,"nanos":2576},"min":{"secs":0,"nanos":647680},"max":{"secs":0,"nanos":4772864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4475852},"median":{"secs":0,"nanos":4256256},"variance":{"secs":0,"nanos":673},"min":{"secs":0,"nanos":3849472},"max":{"secs":0,"nanos":6507008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5535641},"median":{"secs":0,"nanos":5656320},"variance":{"secs":0,"nanos":592},"min":{"secs":0,"nanos":4835840},"max":{"secs":0,"nanos":7040512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8077900},"median":{"secs":0,"nanos":8142336},"variance":{"secs":0,"nanos":759},"min":{"secs":0,"nanos":6707968},"max":{"secs":0,"nanos":10243072}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":4096,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":309401},"median":{"secs":0,"nanos":287232},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":278528},"max":{"secs":0,"nanos":349952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":321459},"median":{"secs":0,"nanos":364032},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":274432},"max":{"secs":0,"nanos":369920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3916595},"median":{"secs":0,"nanos":3640064},"variance":{"secs":0,"nanos":1871},"min":{"secs":0,"nanos":2874880},"max":{"secs":0,"nanos":7302144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3877196},"median":{"secs":0,"nanos":4154880},"variance":{"secs":0,"nanos":1798},"min":{"secs":0,"nanos":1967360},"max":{"secs":0,"nanos":6480384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":6518016},"median":{"secs":0,"nanos":6501376},"variance":{"secs":0,"nanos":2105},"min":{"secs":0,"nanos":4617472},"max":{"secs":0,"nanos":8945152}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":501017},"median":{"secs":0,"nanos":489984},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":471552},"max":{"secs":0,"nanos":598784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1788902},"median":{"secs":0,"nanos":1031680},"variance":{"secs":0,"nanos":1641},"min":{"secs":0,"nanos":977920},"max":{"secs":0,"nanos":4413696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2206438},"median":{"secs":0,"nanos":1664512},"variance":{"secs":0,"nanos":2587},"min":{"secs":0,"nanos":858880},"max":{"secs":0,"nanos":5134848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6208998},"median":{"secs":0,"nanos":5964800},"variance":{"secs":0,"nanos":1676},"min":{"secs":0,"nanos":4835072},"max":{"secs":0,"nanos":8681984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12472396},"median":{"secs":0,"nanos":12755968},"variance":{"secs":0,"nanos":2335},"min":{"secs":0,"nanos":9508352},"max":{"secs":0,"nanos":14672896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9887436},"median":{"secs":0,"nanos":10119680},"variance":{"secs":0,"nanos":3836},"min":{"secs":0,"nanos":6726400},"max":{"secs":0,"nanos":13103616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":14021836},"median":{"secs":0,"nanos":16350976},"variance":{"secs":0,"nanos":10050},"min":{"secs":0,"nanos":8959488},"max":{"secs":0,"nanos":17525248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":17226572},"median":{"secs":0,"nanos":17220864},"variance":{"secs":0,"nanos":490},"min":{"secs":0,"nanos":15859712},"max":{"secs":0,"nanos":18695168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":55099929},"median":{"secs":0,"nanos":51841280},"variance":{"secs":0,"nanos":35440},"min":{"secs":0,"nanos":49720320},"max":{"secs":0,"nanos":67577344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":55667020},"median":{"secs":0,"nanos":55696896},"variance":{"secs":0,"nanos":1014},"min":{"secs":0,"nanos":54422528},"max":{"secs":0,"nanos":58114048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1594931},"median":{"secs":0,"nanos":1610752},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":1460992},"max":{"secs":0,"nanos":1927168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1933875},"median":{"secs":0,"nanos":1934592},"variance":{"secs":0,"nanos":48},"min":{"secs":0,"nanos":1730816},"max":{"secs":0,"nanos":2314496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4725939},"median":{"secs":0,"nanos":4771328},"variance":{"secs":0,"nanos":358},"min":{"secs":0,"nanos":4123648},"max":{"secs":0,"nanos":5820416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":26991616},"median":{"secs":0,"nanos":26781952},"variance":{"secs":0,"nanos":330},"min":{"secs":0,"nanos":26334720},"max":{"secs":0,"nanos":28366592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":30336179},"median":{"secs":0,"nanos":27179008},"variance":{"secs":0,"nanos":141476},"min":{"secs":0,"nanos":24321536},"max":{"secs":0,"nanos":65809920}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":32,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6528},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5888},"max":{"secs":0,"nanos":7424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":19353},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16128},"max":{"secs":0,"nanos":21760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31616},"median":{"secs":0,"nanos":29184},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26368},"max":{"secs":0,"nanos":41984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":90803},"median":{"secs":0,"nanos":85248},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81152},"max":{"secs":0,"nanos":118528}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7466035},"median":{"secs":0,"nanos":7106304},"variance":{"secs":0,"nanos":779},"min":{"secs":0,"nanos":6514688},"max":{"secs":0,"nanos":9399552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":6986649},"median":{"secs":0,"nanos":7123456},"variance":{"secs":0,"nanos":1133},"min":{"secs":0,"nanos":4427264},"max":{"secs":0,"nanos":8643840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":7558963},"median":{"secs":0,"nanos":7714304},"variance":{"secs":0,"nanos":1248},"min":{"secs":0,"nanos":5583360},"max":{"secs":0,"nanos":9086464}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":353408},"median":{"secs":0,"nanos":374272},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":299264},"max":{"secs":0,"nanos":513280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1010918},"median":{"secs":0,"nanos":660480},"variance":{"secs":0,"nanos":673},"min":{"secs":0,"nanos":554240},"max":{"secs":0,"nanos":3395840}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":14054},"median":{"secs":0,"nanos":13312},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12544},"max":{"secs":0,"nanos":21248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":98585},"median":{"secs":0,"nanos":97280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":96256},"max":{"secs":0,"nanos":107008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6656},"median":{"secs":0,"nanos":6912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":7936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10086},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":32281},"median":{"secs":0,"nanos":32000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30976},"max":{"secs":0,"nanos":34560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":54553},"median":{"secs":0,"nanos":54272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52480},"max":{"secs":0,"nanos":58880}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8524},"median":{"secs":0,"nanos":8192},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7936},"max":{"secs":0,"nanos":9984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28928},"median":{"secs":0,"nanos":28160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":32256}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":219187},"median":{"secs":0,"nanos":197888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":196864},"max":{"secs":0,"nanos":265984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":347622},"median":{"secs":0,"nanos":324608},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":321792},"max":{"secs":0,"nanos":388864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":416230},"median":{"secs":0,"nanos":344832},"variance":{"secs":0,"nanos":76},"min":{"secs":0,"nanos":291328},"max":{"secs":0,"nanos":1245696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":439833},"median":{"secs":0,"nanos":415744},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":367872},"max":{"secs":0,"nanos":824832}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":94284},"median":{"secs":0,"nanos":70656},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":68864},"max":{"secs":0,"nanos":137728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":186496},"median":{"secs":0,"nanos":176896},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175104},"max":{"secs":0,"nanos":220160}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":8192,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":377472},"median":{"secs":0,"nanos":377344},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":376576},"max":{"secs":0,"nanos":379392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1242726},"median":{"secs":0,"nanos":1003008},"variance":{"secs":0,"nanos":334},"min":{"secs":0,"nanos":994816},"max":{"secs":0,"nanos":2918144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1825433},"median":{"secs":0,"nanos":1972736},"variance":{"secs":0,"nanos":97},"min":{"secs":0,"nanos":1509376},"max":{"secs":0,"nanos":2467328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4146713},"median":{"secs":0,"nanos":2601728},"variance":{"secs":0,"nanos":10055},"min":{"secs":0,"nanos":2375424},"max":{"secs":0,"nanos":11112448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":13558323},"median":{"secs":0,"nanos":13963520},"variance":{"secs":0,"nanos":2354},"min":{"secs":0,"nanos":11239168},"max":{"secs":0,"nanos":15713280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":512,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":99737},"median":{"secs":0,"nanos":99072},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":97792},"max":{"secs":0,"nanos":108032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":938060},"median":{"secs":0,"nanos":828672},"variance":{"secs":0,"nanos":46},"min":{"secs":0,"nanos":814080},"max":{"secs":0,"nanos":1432576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":992742},"median":{"secs":0,"nanos":969984},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":950272},"max":{"secs":0,"nanos":1106944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":32,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":33100},"median":{"secs":0,"nanos":28928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28416},"max":{"secs":0,"nanos":49920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31360},"median":{"secs":0,"nanos":35072},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":41984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":52889},"median":{"secs":0,"nanos":42240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":40960},"max":{"secs":0,"nanos":84992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":105139},"median":{"secs":0,"nanos":97280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95744},"max":{"secs":0,"nanos":129792}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31564},"median":{"secs":0,"nanos":30208},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26368},"max":{"secs":0,"nanos":42240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99712},"median":{"secs":0,"nanos":96768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95488},"max":{"secs":0,"nanos":129536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":4096,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5944064},"median":{"secs":0,"nanos":5840896},"variance":{"secs":0,"nanos":93},"min":{"secs":0,"nanos":5469696},"max":{"secs":0,"nanos":6485248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6603494},"median":{"secs":0,"nanos":6872320},"variance":{"secs":0,"nanos":534},"min":{"secs":0,"nanos":5498624},"max":{"secs":0,"nanos":8165120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":16234009},"median":{"secs":0,"nanos":16456960},"variance":{"secs":0,"nanos":667},"min":{"secs":0,"nanos":14603776},"max":{"secs":0,"nanos":17638400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":8192,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1320601},"median":{"secs":0,"nanos":1246208},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":1210368},"max":{"secs":0,"nanos":1506048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1596083},"median":{"secs":0,"nanos":1565952},"variance":{"secs":0,"nanos":35},"min":{"secs":0,"nanos":1414656},"max":{"secs":0,"nanos":1984000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1805312},"median":{"secs":0,"nanos":1822208},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":1673216},"max":{"secs":0,"nanos":2065920}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10583398},"median":{"secs":0,"nanos":10593024},"variance":{"secs":0,"nanos":64},"min":{"secs":0,"nanos":10210048},"max":{"secs":0,"nanos":11016960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10488243},"median":{"secs":0,"nanos":10629632},"variance":{"secs":0,"nanos":149},"min":{"secs":0,"nanos":9640960},"max":{"secs":0,"nanos":10932992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13635814},"median":{"secs":0,"nanos":13641472},"variance":{"secs":0,"nanos":68},"min":{"secs":0,"nanos":13311488},"max":{"secs":0,"nanos":14107136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":168549657},"median":{"secs":0,"nanos":168432896},"variance":{"secs":0,"nanos":4346},"min":{"secs":0,"nanos":165645312},"max":{"secs":0,"nanos":173519104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":170876134},"median":{"secs":0,"nanos":170621184},"variance":{"secs":0,"nanos":1019},"min":{"secs":0,"nanos":169524224},"max":{"secs":0,"nanos":172238848}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3898854},"median":{"secs":0,"nanos":3859456},"variance":{"secs":0,"nanos":71},"min":{"secs":0,"nanos":3538944},"max":{"secs":0,"nanos":4324096}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":11619686},"median":{"secs":0,"nanos":11814912},"variance":{"secs":0,"nanos":133},"min":{"secs":0,"nanos":10893824},"max":{"secs":0,"nanos":12211712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":13804211},"median":{"secs":0,"nanos":21007360},"variance":{"secs":0,"nanos":77118},"min":{"secs":0,"nanos":3084288},"max":{"secs":0,"nanos":22594304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24392166},"median":{"secs":0,"nanos":24307200},"variance":{"secs":0,"nanos":566},"min":{"secs":0,"nanos":23443456},"max":{"secs":0,"nanos":25782784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":24703692},"median":{"secs":0,"nanos":24860928},"variance":{"secs":0,"nanos":1243},"min":{"secs":0,"nanos":23325696},"max":{"secs":0,"nanos":26831872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":32,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23040},"median":{"secs":0,"nanos":26368},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17408},"max":{"secs":0,"nanos":30464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":73728},"median":{"secs":0,"nanos":70912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":69632},"max":{"secs":0,"nanos":98304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":3,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":444108},"median":{"secs":0,"nanos":444416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":441600},"max":{"secs":0,"nanos":447488}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":450534},"median":{"secs":0,"nanos":450816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":446720},"max":{"secs":0,"nanos":452864}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1650790},"median":{"secs":0,"nanos":1741824},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":1489920},"max":{"secs":0,"nanos":1839616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3340953},"median":{"secs":0,"nanos":2295808},"variance":{"secs":0,"nanos":3014},"min":{"secs":0,"nanos":1735424},"max":{"secs":0,"nanos":5984512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2999091},"median":{"secs":0,"nanos":2970112},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":2787840},"max":{"secs":0,"nanos":3270912}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4398028},"median":{"secs":0,"nanos":4089600},"variance":{"secs":0,"nanos":168},"min":{"secs":0,"nanos":4083968},"max":{"secs":0,"nanos":5124352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5667865},"median":{"secs":0,"nanos":5726208},"variance":{"secs":0,"nanos":1173},"min":{"secs":0,"nanos":4485888},"max":{"secs":0,"nanos":7638272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10582630},"median":{"secs":0,"nanos":9987840},"variance":{"secs":0,"nanos":2813},"min":{"secs":0,"nanos":8357632},"max":{"secs":0,"nanos":13517568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":13566361},"median":{"secs":0,"nanos":13590016},"variance":{"secs":0,"nanos":256},"min":{"secs":0,"nanos":12969984},"max":{"secs":0,"nanos":14556160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":31290444},"median":{"secs":0,"nanos":32386048},"variance":{"secs":0,"nanos":136350},"min":{"secs":0,"nanos":13623296},"max":{"secs":0,"nanos":54853120}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1060480},"median":{"secs":0,"nanos":1026816},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":1020416},"max":{"secs":0,"nanos":1239296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":12436761},"median":{"secs":0,"nanos":12412416},"variance":{"secs":0,"nanos":15727},"min":{"secs":0,"nanos":6176512},"max":{"secs":0,"nanos":19538688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":12518784},"median":{"secs":0,"nanos":15654912},"variance":{"secs":0,"nanos":13090},"min":{"secs":0,"nanos":7638272},"max":{"secs":0,"nanos":16456960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":8422374},"median":{"secs":0,"nanos":8476416},"variance":{"secs":0,"nanos":795},"min":{"secs":0,"nanos":7084032},"max":{"secs":0,"nanos":10167552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9214899},"median":{"secs":0,"nanos":8867328},"variance":{"secs":0,"nanos":3407},"min":{"secs":0,"nanos":7021056},"max":{"secs":0,"nanos":12228608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":12065228},"median":{"secs":0,"nanos":11575296},"variance":{"secs":0,"nanos":1358},"min":{"secs":0,"nanos":10624512},"max":{"secs":0,"nanos":13895680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17569177},"median":{"secs":0,"nanos":17550592},"variance":{"secs":0,"nanos":515},"min":{"secs":0,"nanos":16826368},"max":{"secs":0,"nanos":19045120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":28223001},"median":{"secs":0,"nanos":20060672},"variance":{"secs":0,"nanos":380845},"min":{"secs":0,"nanos":18967552},"max":{"secs":0,"nanos":83635456}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":283571},"median":{"secs":0,"nanos":280576},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":278784},"max":{"secs":0,"nanos":309248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":308531},"median":{"secs":0,"nanos":305920},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":304128},"max":{"secs":0,"nanos":332032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1280870},"median":{"secs":0,"nanos":1299200},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":1080320},"max":{"secs":0,"nanos":1415168}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1554329},"median":{"secs":0,"nanos":1464064},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":1449728},"max":{"secs":0,"nanos":1927424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10144025},"median":{"secs":0,"nanos":10733056},"variance":{"secs":0,"nanos":1928},"min":{"secs":0,"nanos":7603712},"max":{"secs":0,"nanos":11856128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":733030},"median":{"secs":0,"nanos":704512},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":676096},"max":{"secs":0,"nanos":1043712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1856076},"median":{"secs":0,"nanos":1615616},"variance":{"secs":0,"nanos":167},"min":{"secs":0,"nanos":1518592},"max":{"secs":0,"nanos":2846208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1835392},"median":{"secs":0,"nanos":1686528},"variance":{"secs":0,"nanos":115},"min":{"secs":0,"nanos":1609984},"max":{"secs":0,"nanos":2669568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6193894},"median":{"secs":0,"nanos":6080000},"variance":{"secs":0,"nanos":203},"min":{"secs":0,"nanos":5879552},"max":{"secs":0,"nanos":7262720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":25185689},"median":{"secs":0,"nanos":32146176},"variance":{"secs":0,"nanos":280458},"min":{"secs":0,"nanos":4124160},"max":{"secs":0,"nanos":44143872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":27625369},"median":{"secs":0,"nanos":12870144},"variance":{"secs":0,"nanos":662035},"min":{"secs":0,"nanos":10686976},"max":{"secs":0,"nanos":77155072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14673280},"median":{"secs":0,"nanos":14449152},"variance":{"secs":0,"nanos":233},"min":{"secs":0,"nanos":14258432},"max":{"secs":0,"nanos":15777536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28581862},"median":{"secs":0,"nanos":28272128},"variance":{"secs":0,"nanos":65344},"min":{"secs":0,"nanos":11894528},"max":{"secs":0,"nanos":45217792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":190213785},"median":{"secs":0,"nanos":190499328},"variance":{"secs":0,"nanos":1831},"min":{"secs":0,"nanos":188051456},"max":{"secs":0,"nanos":192256000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":191472409},"median":{"secs":0,"nanos":191872000},"variance":{"secs":0,"nanos":2513},"min":{"secs":0,"nanos":188647680},"max":{"secs":0,"nanos":194107136}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5060556},"median":{"secs":0,"nanos":4890880},"variance":{"secs":0,"nanos":254},"min":{"secs":0,"nanos":4740608},"max":{"secs":0,"nanos":6115840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4969984},"median":{"secs":0,"nanos":4915456},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":4910336},"max":{"secs":0,"nanos":5110272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":7160985},"median":{"secs":0,"nanos":7138560},"variance":{"secs":0,"nanos":59},"min":{"secs":0,"nanos":6947840},"max":{"secs":0,"nanos":7730176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":89947238},"median":{"secs":0,"nanos":89821952},"variance":{"secs":0,"nanos":637},"min":{"secs":0,"nanos":88274688},"max":{"secs":0,"nanos":91324928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":90373708},"median":{"secs":0,"nanos":90107392},"variance":{"secs":0,"nanos":528},"min":{"secs":0,"nanos":89106176},"max":{"secs":0,"nanos":91360768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6348},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5632},"max":{"secs":0,"nanos":8448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":8601},"median":{"secs":0,"nanos":8704},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7936},"max":{"secs":0,"nanos":9216}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":69683},"median":{"secs":0,"nanos":69120},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":68352},"max":{"secs":0,"nanos":73984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":228582},"median":{"secs":0,"nanos":224768},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":222720},"max":{"secs":0,"nanos":239872}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4795904},"median":{"secs":0,"nanos":4828160},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":4636160},"max":{"secs":0,"nanos":5012480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":9209548},"median":{"secs":0,"nanos":9075712},"variance":{"secs":0,"nanos":582},"min":{"secs":0,"nanos":8417792},"max":{"secs":0,"nanos":11182336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":20920883},"median":{"secs":0,"nanos":28364800},"variance":{"secs":0,"nanos":129344},"min":{"secs":0,"nanos":3578624},"max":{"secs":0,"nanos":31237376}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":202214},"median":{"secs":0,"nanos":201984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":201216},"max":{"secs":0,"nanos":204288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":373196},"median":{"secs":0,"nanos":372736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":371968},"max":{"secs":0,"nanos":377088}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":177484},"median":{"secs":0,"nanos":177664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":176896},"max":{"secs":0,"nanos":178688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":663936},"median":{"secs":0,"nanos":654848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":652288},"max":{"secs":0,"nanos":752128}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58675},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56576},"max":{"secs":0,"nanos":61184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":70246},"median":{"secs":0,"nanos":66560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":64000},"max":{"secs":0,"nanos":105728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":455961},"median":{"secs":0,"nanos":450304},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":449024},"max":{"secs":0,"nanos":502272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":787123},"median":{"secs":0,"nanos":786688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":783104},"max":{"secs":0,"nanos":795648}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":133529},"median":{"secs":0,"nanos":134144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":132352},"max":{"secs":0,"nanos":134912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":298752},"median":{"secs":0,"nanos":291584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288256},"max":{"secs":0,"nanos":372736}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":996044},"median":{"secs":0,"nanos":994816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":989440},"max":{"secs":0,"nanos":1010176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1883494},"median":{"secs":0,"nanos":1731840},"variance":{"secs":0,"nanos":101},"min":{"secs":0,"nanos":1700352},"max":{"secs":0,"nanos":2742784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3879014},"median":{"secs":0,"nanos":3669504},"variance":{"secs":0,"nanos":291},"min":{"secs":0,"nanos":3352576},"max":{"secs":0,"nanos":4637952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4347084},"median":{"secs":0,"nanos":3688960},"variance":{"secs":0,"nanos":2630},"min":{"secs":0,"nanos":3312384},"max":{"secs":0,"nanos":9013504}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":512,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":393523},"median":{"secs":0,"nanos":383744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368384},"max":{"secs":0,"nanos":467456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":416153},"median":{"secs":0,"nanos":413952},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":382208},"max":{"secs":0,"nanos":498176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":16384,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1368371},"median":{"secs":0,"nanos":1353984},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1352960},"max":{"secs":0,"nanos":1499648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3309875},"median":{"secs":0,"nanos":2069504},"variance":{"secs":0,"nanos":13229},"min":{"secs":0,"nanos":1753600},"max":{"secs":0,"nanos":14150656}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":9227801},"median":{"secs":0,"nanos":9332992},"variance":{"secs":0,"nanos":208},"min":{"secs":0,"nanos":8697600},"max":{"secs":0,"nanos":10364416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":9680716},"median":{"secs":0,"nanos":9670400},"variance":{"secs":0,"nanos":77},"min":{"secs":0,"nanos":9298944},"max":{"secs":0,"nanos":10192384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10995430},"median":{"secs":0,"nanos":9881600},"variance":{"secs":0,"nanos":6081},"min":{"secs":0,"nanos":9106944},"max":{"secs":0,"nanos":16126720}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":1024,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":309171},"median":{"secs":0,"nanos":308992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":307200},"max":{"secs":0,"nanos":310784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3570636},"median":{"secs":0,"nanos":3149312},"variance":{"secs":0,"nanos":1393},"min":{"secs":0,"nanos":2813696},"max":{"secs":0,"nanos":6971136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8573132},"median":{"secs":0,"nanos":8125952},"variance":{"secs":0,"nanos":3642},"min":{"secs":0,"nanos":7002624},"max":{"secs":0,"nanos":13848064}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":54732},"median":{"secs":0,"nanos":55040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":53248},"max":{"secs":0,"nanos":56320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":73702},"median":{"secs":0,"nanos":68096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":64768},"max":{"secs":0,"nanos":118784}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":116275},"median":{"secs":0,"nanos":110080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":107008},"max":{"secs":0,"nanos":179712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":329164},"median":{"secs":0,"nanos":327680},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":325120},"max":{"secs":0,"nanos":337664}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":81920},"median":{"secs":0,"nanos":81408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":76544},"max":{"secs":0,"nanos":98304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":253619},"median":{"secs":0,"nanos":243200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":240128},"max":{"secs":0,"nanos":282112}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":8192,"n":16384,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":34587340},"median":{"secs":0,"nanos":20269824},"variance":{"secs":0,"nanos":825790},"min":{"secs":0,"nanos":19781632},"max":{"secs":0,"nanos":97835008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":23249817},"median":{"secs":0,"nanos":21177088},"variance":{"secs":0,"nanos":61775},"min":{"secs":0,"nanos":19234304},"max":{"secs":0,"nanos":46723328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":43139430},"median":{"secs":0,"nanos":30478848},"variance":{"secs":0,"nanos":610796},"min":{"secs":0,"nanos":28125696},"max":{"secs":0,"nanos":92635136}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":8192,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":7319731},"median":{"secs":0,"nanos":7243520},"variance":{"secs":0,"nanos":137},"min":{"secs":0,"nanos":7076864},"max":{"secs":0,"nanos":8253440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":26819891},"median":{"secs":0,"nanos":16675328},"variance":{"secs":0,"nanos":572450},"min":{"secs":0,"nanos":5268480},"max":{"secs":0,"nanos":59665664}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20993894},"median":{"secs":0,"nanos":20780800},"variance":{"secs":0,"nanos":1341},"min":{"secs":0,"nanos":19692544},"max":{"secs":0,"nanos":23414784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":33550003},"median":{"secs":0,"nanos":33283328},"variance":{"secs":0,"nanos":255},"min":{"secs":0,"nanos":33079040},"max":{"secs":0,"nanos":34582528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":36257612},"median":{"secs":0,"nanos":36604160},"variance":{"secs":0,"nanos":479},"min":{"secs":0,"nanos":35412992},"max":{"secs":0,"nanos":37237248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":50727347},"median":{"secs":0,"nanos":50851072},"variance":{"secs":0,"nanos":182},"min":{"secs":0,"nanos":50183424},"max":{"secs":0,"nanos":51690240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":642460160},"median":{"secs":0,"nanos":642402560},"variance":{"secs":0,"nanos":592},"min":{"secs":0,"nanos":641153280},"max":{"secs":0,"nanos":643976960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":671708262},"median":{"secs":0,"nanos":662825216},"variance":{"secs":0,"nanos":165563},"min":{"secs":0,"nanos":660317952},"max":{"secs":0,"nanos":695344896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":23449472},"median":{"secs":0,"nanos":27831296},"variance":{"secs":0,"nanos":59200},"min":{"secs":0,"nanos":8209152},"max":{"secs":0,"nanos":29090816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":26344755},"median":{"secs":0,"nanos":35233792},"variance":{"secs":0,"nanos":136625},"min":{"secs":0,"nanos":9345280},"max":{"secs":0,"nanos":36146432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":28113126},"median":{"secs":0,"nanos":36273152},"variance":{"secs":0,"nanos":300842},"min":{"secs":0,"nanos":6608128},"max":{"secs":0,"nanos":60301312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":90515404},"median":{"secs":0,"nanos":90031616},"variance":{"secs":0,"nanos":3509},"min":{"secs":0,"nanos":88845312},"max":{"secs":0,"nanos":95628288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":92449536},"median":{"secs":0,"nanos":92367104},"variance":{"secs":0,"nanos":353},"min":{"secs":0,"nanos":91471616},"max":{"secs":0,"nanos":93382144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":64,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":7961},"median":{"secs":0,"nanos":7680},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":6912},"max":{"secs":0,"nanos":10752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":28851},"median":{"secs":0,"nanos":28672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27904},"max":{"secs":0,"nanos":32768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1539456},"median":{"secs":0,"nanos":1495552},"variance":{"secs":0,"nanos":8},"min":{"secs":0,"nanos":1495040},"max":{"secs":0,"nanos":1766400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1985408},"median":{"secs":0,"nanos":1868800},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":1864448},"max":{"secs":0,"nanos":2261504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3578112},"median":{"secs":0,"nanos":3525120},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":3522816},"max":{"secs":0,"nanos":3709184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5450931},"median":{"secs":0,"nanos":5397248},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":5391104},"max":{"secs":0,"nanos":5612032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5551155},"median":{"secs":0,"nanos":5512960},"variance":{"secs":0,"nanos":90},"min":{"secs":0,"nanos":5335040},"max":{"secs":0,"nanos":6161408}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":860928},"median":{"secs":0,"nanos":809728},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":806144},"max":{"secs":0,"nanos":1004288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":810240},"median":{"secs":0,"nanos":812288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":803584},"max":{"secs":0,"nanos":816384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1420083},"median":{"secs":0,"nanos":1371904},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":1370880},"max":{"secs":0,"nanos":1713664}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1649228},"median":{"secs":0,"nanos":1631744},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":1629952},"max":{"secs":0,"nanos":1804032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2674355},"median":{"secs":0,"nanos":2642944},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":2641152},"max":{"secs":0,"nanos":2817280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1522790},"median":{"secs":0,"nanos":1294336},"variance":{"secs":0,"nanos":220},"min":{"secs":0,"nanos":1245952},"max":{"secs":0,"nanos":2800384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1622502},"median":{"secs":0,"nanos":1295104},"variance":{"secs":0,"nanos":295},"min":{"secs":0,"nanos":1208320},"max":{"secs":0,"nanos":2735616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1985920},"median":{"secs":0,"nanos":1490688},"variance":{"secs":0,"nanos":747},"min":{"secs":0,"nanos":1305856},"max":{"secs":0,"nanos":3413248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12579660},"median":{"secs":0,"nanos":12660992},"variance":{"secs":0,"nanos":256},"min":{"secs":0,"nanos":11732736},"max":{"secs":0,"nanos":13472768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":25090841},"median":{"secs":0,"nanos":31987712},"variance":{"secs":0,"nanos":348455},"min":{"secs":0,"nanos":7110912},"max":{"secs":0,"nanos":58775040}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":256,"k":2048,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":7341952},"median":{"secs":0,"nanos":7218688},"variance":{"secs":0,"nanos":86},"min":{"secs":0,"nanos":7061248},"max":{"secs":0,"nanos":8057856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":17455104},"median":{"secs":0,"nanos":22214400},"variance":{"secs":0,"nanos":49989},"min":{"secs":0,"nanos":9715456},"max":{"secs":0,"nanos":27356160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19327820},"median":{"secs":0,"nanos":25230848},"variance":{"secs":0,"nanos":124496},"min":{"secs":0,"nanos":5667584},"max":{"secs":0,"nanos":33099520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":100815104},"median":{"secs":0,"nanos":99323904},"variance":{"secs":0,"nanos":12985},"min":{"secs":0,"nanos":98320640},"max":{"secs":0,"nanos":109970432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":108387046},"median":{"secs":0,"nanos":108420096},"variance":{"secs":0,"nanos":601},"min":{"secs":0,"nanos":106416896},"max":{"secs":0,"nanos":109629696}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":512,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":19621606},"median":{"secs":0,"nanos":19626496},"variance":{"secs":0,"nanos":197},"min":{"secs":0,"nanos":18626816},"max":{"secs":0,"nanos":20260608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":20664652},"median":{"secs":0,"nanos":20697600},"variance":{"secs":0,"nanos":1314},"min":{"secs":0,"nanos":19457792},"max":{"secs":0,"nanos":22861312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29978137},"median":{"secs":0,"nanos":29906176},"variance":{"secs":0,"nanos":359},"min":{"secs":0,"nanos":29204224},"max":{"secs":0,"nanos":31069184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":395244569},"median":{"secs":0,"nanos":395419136},"variance":{"secs":0,"nanos":1284},"min":{"secs":0,"nanos":393469184},"max":{"secs":0,"nanos":397208320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":396548531},"median":{"secs":0,"nanos":397008640},"variance":{"secs":0,"nanos":1437},"min":{"secs":0,"nanos":394344448},"max":{"secs":0,"nanos":398578944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":8,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":154931},"median":{"secs":0,"nanos":21504},"variance":{"secs":0,"nanos":158},"min":{"secs":0,"nanos":20480},"max":{"secs":0,"nanos":1348608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":30899},"median":{"secs":0,"nanos":28416},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27648},"max":{"secs":0,"nanos":48128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":84096},"median":{"secs":0,"nanos":75520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":71680},"max":{"secs":0,"nanos":118016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":184217},"median":{"secs":0,"nanos":173824},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":171520},"max":{"secs":0,"nanos":234240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13669478},"median":{"secs":0,"nanos":14001152},"variance":{"secs":0,"nanos":881},"min":{"secs":0,"nanos":11030016},"max":{"secs":0,"nanos":14599680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":15486848},"median":{"secs":0,"nanos":15390976},"variance":{"secs":0,"nanos":306},"min":{"secs":0,"nanos":14690560},"max":{"secs":0,"nanos":16799232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":23064166},"median":{"secs":0,"nanos":20777216},"variance":{"secs":0,"nanos":18901},"min":{"secs":0,"nanos":19209728},"max":{"secs":0,"nanos":30651904}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":701465},"median":{"secs":0,"nanos":700672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":698112},"max":{"secs":0,"nanos":710144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1259084},"median":{"secs":0,"nanos":1213440},"variance":{"secs":0,"nanos":10},"min":{"secs":0,"nanos":1208832},"max":{"secs":0,"nanos":1552128}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":183552},"median":{"secs":0,"nanos":182784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":182016},"max":{"secs":0,"nanos":193024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":692326},"median":{"secs":0,"nanos":692480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":690432},"max":{"secs":0,"nanos":694784}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":512,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58470},"median":{"secs":0,"nanos":43008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":40704},"max":{"secs":0,"nanos":120576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":74675},"median":{"secs":0,"nanos":70144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":67840},"max":{"secs":0,"nanos":117504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1638528},"median":{"secs":0,"nanos":1525504},"variance":{"secs":0,"nanos":101},"min":{"secs":0,"nanos":1520128},"max":{"secs":0,"nanos":2593280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1839206},"median":{"secs":0,"nanos":2096640},"variance":{"secs":0,"nanos":1161},"min":{"secs":0,"nanos":805888},"max":{"secs":0,"nanos":3933184}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":154163},"median":{"secs":0,"nanos":140288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":136960},"max":{"secs":0,"nanos":220928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":247014},"median":{"secs":0,"nanos":246272},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":244736},"max":{"secs":0,"nanos":252160}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":1024,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1207731},"median":{"secs":0,"nanos":667648},"variance":{"secs":0,"nanos":783},"min":{"secs":0,"nanos":645632},"max":{"secs":0,"nanos":3216128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1774976},"median":{"secs":0,"nanos":1542912},"variance":{"secs":0,"nanos":862},"min":{"secs":0,"nanos":999424},"max":{"secs":0,"nanos":4422912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4582016},"median":{"secs":0,"nanos":4194048},"variance":{"secs":0,"nanos":6910},"min":{"secs":0,"nanos":1842944},"max":{"secs":0,"nanos":9022976}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6080588},"median":{"secs":0,"nanos":6535680},"variance":{"secs":0,"nanos":1704},"min":{"secs":0,"nanos":4262400},"max":{"secs":0,"nanos":8520448}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":512,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Medium","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":652800},"median":{"secs":0,"nanos":655360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":638464},"max":{"secs":0,"nanos":673280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":803737},"median":{"secs":0,"nanos":684032},"variance":{"secs":0,"nanos":125},"min":{"secs":0,"nanos":680704},"max":{"secs":0,"nanos":1865216}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":16384,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1397094},"median":{"secs":0,"nanos":1381376},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1380608},"max":{"secs":0,"nanos":1536512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":7432908},"median":{"secs":0,"nanos":2596352},"variance":{"secs":0,"nanos":41840},"min":{"secs":0,"nanos":1843968},"max":{"secs":0,"nanos":16595200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":11426380},"median":{"secs":0,"nanos":11325440},"variance":{"secs":0,"nanos":1787},"min":{"secs":0,"nanos":9597440},"max":{"secs":0,"nanos":13233152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17443072},"median":{"secs":0,"nanos":17561600},"variance":{"secs":0,"nanos":630},"min":{"secs":0,"nanos":16227072},"max":{"secs":0,"nanos":18742272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":18718976},"median":{"secs":0,"nanos":19072512},"variance":{"secs":0,"nanos":804},"min":{"secs":0,"nanos":17550336},"max":{"secs":0,"nanos":20130304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":1024,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":530688},"median":{"secs":0,"nanos":530432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":528384},"max":{"secs":0,"nanos":532480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4611968},"median":{"secs":0,"nanos":4585984},"variance":{"secs":0,"nanos":193},"min":{"secs":0,"nanos":4104960},"max":{"secs":0,"nanos":5480960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13194214},"median":{"secs":0,"nanos":12532736},"variance":{"secs":0,"nanos":6031},"min":{"secs":0,"nanos":11334656},"max":{"secs":0,"nanos":20325888}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15206},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14336},"max":{"secs":0,"nanos":16640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":19020},"median":{"secs":0,"nanos":18944},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":20480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":31257},"median":{"secs":0,"nanos":30208},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28928},"max":{"secs":0,"nanos":34816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":59084},"median":{"secs":0,"nanos":58624},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":58112},"max":{"secs":0,"nanos":64256}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17715},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":16896},"max":{"secs":0,"nanos":18944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53017},"median":{"secs":0,"nanos":52992},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52480},"max":{"secs":0,"nanos":54016}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":8192,"n":32768,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":39097856},"median":{"secs":0,"nanos":38906624},"variance":{"secs":0,"nanos":1839},"min":{"secs":0,"nanos":37133568},"max":{"secs":0,"nanos":41883136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":43165900},"median":{"secs":0,"nanos":40313088},"variance":{"secs":0,"nanos":142955},"min":{"secs":0,"nanos":36641792},"max":{"secs":0,"nanos":78679552}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":58958924},"median":{"secs":0,"nanos":59155968},"variance":{"secs":0,"nanos":4539},"min":{"secs":0,"nanos":56209408},"max":{"secs":0,"nanos":62394880}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":16384,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":9405363},"median":{"secs":0,"nanos":9394176},"variance":{"secs":0,"nanos":140},"min":{"secs":0,"nanos":8809984},"max":{"secs":0,"nanos":10365696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":25077350},"median":{"secs":0,"nanos":11303680},"variance":{"secs":0,"nanos":868540},"min":{"secs":0,"nanos":9610752},"max":{"secs":0,"nanos":95993344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":13433548},"median":{"secs":0,"nanos":13357312},"variance":{"secs":0,"nanos":56},"min":{"secs":0,"nanos":13174016},"max":{"secs":0,"nanos":14104064}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":512,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":66881203},"median":{"secs":0,"nanos":67029504},"variance":{"secs":0,"nanos":183},"min":{"secs":0,"nanos":66223360},"max":{"secs":0,"nanos":67458048}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":75219686},"median":{"secs":0,"nanos":75282944},"variance":{"secs":0,"nanos":93},"min":{"secs":0,"nanos":74766848},"max":{"secs":0,"nanos":75861248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":97410483},"median":{"secs":0,"nanos":97434112},"variance":{"secs":0,"nanos":599},"min":{"secs":0,"nanos":96761856},"max":{"secs":0,"nanos":99592448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":1,"nanos":296542310},"median":{"secs":1,"nanos":292398336},"variance":{"secs":0,"nanos":120246},"min":{"secs":1,"nanos":290953216},"max":{"secs":1,"nanos":328642816}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":1,"nanos":303565132},"median":{"secs":1,"nanos":303598336},"variance":{"secs":0,"nanos":573},"min":{"secs":1,"nanos":302324480},"max":{"secs":1,"nanos":304818432}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":11762790},"median":{"secs":0,"nanos":11805184},"variance":{"secs":0,"nanos":93},"min":{"secs":0,"nanos":11357440},"max":{"secs":0,"nanos":12442368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":23266355},"median":{"secs":0,"nanos":14855424},"variance":{"secs":0,"nanos":181361},"min":{"secs":0,"nanos":14028544},"max":{"secs":0,"nanos":47611136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":15841868},"median":{"secs":0,"nanos":15803392},"variance":{"secs":0,"nanos":44},"min":{"secs":0,"nanos":15595264},"max":{"secs":0,"nanos":16211968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":182131507},"median":{"secs":0,"nanos":182181888},"variance":{"secs":0,"nanos":228},"min":{"secs":0,"nanos":181442304},"max":{"secs":0,"nanos":182945280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":183227366},"median":{"secs":0,"nanos":183218944},"variance":{"secs":0,"nanos":103},"min":{"secs":0,"nanos":182721792},"max":{"secs":0,"nanos":183704320}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32768,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":10075468},"median":{"secs":0,"nanos":9958144},"variance":{"secs":0,"nanos":275},"min":{"secs":0,"nanos":9626624},"max":{"secs":0,"nanos":11315200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11126912},"median":{"secs":0,"nanos":10993152},"variance":{"secs":0,"nanos":129},"min":{"secs":0,"nanos":10815488},"max":{"secs":0,"nanos":12051712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14550451},"median":{"secs":0,"nanos":14388480},"variance":{"secs":0,"nanos":176},"min":{"secs":0,"nanos":14241280},"max":{"secs":0,"nanos":15465216}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2489548},"median":{"secs":0,"nanos":2359040},"variance":{"secs":0,"nanos":59},"min":{"secs":0,"nanos":2340096},"max":{"secs":0,"nanos":3044352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2969958},"median":{"secs":0,"nanos":2830080},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":2819584},"max":{"secs":0,"nanos":3471616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3371084},"median":{"secs":0,"nanos":3339008},"variance":{"secs":0,"nanos":4},"min":{"secs":0,"nanos":3337728},"max":{"secs":0,"nanos":3503616}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20118809},"median":{"secs":0,"nanos":20053248},"variance":{"secs":0,"nanos":76},"min":{"secs":0,"nanos":19879936},"max":{"secs":0,"nanos":20648448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21586304},"median":{"secs":0,"nanos":21416448},"variance":{"secs":0,"nanos":903},"min":{"secs":0,"nanos":20334592},"max":{"secs":0,"nanos":23608832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":27420646},"median":{"secs":0,"nanos":27356672},"variance":{"secs":0,"nanos":202},"min":{"secs":0,"nanos":26802176},"max":{"secs":0,"nanos":28204544}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":333673958},"median":{"secs":0,"nanos":330093568},"variance":{"secs":0,"nanos":47551},"min":{"secs":0,"nanos":326282240},"max":{"secs":0,"nanos":344820736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":331410483},"median":{"secs":0,"nanos":330222592},"variance":{"secs":0,"nanos":17040},"min":{"secs":0,"nanos":327560960},"max":{"secs":0,"nanos":341102336}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4566348},"median":{"secs":0,"nanos":4609024},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":4465664},"max":{"secs":0,"nanos":4799232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5213209},"median":{"secs":0,"nanos":5165568},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":5163520},"max":{"secs":0,"nanos":5337344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6680448},"median":{"secs":0,"nanos":6671872},"variance":{"secs":0,"nanos":26},"min":{"secs":0,"nanos":6521600},"max":{"secs":0,"nanos":7029248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":45682252},"median":{"secs":0,"nanos":45860352},"variance":{"secs":0,"nanos":437},"min":{"secs":0,"nanos":44809728},"max":{"secs":0,"nanos":46572032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":45648768},"median":{"secs":0,"nanos":45882368},"variance":{"secs":0,"nanos":179},"min":{"secs":0,"nanos":45052160},"max":{"secs":0,"nanos":46173952}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8,"n":64,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":8345},"median":{"secs":0,"nanos":8448},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":7680},"max":{"secs":0,"nanos":8960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":47590},"median":{"secs":0,"nanos":47616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":47104},"max":{"secs":0,"nanos":48128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":512,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2776473},"median":{"secs":0,"nanos":2758656},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":2758144},"max":{"secs":0,"nanos":2931968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3639091},"median":{"secs":0,"nanos":3683584},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":3510528},"max":{"secs":0,"nanos":3893248}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6614041},"median":{"secs":0,"nanos":6554624},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":6548992},"max":{"secs":0,"nanos":6723584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":10112537},"median":{"secs":0,"nanos":10032640},"variance":{"secs":0,"nanos":144},"min":{"secs":0,"nanos":9854464},"max":{"secs":0,"nanos":11218432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10229120},"median":{"secs":0,"nanos":10173440},"variance":{"secs":0,"nanos":65},"min":{"secs":0,"nanos":9989120},"max":{"secs":0,"nanos":10927104}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1692595},"median":{"secs":0,"nanos":1714432},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":1582592},"max":{"secs":0,"nanos":1856000}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1779942},"median":{"secs":0,"nanos":1766656},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":1749760},"max":{"secs":0,"nanos":1945856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2856550},"median":{"secs":0,"nanos":2886144},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":2735616},"max":{"secs":0,"nanos":3049216}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5847910},"median":{"secs":0,"nanos":5871616},"variance":{"secs":0,"nanos":19},"min":{"secs":0,"nanos":5700608},"max":{"secs":0,"nanos":6102528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":21212211},"median":{"secs":0,"nanos":32975872},"variance":{"secs":0,"nanos":192801},"min":{"secs":0,"nanos":3871232},"max":{"secs":0,"nanos":33976832}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":16384,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":24622643},"median":{"secs":0,"nanos":10195712},"variance":{"secs":0,"nanos":515644},"min":{"secs":0,"nanos":8726784},"max":{"secs":0,"nanos":60785408}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":11624780},"median":{"secs":0,"nanos":10851072},"variance":{"secs":0,"nanos":2401},"min":{"secs":0,"nanos":10371584},"max":{"secs":0,"nanos":15446016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29901004},"median":{"secs":0,"nanos":13478400},"variance":{"secs":0,"nanos":739985},"min":{"secs":0,"nanos":11907328},"max":{"secs":0,"nanos":80936704}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":512,"k":16384,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":18542848},"median":{"secs":0,"nanos":12204800},"variance":{"secs":0,"nanos":167780},"min":{"secs":0,"nanos":10798592},"max":{"secs":0,"nanos":45121792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":30053376},"median":{"secs":0,"nanos":15074560},"variance":{"secs":0,"nanos":969154},"min":{"secs":0,"nanos":14083584},"max":{"secs":0,"nanos":106968832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":31793331},"median":{"secs":0,"nanos":47440896},"variance":{"secs":0,"nanos":386624},"min":{"secs":0,"nanos":12194304},"max":{"secs":0,"nanos":62134272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":196130995},"median":{"secs":0,"nanos":196281856},"variance":{"secs":0,"nanos":289},"min":{"secs":0,"nanos":195196160},"max":{"secs":0,"nanos":196919808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":214548736},"median":{"secs":0,"nanos":214851072},"variance":{"secs":0,"nanos":157},"min":{"secs":0,"nanos":213975552},"max":{"secs":0,"nanos":215061504}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2714393},"median":{"secs":0,"nanos":2695424},"variance":{"secs":0,"nanos":3},"min":{"secs":0,"nanos":2688256},"max":{"secs":0,"nanos":2894336}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2805248},"median":{"secs":0,"nanos":2764800},"variance":{"secs":0,"nanos":14},"min":{"secs":0,"nanos":2699776},"max":{"secs":0,"nanos":3042560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3033574},"median":{"secs":0,"nanos":2995712},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":2993408},"max":{"secs":0,"nanos":3192320}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":256,"k":16384,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4451968},"median":{"secs":0,"nanos":4403712},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":4397568},"max":{"secs":0,"nanos":4577024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4612761},"median":{"secs":0,"nanos":4551680},"variance":{"secs":0,"nanos":6},"min":{"secs":0,"nanos":4539136},"max":{"secs":0,"nanos":4724480}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16014924},"median":{"secs":0,"nanos":18845696},"variance":{"secs":0,"nanos":15662},"min":{"secs":0,"nanos":9970432},"max":{"secs":0,"nanos":19577088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":49163136},"median":{"secs":0,"nanos":49278976},"variance":{"secs":0,"nanos":193},"min":{"secs":0,"nanos":48403968},"max":{"secs":0,"nanos":49881856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":56861644},"median":{"secs":0,"nanos":54326272},"variance":{"secs":0,"nanos":78724},"min":{"secs":0,"nanos":52920320},"max":{"secs":0,"nanos":83409664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":16384,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2278988},"median":{"secs":0,"nanos":2172928},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":2157568},"max":{"secs":0,"nanos":2714880}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2936320},"median":{"secs":0,"nanos":2573056},"variance":{"secs":0,"nanos":952},"min":{"secs":0,"nanos":2360576},"max":{"secs":0,"nanos":5687040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11628364},"median":{"secs":0,"nanos":11469824},"variance":{"secs":0,"nanos":448},"min":{"secs":0,"nanos":10952192},"max":{"secs":0,"nanos":13177088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":17520051},"median":{"secs":0,"nanos":14042624},"variance":{"secs":0,"nanos":28176},"min":{"secs":0,"nanos":13026560},"max":{"secs":0,"nanos":26324992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":24979353},"median":{"secs":0,"nanos":22965248},"variance":{"secs":0,"nanos":17990},"min":{"secs":0,"nanos":21043456},"max":{"secs":0,"nanos":31339008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
