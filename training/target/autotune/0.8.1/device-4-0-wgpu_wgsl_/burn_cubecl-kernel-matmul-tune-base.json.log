{"key":{"key":{"definition":{"m":16384,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":78924},"median":{"secs":0,"nanos":78848},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":77824},"max":{"secs":0,"nanos":79872}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84838},"median":{"secs":0,"nanos":83200},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":81920},"max":{"secs":0,"nanos":89856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":127513},"median":{"secs":0,"nanos":127744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126208},"max":{"secs":0,"nanos":128256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1813376},"median":{"secs":0,"nanos":2008576},"variance":{"secs":0,"nanos":277},"min":{"secs":0,"nanos":776448},"max":{"secs":0,"nanos":2255104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3172121},"median":{"secs":0,"nanos":3086592},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":2596352},"max":{"secs":0,"nanos":4029696}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":47513},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46592},"max":{"secs":0,"nanos":49152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48640},"median":{"secs":0,"nanos":48640},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":87270},"median":{"secs":0,"nanos":87040},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":86016},"max":{"secs":0,"nanos":88576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":369971},"median":{"secs":0,"nanos":370176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":368896},"max":{"secs":0,"nanos":371456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1455641},"median":{"secs":0,"nanos":1934592},"variance":{"secs":0,"nanos":425},"min":{"secs":0,"nanos":657920},"max":{"secs":0,"nanos":2091264}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241100},"median":{"secs":0,"nanos":241152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":239360},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":258713},"median":{"secs":0,"nanos":258816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":257792},"max":{"secs":0,"nanos":260352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":926848},"median":{"secs":0,"nanos":446976},"variance":{"secs":0,"nanos":566},"min":{"secs":0,"nanos":443904},"max":{"secs":0,"nanos":2368512}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":9396428},"median":{"secs":0,"nanos":9568256},"variance":{"secs":0,"nanos":253},"min":{"secs":0,"nanos":8057856},"max":{"secs":0,"nanos":10051072}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12117401},"median":{"secs":0,"nanos":12157696},"variance":{"secs":0,"nanos":48},"min":{"secs":0,"nanos":11742720},"max":{"secs":0,"nanos":12550144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":131865},"median":{"secs":0,"nanos":132096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":133888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":157004},"median":{"secs":0,"nanos":156672},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":156416},"max":{"secs":0,"nanos":159232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":299315},"median":{"secs":0,"nanos":299264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":298240},"max":{"secs":0,"nanos":300800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4648089},"median":{"secs":0,"nanos":4743168},"variance":{"secs":0,"nanos":330},"min":{"secs":0,"nanos":3098624},"max":{"secs":0,"nanos":5299200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":5799168},"median":{"secs":0,"nanos":5455104},"variance":{"secs":0,"nanos":367},"min":{"secs":0,"nanos":5152512},"max":{"secs":0,"nanos":6633984}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1475686},"median":{"secs":0,"nanos":1986304},"variance":{"secs":0,"nanos":559},"min":{"secs":0,"nanos":703744},"max":{"secs":0,"nanos":2598400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1749657},"median":{"secs":0,"nanos":2099712},"variance":{"secs":0,"nanos":401},"min":{"secs":0,"nanos":787200},"max":{"secs":0,"nanos":2355456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1802905},"median":{"secs":0,"nanos":2217728},"variance":{"secs":0,"nanos":454},"min":{"secs":0,"nanos":980992},"max":{"secs":0,"nanos":2553344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":30080076},"median":{"secs":0,"nanos":29821184},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":29470208},"max":{"secs":0,"nanos":31312896}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":36225868},"median":{"secs":0,"nanos":36247552},"variance":{"secs":0,"nanos":72},"min":{"secs":0,"nanos":35538688},"max":{"secs":0,"nanos":36578304}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":8192,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":589824},"median":{"secs":0,"nanos":445184},"variance":{"secs":0,"nanos":188},"min":{"secs":0,"nanos":442880},"max":{"secs":0,"nanos":1893376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1064960},"median":{"secs":0,"nanos":496640},"variance":{"secs":0,"nanos":504},"min":{"secs":0,"nanos":493568},"max":{"secs":0,"nanos":2270720}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1264742},"median":{"secs":0,"nanos":1919232},"variance":{"secs":0,"nanos":545},"min":{"secs":0,"nanos":525312},"max":{"secs":0,"nanos":2094592}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":14605619},"median":{"secs":0,"nanos":14573568},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":14064128},"max":{"secs":0,"nanos":15123456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":17460556},"median":{"secs":0,"nanos":17382912},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":16971264},"max":{"secs":0,"nanos":18785792}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1322649},"median":{"secs":0,"nanos":1814784},"variance":{"secs":0,"nanos":495},"min":{"secs":0,"nanos":621056},"max":{"secs":0,"nanos":2107136}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1333196},"median":{"secs":0,"nanos":1818880},"variance":{"secs":0,"nanos":491},"min":{"secs":0,"nanos":636416},"max":{"secs":0,"nanos":2146304}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2547737},"median":{"secs":0,"nanos":2468608},"variance":{"secs":0,"nanos":90},"min":{"secs":0,"nanos":2291200},"max":{"secs":0,"nanos":3327744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":27955},"median":{"secs":0,"nanos":28160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":28672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":99865},"median":{"secs":0,"nanos":101888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":93696},"max":{"secs":0,"nanos":103168}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":13798},"median":{"secs":0,"nanos":14080},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":9728},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":53222},"median":{"secs":0,"nanos":54784},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":49920},"max":{"secs":0,"nanos":56576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":6297},"median":{"secs":0,"nanos":6144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":5376},"max":{"secs":0,"nanos":7680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11571},"median":{"secs":0,"nanos":12032},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":13824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20326},"median":{"secs":0,"nanos":20736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18176},"max":{"secs":0,"nanos":22016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":31462},"median":{"secs":0,"nanos":32256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9600},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8704},"max":{"secs":0,"nanos":11008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":29875},"median":{"secs":0,"nanos":30720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27136},"max":{"secs":0,"nanos":32256}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":20070},"median":{"secs":0,"nanos":19968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":18944},"max":{"secs":0,"nanos":21760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":33587},"median":{"secs":0,"nanos":34816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":30208},"max":{"secs":0,"nanos":35328}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":54860},"median":{"secs":0,"nanos":56320},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":50432},"max":{"secs":0,"nanos":58368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":62438},"median":{"secs":0,"nanos":63232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":57856},"max":{"secs":0,"nanos":65024}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":23859},"median":{"secs":0,"nanos":23808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":23296},"max":{"secs":0,"nanos":25088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":32588},"median":{"secs":0,"nanos":33024},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":29696},"max":{"secs":0,"nanos":34816}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":64,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":84659},"median":{"secs":0,"nanos":84480},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":83456},"max":{"secs":0,"nanos":87040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":95820},"median":{"secs":0,"nanos":96000},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":94976},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":106035},"median":{"secs":0,"nanos":105728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":104960},"max":{"secs":0,"nanos":108288}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1168512},"median":{"secs":0,"nanos":590592},"variance":{"secs":0,"nanos":513},"min":{"secs":0,"nanos":585472},"max":{"secs":0,"nanos":2264576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1024358},"median":{"secs":0,"nanos":652544},"variance":{"secs":0,"nanos":404},"min":{"secs":0,"nanos":580096},"max":{"secs":0,"nanos":2198784}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":33049},"median":{"secs":0,"nanos":33280},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":32256},"max":{"secs":0,"nanos":33792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49536},"median":{"secs":0,"nanos":49664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":48640},"max":{"secs":0,"nanos":51456}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":60697},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":56064},"max":{"secs":0,"nanos":68608}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3159936},"median":{"secs":0,"nanos":2837504},"variance":{"secs":0,"nanos":549},"min":{"secs":0,"nanos":2586368},"max":{"secs":0,"nanos":4791296}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3912883},"median":{"secs":0,"nanos":4368384},"variance":{"secs":0,"nanos":620},"min":{"secs":0,"nanos":2815744},"max":{"secs":0,"nanos":4789504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4364313},"median":{"secs":0,"nanos":4728320},"variance":{"secs":0,"nanos":418},"min":{"secs":0,"nanos":3090688},"max":{"secs":0,"nanos":4894208}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":663577},"median":{"secs":0,"nanos":396800},"variance":{"secs":0,"nanos":292},"min":{"secs":0,"nanos":390656},"max":{"secs":0,"nanos":1881088}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":477593},"median":{"secs":0,"nanos":477440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":475904},"max":{"secs":0,"nanos":481792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1051648},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":491776},"max":{"secs":0,"nanos":1971712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4388812},"median":{"secs":0,"nanos":4470272},"variance":{"secs":0,"nanos":105},"min":{"secs":0,"nanos":3484416},"max":{"secs":0,"nanos":4717056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4677811},"median":{"secs":0,"nanos":4743936},"variance":{"secs":0,"nanos":87},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5408179},"median":{"secs":0,"nanos":5401088},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":5086720},"max":{"secs":0,"nanos":5629952}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":89216230},"median":{"secs":0,"nanos":89440256},"variance":{"secs":0,"nanos":360},"min":{"secs":0,"nanos":88281600},"max":{"secs":0,"nanos":89995776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":90050073},"median":{"secs":0,"nanos":90768640},"variance":{"secs":0,"nanos":3784},"min":{"secs":0,"nanos":85283840},"max":{"secs":0,"nanos":92812544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2846592},"median":{"secs":0,"nanos":2911744},"variance":{"secs":0,"nanos":38},"min":{"secs":0,"nanos":2603776},"max":{"secs":0,"nanos":3213824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3134438},"median":{"secs":0,"nanos":3144704},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":2890240},"max":{"secs":0,"nanos":3370752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4747571},"median":{"secs":0,"nanos":4781824},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":4491008},"max":{"secs":0,"nanos":5148672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12969395},"median":{"secs":0,"nanos":12839424},"variance":{"secs":0,"nanos":482},"min":{"secs":0,"nanos":11920640},"max":{"secs":0,"nanos":14241536}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":12890009},"median":{"secs":0,"nanos":12916736},"variance":{"secs":0,"nanos":402},"min":{"secs":0,"nanos":12069888},"max":{"secs":0,"nanos":14033664}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":457395},"median":{"secs":0,"nanos":459008},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":448256},"max":{"secs":0,"nanos":466432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1108249},"median":{"secs":0,"nanos":567808},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":556032},"max":{"secs":0,"nanos":2232320}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1257881},"median":{"secs":0,"nanos":1859072},"variance":{"secs":0,"nanos":500},"min":{"secs":0,"nanos":551168},"max":{"secs":0,"nanos":2175488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":154393},"median":{"secs":0,"nanos":154112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152576},"max":{"secs":0,"nanos":157184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":171008},"median":{"secs":0,"nanos":170752},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":169984},"max":{"secs":0,"nanos":174080}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":176844},"median":{"secs":0,"nanos":177152},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":175104},"max":{"secs":0,"nanos":178944}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2747673},"median":{"secs":0,"nanos":2727424},"variance":{"secs":0,"nanos":15},"min":{"secs":0,"nanos":2589952},"max":{"secs":0,"nanos":2972928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3355545},"median":{"secs":0,"nanos":3318784},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":3174144},"max":{"secs":0,"nanos":3715584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4769228},"median":{"secs":0,"nanos":4724992},"variance":{"secs":0,"nanos":23},"min":{"secs":0,"nanos":4530176},"max":{"secs":0,"nanos":5030144}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":22860083},"median":{"secs":0,"nanos":22610432},"variance":{"secs":0,"nanos":692},"min":{"secs":0,"nanos":22076928},"max":{"secs":0,"nanos":24517888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":23029324},"median":{"secs":0,"nanos":23006208},"variance":{"secs":0,"nanos":351},"min":{"secs":0,"nanos":22123776},"max":{"secs":0,"nanos":23974144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2768409},"median":{"secs":0,"nanos":2835200},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2596096},"max":{"secs":0,"nanos":2918912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2956953},"median":{"secs":0,"nanos":2889984},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":2803456},"max":{"secs":0,"nanos":3351040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3377254},"median":{"secs":0,"nanos":3007232},"variance":{"secs":0,"nanos":408},"min":{"secs":0,"nanos":2758400},"max":{"secs":0,"nanos":4370176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3361459},"median":{"secs":0,"nanos":3155200},"variance":{"secs":0,"nanos":386},"min":{"secs":0,"nanos":2711552},"max":{"secs":0,"nanos":4545024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4098585},"median":{"secs":0,"nanos":4202496},"variance":{"secs":0,"nanos":29},"min":{"secs":0,"nanos":3863552},"max":{"secs":0,"nanos":4319232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":8192,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2723609},"median":{"secs":0,"nanos":2778112},"variance":{"secs":0,"nanos":9},"min":{"secs":0,"nanos":2591744},"max":{"secs":0,"nanos":2860032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2994636},"median":{"secs":0,"nanos":2810368},"variance":{"secs":0,"nanos":242},"min":{"secs":0,"nanos":2573824},"max":{"secs":0,"nanos":4035584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3095347},"median":{"secs":0,"nanos":2846720},"variance":{"secs":0,"nanos":470},"min":{"secs":0,"nanos":2555904},"max":{"secs":0,"nanos":4527360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2918297},"median":{"secs":0,"nanos":3033088},"variance":{"secs":0,"nanos":178},"min":{"secs":0,"nanos":1715712},"max":{"secs":0,"nanos":3332352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4619648},"median":{"secs":0,"nanos":4735744},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":4385024},"max":{"secs":0,"nanos":4851712}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":29465},"median":{"secs":0,"nanos":31744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":19968},"max":{"secs":0,"nanos":32256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":35584},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":24576},"max":{"secs":0,"nanos":39680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":48486},"median":{"secs":0,"nanos":50176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":41984},"max":{"secs":0,"nanos":50688}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":151884},"median":{"secs":0,"nanos":151808},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":148992},"max":{"secs":0,"nanos":156160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":265267},"median":{"secs":0,"nanos":265984},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":262912},"max":{"secs":0,"nanos":267008}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":67865},"median":{"secs":0,"nanos":68096},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":67072},"max":{"secs":0,"nanos":68608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":85760},"median":{"secs":0,"nanos":86016},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":84992},"max":{"secs":0,"nanos":86528}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":153395},"median":{"secs":0,"nanos":153600},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":152320},"max":{"secs":0,"nanos":154112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1614310},"median":{"secs":0,"nanos":1999104},"variance":{"secs":0,"nanos":523},"min":{"secs":0,"nanos":746496},"max":{"secs":0,"nanos":2496768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2211968},"median":{"secs":0,"nanos":2457856},"variance":{"secs":0,"nanos":411},"min":{"secs":0,"nanos":977152},"max":{"secs":0,"nanos":2748928}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":171699},"median":{"secs":0,"nanos":171520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":170752},"max":{"secs":0,"nanos":173568}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":203161},"median":{"secs":0,"nanos":203264},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":201984},"max":{"secs":0,"nanos":204032}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":265164},"median":{"secs":0,"nanos":265216},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":264704},"max":{"secs":0,"nanos":265728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":5617945},"median":{"secs":0,"nanos":5809408},"variance":{"secs":0,"nanos":459},"min":{"secs":0,"nanos":4139264},"max":{"secs":0,"nanos":6855680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6383744},"median":{"secs":0,"nanos":7002368},"variance":{"secs":0,"nanos":2283},"min":{"secs":0,"nanos":3175936},"max":{"secs":0,"nanos":8116736}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":854041},"median":{"secs":0,"nanos":588032},"variance":{"secs":0,"nanos":272},"min":{"secs":0,"nanos":586240},"max":{"secs":0,"nanos":2317056}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1432345},"median":{"secs":0,"nanos":1067520},"variance":{"secs":0,"nanos":481},"min":{"secs":0,"nanos":1064960},"max":{"secs":0,"nanos":2841600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1408563},"median":{"secs":0,"nanos":1902848},"variance":{"secs":0,"nanos":627},"min":{"secs":0,"nanos":624896},"max":{"secs":0,"nanos":2342400}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":17843},"median":{"secs":0,"nanos":19456},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15360},"max":{"secs":0,"nanos":20736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":96358},"median":{"secs":0,"nanos":99584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90624},"max":{"secs":0,"nanos":99840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":11161},"median":{"secs":0,"nanos":12544},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":13312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":49433},"median":{"secs":0,"nanos":50688},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46848},"max":{"secs":0,"nanos":52224}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":4761},"median":{"secs":0,"nanos":4864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":3840},"max":{"secs":0,"nanos":5376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10342},"median":{"secs":0,"nanos":9728},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8960},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":11392},"median":{"secs":0,"nanos":11520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11008},"max":{"secs":0,"nanos":11776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":15718},"median":{"secs":0,"nanos":17664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":12032},"max":{"secs":0,"nanos":18176}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":9779},"median":{"secs":0,"nanos":10240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":8448},"max":{"secs":0,"nanos":10496}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":16512},"median":{"secs":0,"nanos":16128},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":15616},"max":{"secs":0,"nanos":18688}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":12211},"median":{"secs":0,"nanos":12288},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":11264},"max":{"secs":0,"nanos":12800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":16307},"median":{"secs":0,"nanos":17408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":13824},"max":{"secs":0,"nanos":17920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":24755},"median":{"secs":0,"nanos":23296},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":28928}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":56883},"median":{"secs":0,"nanos":58880},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":52992},"max":{"secs":0,"nanos":59392}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":15078},"median":{"secs":0,"nanos":15104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":14592},"max":{"secs":0,"nanos":15616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18764},"median":{"secs":0,"nanos":18432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":17152},"max":{"secs":0,"nanos":20480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":32,"n":4096,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":81382},"median":{"secs":0,"nanos":81664},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":80384},"max":{"secs":0,"nanos":81920}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":96102},"median":{"secs":0,"nanos":96256},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":95232},"max":{"secs":0,"nanos":96768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":103552},"median":{"secs":0,"nanos":103424},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102912},"max":{"secs":0,"nanos":104192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":230092},"median":{"secs":0,"nanos":230144},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":229120},"max":{"secs":0,"nanos":230912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":235468},"median":{"secs":0,"nanos":235520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":234496},"max":{"secs":0,"nanos":236288}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":27008},"median":{"secs":0,"nanos":27136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":26112},"max":{"secs":0,"nanos":27648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":35788},"median":{"secs":0,"nanos":35584},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":35328},"max":{"secs":0,"nanos":36608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":48716},"median":{"secs":0,"nanos":47360},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":46336},"max":{"secs":0,"nanos":53760}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1744870},"median":{"secs":0,"nanos":2263040},"variance":{"secs":0,"nanos":781},"min":{"secs":0,"nanos":699648},"max":{"secs":0,"nanos":2929152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2141081},"median":{"secs":0,"nanos":2369280},"variance":{"secs":0,"nanos":456},"min":{"secs":0,"nanos":866304},"max":{"secs":0,"nanos":2792192}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2445542},"median":{"secs":0,"nanos":2424320},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":2179584},"max":{"secs":0,"nanos":2655232}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":4096,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":287923},"median":{"secs":0,"nanos":287744},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":286208},"max":{"secs":0,"nanos":291840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":292531},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":288512},"max":{"secs":0,"nanos":293888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":314700},"median":{"secs":0,"nanos":316160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":297728},"max":{"secs":0,"nanos":322048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":2858675},"median":{"secs":0,"nanos":2860032},"variance":{"secs":0,"nanos":39},"min":{"secs":0,"nanos":2631424},"max":{"secs":0,"nanos":3207936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3077504},"median":{"secs":0,"nanos":3134976},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":2892032},"max":{"secs":0,"nanos":3203840}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":3393510},"median":{"secs":0,"nanos":3485696},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":3169280},"max":{"secs":0,"nanos":3584768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":46301209},"median":{"secs":0,"nanos":46567680},"variance":{"secs":0,"nanos":917},"min":{"secs":0,"nanos":45009152},"max":{"secs":0,"nanos":47776256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":48641536},"median":{"secs":0,"nanos":49084928},"variance":{"secs":0,"nanos":897},"min":{"secs":0,"nanos":47056896},"max":{"secs":0,"nanos":49919488}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1511398},"median":{"secs":0,"nanos":2010112},"variance":{"secs":0,"nanos":498},"min":{"secs":0,"nanos":811264},"max":{"secs":0,"nanos":2388224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1580953},"median":{"secs":0,"nanos":2100736},"variance":{"secs":0,"nanos":484},"min":{"secs":0,"nanos":894208},"max":{"secs":0,"nanos":2541312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2823014},"median":{"secs":0,"nanos":2794240},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":2614272},"max":{"secs":0,"nanos":3104768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":7104921},"median":{"secs":0,"nanos":7248640},"variance":{"secs":0,"nanos":527},"min":{"secs":0,"nanos":5942784},"max":{"secs":0,"nanos":8421632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":7807820},"median":{"secs":0,"nanos":7682304},"variance":{"secs":0,"nanos":329},"min":{"secs":0,"nanos":7330048},"max":{"secs":0,"nanos":9185536}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":241228},"median":{"secs":0,"nanos":242176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":237056},"max":{"secs":0,"nanos":243200}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":284313},"median":{"secs":0,"nanos":284160},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":283136},"max":{"secs":0,"nanos":286464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":291609},"median":{"secs":0,"nanos":292864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":287232},"max":{"secs":0,"nanos":294144}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":4096,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":123417},"median":{"secs":0,"nanos":123136},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":121600},"max":{"secs":0,"nanos":125440}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":131097},"median":{"secs":0,"nanos":130816},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":132352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":135321},"median":{"secs":0,"nanos":138240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":126976},"max":{"secs":0,"nanos":139776}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1374924},"median":{"secs":0,"nanos":1930240},"variance":{"secs":0,"nanos":447},"min":{"secs":0,"nanos":709888},"max":{"secs":0,"nanos":2182400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1608448},"median":{"secs":0,"nanos":2105088},"variance":{"secs":0,"nanos":487},"min":{"secs":0,"nanos":913408},"max":{"secs":0,"nanos":2474240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3066265},"median":{"secs":0,"nanos":3121664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":2834688},"max":{"secs":0,"nanos":3227904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":11702323},"median":{"secs":0,"nanos":12060928},"variance":{"secs":0,"nanos":740},"min":{"secs":0,"nanos":10090240},"max":{"secs":0,"nanos":12796416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12374656},"median":{"secs":0,"nanos":12242688},"variance":{"secs":0,"nanos":279},"min":{"secs":0,"nanos":11604992},"max":{"secs":0,"nanos":13507584}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1454131},"median":{"secs":0,"nanos":1996544},"variance":{"secs":0,"nanos":430},"min":{"secs":0,"nanos":798208},"max":{"secs":0,"nanos":2208256}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1685990},"median":{"secs":0,"nanos":2030848},"variance":{"secs":0,"nanos":497},"min":{"secs":0,"nanos":817920},"max":{"secs":0,"nanos":2525184}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1548697},"median":{"secs":0,"nanos":2036480},"variance":{"secs":0,"nanos":489},"min":{"secs":0,"nanos":856064},"max":{"secs":0,"nanos":2447104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2235340},"median":{"secs":0,"nanos":2341376},"variance":{"secs":0,"nanos":251},"min":{"secs":0,"nanos":879104},"max":{"secs":0,"nanos":2958848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2489856},"median":{"secs":0,"nanos":2488320},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":2264832},"max":{"secs":0,"nanos":2931456}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":4096,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1403494},"median":{"secs":0,"nanos":1913600},"variance":{"secs":0,"nanos":493},"min":{"secs":0,"nanos":714496},"max":{"secs":0,"nanos":2437888}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":1716454},"median":{"secs":0,"nanos":2025472},"variance":{"secs":0,"nanos":460},"min":{"secs":0,"nanos":704000},"max":{"secs":0,"nanos":2404608}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":1742336},"median":{"secs":0,"nanos":2047488},"variance":{"secs":0,"nanos":520},"min":{"secs":0,"nanos":684544},"max":{"secs":0,"nanos":2651904}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1674419},"median":{"secs":0,"nanos":2202112},"variance":{"secs":0,"nanos":669},"min":{"secs":0,"nanos":866560},"max":{"secs":0,"nanos":2809344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3120486},"median":{"secs":0,"nanos":3109120},"variance":{"secs":0,"nanos":63},"min":{"secs":0,"nanos":2810624},"max":{"secs":0,"nanos":3551744}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":4096,"lhs_pow2_factor":0,"rhs_pow2_factor":0,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":141696},"median":{"secs":0,"nanos":148224},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":130560},"max":{"secs":0,"nanos":149760}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":149785},"median":{"secs":0,"nanos":156928},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":137984},"max":{"secs":0,"nanos":157696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1397478},"median":{"secs":0,"nanos":1858304},"variance":{"secs":0,"nanos":412},"min":{"secs":0,"nanos":762368},"max":{"secs":0,"nanos":2256640}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1442457},"median":{"secs":0,"nanos":1890048},"variance":{"secs":0,"nanos":446},"min":{"secs":0,"nanos":793856},"max":{"secs":0,"nanos":2423808}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2390169},"median":{"secs":0,"nanos":2469632},"variance":{"secs":0,"nanos":25},"min":{"secs":0,"nanos":2235136},"max":{"secs":0,"nanos":2723328}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4303820},"median":{"secs":0,"nanos":4352768},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":4071936},"max":{"secs":0,"nanos":4536832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4471091},"median":{"secs":0,"nanos":4539904},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":4215040},"max":{"secs":0,"nanos":4889600}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6750694},"median":{"secs":0,"nanos":6797312},"variance":{"secs":0,"nanos":31},"min":{"secs":0,"nanos":6456576},"max":{"secs":0,"nanos":6977280}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":64,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":452070},"median":{"secs":0,"nanos":334592},"variance":{"secs":0,"nanos":131},"min":{"secs":0,"nanos":309760},"max":{"secs":0,"nanos":1537280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":530636},"median":{"secs":0,"nanos":394240},"variance":{"secs":0,"nanos":167},"min":{"secs":0,"nanos":393472},"max":{"secs":0,"nanos":1756416}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":931174},"median":{"secs":0,"nanos":505856},"variance":{"secs":0,"nanos":352},"min":{"secs":0,"nanos":500992},"max":{"secs":0,"nanos":1889280}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6408576},"median":{"secs":0,"nanos":6811136},"variance":{"secs":0,"nanos":486},"min":{"secs":0,"nanos":5229568},"max":{"secs":0,"nanos":7111424}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":6496025},"median":{"secs":0,"nanos":6904064},"variance":{"secs":0,"nanos":505},"min":{"secs":0,"nanos":5234176},"max":{"secs":0,"nanos":7331840}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":116812},"median":{"secs":0,"nanos":116736},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":114176},"max":{"secs":0,"nanos":120832}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":209689},"median":{"secs":0,"nanos":209408},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":207872},"max":{"secs":0,"nanos":214016}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":222873},"median":{"secs":0,"nanos":223232},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":221184},"max":{"secs":0,"nanos":224768}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4199270},"median":{"secs":0,"nanos":4310016},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":3977728},"max":{"secs":0,"nanos":4468992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":4398438},"median":{"secs":0,"nanos":4438784},"variance":{"secs":0,"nanos":37},"min":{"secs":0,"nanos":4174080},"max":{"secs":0,"nanos":4658176}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6667136},"median":{"secs":0,"nanos":6691328},"variance":{"secs":0,"nanos":40},"min":{"secs":0,"nanos":6347264},"max":{"secs":0,"nanos":7027968}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":308454},"median":{"secs":0,"nanos":303872},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":296192},"max":{"secs":0,"nanos":331520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":507852},"median":{"secs":0,"nanos":388864},"variance":{"secs":0,"nanos":127},"min":{"secs":0,"nanos":387840},"max":{"secs":0,"nanos":1579520}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":862924},"median":{"secs":0,"nanos":493824},"variance":{"secs":0,"nanos":319},"min":{"secs":0,"nanos":492032},"max":{"secs":0,"nanos":1786368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":2512588},"median":{"secs":0,"nanos":2562560},"variance":{"secs":0,"nanos":12},"min":{"secs":0,"nanos":2298368},"max":{"secs":0,"nanos":2671104}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2504243},"median":{"secs":0,"nanos":2598912},"variance":{"secs":0,"nanos":30},"min":{"secs":0,"nanos":2280704},"max":{"secs":0,"nanos":2753024}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":32,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":94284},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":88320},"max":{"secs":0,"nanos":109312}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":140672},"median":{"secs":0,"nanos":139520},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":138752},"max":{"secs":0,"nanos":153344}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":144153},"median":{"secs":0,"nanos":143616},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":142592},"max":{"secs":0,"nanos":149248}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":32,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1093657},"median":{"secs":0,"nanos":742912},"variance":{"secs":0,"nanos":576},"min":{"secs":0,"nanos":686336},"max":{"secs":0,"nanos":3087616}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1415398},"median":{"secs":0,"nanos":1443584},"variance":{"secs":0,"nanos":149},"min":{"secs":0,"nanos":911872},"max":{"secs":0,"nanos":2127360}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1982233},"median":{"secs":0,"nanos":2361344},"variance":{"secs":0,"nanos":998},"min":{"secs":0,"nanos":629760},"max":{"secs":0,"nanos":3198464}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":10076851},"median":{"secs":0,"nanos":9892864},"variance":{"secs":0,"nanos":2699},"min":{"secs":0,"nanos":7765760},"max":{"secs":0,"nanos":12916224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":14860697},"median":{"secs":0,"nanos":15601664},"variance":{"secs":0,"nanos":3901},"min":{"secs":0,"nanos":10742272},"max":{"secs":0,"nanos":18316800}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":64,"k":512,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":498227},"median":{"secs":0,"nanos":498176},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":497408},"max":{"secs":0,"nanos":499968}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1681177},"median":{"secs":0,"nanos":1527040},"variance":{"secs":0,"nanos":113},"min":{"secs":0,"nanos":1404416},"max":{"secs":0,"nanos":2494208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3201024},"median":{"secs":0,"nanos":3132928},"variance":{"secs":0,"nanos":265},"min":{"secs":0,"nanos":2769664},"max":{"secs":0,"nanos":4104704}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":12033484},"median":{"secs":0,"nanos":12039936},"variance":{"secs":0,"nanos":151},"min":{"secs":0,"nanos":11570944},"max":{"secs":0,"nanos":13067776}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21854208},"median":{"secs":0,"nanos":15919360},"variance":{"secs":0,"nanos":181237},"min":{"secs":0,"nanos":8983552},"max":{"secs":0,"nanos":40704512}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":32768,"n":128,"k":1024,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1534643},"median":{"secs":0,"nanos":1439488},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":1384448},"max":{"secs":0,"nanos":1799936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1964569},"median":{"secs":0,"nanos":1844480},"variance":{"secs":0,"nanos":45},"min":{"secs":0,"nanos":1843968},"max":{"secs":0,"nanos":2473728}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5438182},"median":{"secs":0,"nanos":5225728},"variance":{"secs":0,"nanos":475},"min":{"secs":0,"nanos":4580096},"max":{"secs":0,"nanos":7188224}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":35342156},"median":{"secs":0,"nanos":29753600},"variance":{"secs":0,"nanos":294078},"min":{"secs":0,"nanos":27257344},"max":{"secs":0,"nanos":86686208}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":34319206},"median":{"secs":0,"nanos":34471424},"variance":{"secs":0,"nanos":236},"min":{"secs":0,"nanos":33640704},"max":{"secs":0,"nanos":35090176}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":17967257},"median":{"secs":0,"nanos":14934784},"variance":{"secs":0,"nanos":26716},"min":{"secs":0,"nanos":12687872},"max":{"secs":0,"nanos":25114368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":13458688},"median":{"secs":0,"nanos":15065344},"variance":{"secs":0,"nanos":6803},"min":{"secs":0,"nanos":10482176},"max":{"secs":0,"nanos":16441856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":16014259},"median":{"secs":0,"nanos":16199424},"variance":{"secs":0,"nanos":603},"min":{"secs":0,"nanos":14998528},"max":{"secs":0,"nanos":17384960}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":38067},"median":{"secs":0,"nanos":37888},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":36864},"max":{"secs":0,"nanos":40448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":104166},"median":{"secs":0,"nanos":102912},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":102144},"max":{"secs":0,"nanos":113408}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":39731},"median":{"secs":0,"nanos":29184},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":28672},"max":{"secs":0,"nanos":69632}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":219136},"median":{"secs":0,"nanos":211712},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":198912},"max":{"secs":0,"nanos":240896}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":128,"k":4,"lhs_pow2_factor":2,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":25548},"median":{"secs":0,"nanos":24064},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":22784},"max":{"secs":0,"nanos":37376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":72064},"median":{"secs":0,"nanos":57856},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":54016},"max":{"secs":0,"nanos":104448}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":179891},"median":{"secs":0,"nanos":175104},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":172032},"max":{"secs":0,"nanos":212736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":433177},"median":{"secs":0,"nanos":299008},"variance":{"secs":0,"nanos":145},"min":{"secs":0,"nanos":294912},"max":{"secs":0,"nanos":1574656}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":4,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":2,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":31001},"median":{"secs":0,"nanos":29696},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":27392},"max":{"secs":0,"nanos":48128}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":167782},"median":{"secs":0,"nanos":164864},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":151040},"max":{"secs":0,"nanos":197376}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":94336},"median":{"secs":0,"nanos":93440},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":90880},"max":{"secs":0,"nanos":100352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":198169},"median":{"secs":0,"nanos":187904},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":183808},"max":{"secs":0,"nanos":236800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":319001},"median":{"secs":0,"nanos":323840},"variance":{"secs":0,"nanos":2},"min":{"secs":0,"nanos":177152},"max":{"secs":0,"nanos":377856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":618905},"median":{"secs":0,"nanos":555520},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":551936},"max":{"secs":0,"nanos":1112064}}}},{"Err":"Skip"},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":256,"n":128,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Small","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":0,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::naive<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":0,"computation":{"mean":{"secs":0,"nanos":191206},"median":{"secs":0,"nanos":186112},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":180224},"max":{"secs":0,"nanos":229376}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":340300},"median":{"secs":0,"nanos":319232},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":317440},"max":{"secs":0,"nanos":398592}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":"Skip"},{"Err":"Skip"}]}}
{"key":{"key":{"definition":{"m":128,"n":16384,"k":256,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1438643},"median":{"secs":0,"nanos":1330176},"variance":{"secs":0,"nanos":27},"min":{"secs":0,"nanos":1251840},"max":{"secs":0,"nanos":1670912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":5608217},"median":{"secs":0,"nanos":5342720},"variance":{"secs":0,"nanos":2626},"min":{"secs":0,"nanos":3538176},"max":{"secs":0,"nanos":8026368}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":6190105},"median":{"secs":0,"nanos":6143232},"variance":{"secs":0,"nanos":96},"min":{"secs":0,"nanos":5804544},"max":{"secs":0,"nanos":6816768}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6371609},"median":{"secs":0,"nanos":6192640},"variance":{"secs":0,"nanos":3512},"min":{"secs":0,"nanos":4190208},"max":{"secs":0,"nanos":9163264}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21803366},"median":{"secs":0,"nanos":6904576},"variance":{"secs":0,"nanos":884883},"min":{"secs":0,"nanos":6030336},"max":{"secs":0,"nanos":94882048}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":16384,"n":256,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}},"matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":996121},"median":{"secs":0,"nanos":963584},"variance":{"secs":0,"nanos":5},"min":{"secs":0,"nanos":942336},"max":{"secs":0,"nanos":1204736}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1830374},"median":{"secs":0,"nanos":1594624},"variance":{"secs":0,"nanos":340},"min":{"secs":0,"nanos":1521664},"max":{"secs":0,"nanos":3479040}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":2140979},"median":{"secs":0,"nanos":1894656},"variance":{"secs":0,"nanos":393},"min":{"secs":0,"nanos":1652992},"max":{"secs":0,"nanos":3760128}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Row Major layout is supported for Lhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":32768,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":2,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":3693337},"median":{"secs":0,"nanos":3740928},"variance":{"secs":0,"nanos":67},"min":{"secs":0,"nanos":3436288},"max":{"secs":0,"nanos":4201472}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":14573132},"median":{"secs":0,"nanos":16848128},"variance":{"secs":0,"nanos":18786},"min":{"secs":0,"nanos":9231872},"max":{"secs":0,"nanos":19716352}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19431116},"median":{"secs":0,"nanos":18908672},"variance":{"secs":0,"nanos":7042},"min":{"secs":0,"nanos":16677888},"max":{"secs":0,"nanos":26026240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":16384,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3669836},"median":{"secs":0,"nanos":3518464},"variance":{"secs":0,"nanos":609},"min":{"secs":0,"nanos":2843136},"max":{"secs":0,"nanos":5019392}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":4430361},"median":{"secs":0,"nanos":4444928},"variance":{"secs":0,"nanos":716},"min":{"secs":0,"nanos":3609088},"max":{"secs":0,"nanos":6687232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8967475},"median":{"secs":0,"nanos":9475328},"variance":{"secs":0,"nanos":3323},"min":{"secs":0,"nanos":6718720},"max":{"secs":0,"nanos":11831552}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":4096,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6429081},"median":{"secs":0,"nanos":6463744},"variance":{"secs":0,"nanos":50},"min":{"secs":0,"nanos":6057472},"max":{"secs":0,"nanos":6874112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6845235},"median":{"secs":0,"nanos":6836224},"variance":{"secs":0,"nanos":21},"min":{"secs":0,"nanos":6551040},"max":{"secs":0,"nanos":7118848}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":8046489},"median":{"secs":0,"nanos":8049664},"variance":{"secs":0,"nanos":11},"min":{"secs":0,"nanos":7869952},"max":{"secs":0,"nanos":8253696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":83455462},"median":{"secs":0,"nanos":83596032},"variance":{"secs":0,"nanos":75},"min":{"secs":0,"nanos":83105280},"max":{"secs":0,"nanos":83922432}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":83787417},"median":{"secs":0,"nanos":83769856},"variance":{"secs":0,"nanos":49},"min":{"secs":0,"nanos":83507968},"max":{"secs":0,"nanos":84268800}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3702656},"median":{"secs":0,"nanos":3708928},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":3543552},"max":{"secs":0,"nanos":3963648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":10744550},"median":{"secs":0,"nanos":10586624},"variance":{"secs":0,"nanos":120},"min":{"secs":0,"nanos":10247680},"max":{"secs":0,"nanos":11462400}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":11976371},"median":{"secs":0,"nanos":12006912},"variance":{"secs":0,"nanos":185},"min":{"secs":0,"nanos":11264512},"max":{"secs":0,"nanos":12703232}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":13132902},"median":{"secs":0,"nanos":13205504},"variance":{"secs":0,"nanos":150},"min":{"secs":0,"nanos":12390912},"max":{"secs":0,"nanos":13904384}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":18075724},"median":{"secs":0,"nanos":22734336},"variance":{"secs":0,"nanos":50955},"min":{"secs":0,"nanos":7036416},"max":{"secs":0,"nanos":23796480}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":32768,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":831539},"median":{"secs":0,"nanos":836608},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":814080},"max":{"secs":0,"nanos":845824}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1084313},"median":{"secs":0,"nanos":1070336},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1068288},"max":{"secs":0,"nanos":1212672}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":1091737},"median":{"secs":0,"nanos":1077248},"variance":{"secs":0,"nanos":1},"min":{"secs":0,"nanos":1075712},"max":{"secs":0,"nanos":1217024}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":16384,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":254950},"median":{"secs":0,"nanos":254464},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":251648},"max":{"secs":0,"nanos":263680}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":307916},"median":{"secs":0,"nanos":307968},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":307200},"max":{"secs":0,"nanos":308992}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":318438},"median":{"secs":0,"nanos":318720},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":316672},"max":{"secs":0,"nanos":320000}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":2048,"n":64,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2854348},"median":{"secs":0,"nanos":2741760},"variance":{"secs":0,"nanos":24},"min":{"secs":0,"nanos":2739712},"max":{"secs":0,"nanos":3145472}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":4107596},"median":{"secs":0,"nanos":4123904},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":3842560},"max":{"secs":0,"nanos":4497152}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6656665},"median":{"secs":0,"nanos":6635520},"variance":{"secs":0,"nanos":7},"min":{"secs":0,"nanos":6530560},"max":{"secs":0,"nanos":6785024}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":20676684},"median":{"secs":0,"nanos":20399104},"variance":{"secs":0,"nanos":317},"min":{"secs":0,"nanos":20233728},"max":{"secs":0,"nanos":21930240}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":20983936},"median":{"secs":0,"nanos":20862720},"variance":{"secs":0,"nanos":297},"min":{"secs":0,"nanos":20465152},"max":{"secs":0,"nanos":21934336}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2853990},"median":{"secs":0,"nanos":2767616},"variance":{"secs":0,"nanos":13},"min":{"secs":0,"nanos":2766592},"max":{"secs":0,"nanos":3080960}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":3054105},"median":{"secs":0,"nanos":2935296},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":2909184},"max":{"secs":0,"nanos":3451648}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3178214},"median":{"secs":0,"nanos":3114496},"variance":{"secs":0,"nanos":82},"min":{"secs":0,"nanos":2965760},"max":{"secs":0,"nanos":3795712}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":3324288},"median":{"secs":0,"nanos":3375872},"variance":{"secs":0,"nanos":18},"min":{"secs":0,"nanos":3143424},"max":{"secs":0,"nanos":3499008}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5457740},"median":{"secs":0,"nanos":5553664},"variance":{"secs":0,"nanos":20},"min":{"secs":0,"nanos":5283840},"max":{"secs":0,"nanos":5610240}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":32,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":2936627},"median":{"secs":0,"nanos":2848256},"variance":{"secs":0,"nanos":33},"min":{"secs":0,"nanos":2842624},"max":{"secs":0,"nanos":3357696}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":3069926},"median":{"secs":0,"nanos":2890496},"variance":{"secs":0,"nanos":78},"min":{"secs":0,"nanos":2880512},"max":{"secs":0,"nanos":3554560}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2979609},"median":{"secs":0,"nanos":3013888},"variance":{"secs":0,"nanos":16},"min":{"secs":0,"nanos":2763776},"max":{"secs":0,"nanos":3126272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":20880716},"median":{"secs":0,"nanos":25651968},"variance":{"secs":0,"nanos":75564},"min":{"secs":0,"nanos":7217664},"max":{"secs":0,"nanos":28026112}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19703961},"median":{"secs":0,"nanos":26590464},"variance":{"secs":0,"nanos":110438},"min":{"secs":0,"nanos":4252672},"max":{"secs":0,"nanos":29412352}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":128,"n":32,"k":16384,"lhs_pow2_factor":3,"rhs_pow2_factor":3,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":4,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":546841},"median":{"secs":0,"nanos":546560},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":544768},"max":{"secs":0,"nanos":550912}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":584857},"median":{"secs":0,"nanos":586240},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":569856},"max":{"secs":0,"nanos":594944}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2935526},"median":{"secs":0,"nanos":2813184},"variance":{"secs":0,"nanos":85},"min":{"secs":0,"nanos":2670080},"max":{"secs":0,"nanos":3495936}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5875507},"median":{"secs":0,"nanos":5905152},"variance":{"secs":0,"nanos":32},"min":{"secs":0,"nanos":5494272},"max":{"secs":0,"nanos":6170624}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":19583360},"median":{"secs":0,"nanos":32427776},"variance":{"secs":0,"nanos":187140},"min":{"secs":0,"nanos":3954176},"max":{"secs":0,"nanos":33453056}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":8192,"k":128,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":1592473},"median":{"secs":0,"nanos":1396736},"variance":{"secs":0,"nanos":324},"min":{"secs":0,"nanos":1381120},"max":{"secs":0,"nanos":3299584}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":5166720},"median":{"secs":0,"nanos":4754432},"variance":{"secs":0,"nanos":1522},"min":{"secs":0,"nanos":3846656},"max":{"secs":0,"nanos":7577856}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":5461350},"median":{"secs":0,"nanos":5367808},"variance":{"secs":0,"nanos":1295},"min":{"secs":0,"nanos":3847936},"max":{"secs":0,"nanos":7802368}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":1024,"n":128,"k":8192,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":1848908},"median":{"secs":0,"nanos":1743360},"variance":{"secs":0,"nanos":22},"min":{"secs":0,"nanos":1740544},"max":{"secs":0,"nanos":2204160}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4710451},"median":{"secs":0,"nanos":4727296},"variance":{"secs":0,"nanos":46},"min":{"secs":0,"nanos":4426240},"max":{"secs":0,"nanos":5119744}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":10421862},"median":{"secs":0,"nanos":10817280},"variance":{"secs":0,"nanos":413},"min":{"secs":0,"nanos":9217536},"max":{"secs":0,"nanos":11113984}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":12941721},"median":{"secs":0,"nanos":12724736},"variance":{"secs":0,"nanos":1453},"min":{"secs":0,"nanos":11183872},"max":{"secs":0,"nanos":15297792}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":21671884},"median":{"secs":0,"nanos":23159552},"variance":{"secs":0,"nanos":29648},"min":{"secs":0,"nanos":6394624},"max":{"secs":0,"nanos":27064576}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":8192,"k":64,"lhs_pow2_factor":3,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":"Contiguous"},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":1,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":178508},"median":{"secs":0,"nanos":178432},"variance":{"secs":0,"nanos":0},"min":{"secs":0,"nanos":176640},"max":{"secs":0,"nanos":181504}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":645094},"median":{"secs":0,"nanos":591872},"variance":{"secs":0,"nanos":17},"min":{"secs":0,"nanos":565504},"max":{"secs":0,"nanos":997120}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":777164},"median":{"secs":0,"nanos":635136},"variance":{"secs":0,"nanos":142},"min":{"secs":0,"nanos":608768},"max":{"secs":0,"nanos":1900544}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because the config is invalid: \"Only Col Major layout is supported for Rhs\"\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
{"key":{"key":{"definition":{"m":512,"n":64,"k":8192,"lhs_pow2_factor":1,"rhs_pow2_factor":1,"elem_lhs":{"elem":{"Float":"F32"},"quantized":false},"elem_rhs":{"elem":{"Float":"F32"},"quantized":false},"elem_out":{"elem":{"Float":"F32"},"quantized":false},"matrix_layout_lhs":"Contiguous","matrix_layout_rhs":{"MildlyPermuted":{"transposed":true,"batch_swap":false}}},"analysis":{"scale_global":"Large","kind":"General"}},"checksum":"45afdc4f1d5ab44f4bd2bd66b0e8e524"},"value":{"fastest_index":5,"results":[{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_unit<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":5,"computation":{"mean":{"secs":0,"nanos":2245222},"median":{"secs":0,"nanos":1720576},"variance":{"secs":0,"nanos":1017},"min":{"secs":0,"nanos":1384448},"max":{"secs":0,"nanos":4108800}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::double_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":4,"computation":{"mean":{"secs":0,"nanos":4201753},"median":{"secs":0,"nanos":4007680},"variance":{"secs":0,"nanos":289},"min":{"secs":0,"nanos":3652608},"max":{"secs":0,"nanos":5258752}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_vec_mat<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":3,"computation":{"mean":{"secs":0,"nanos":4434969},"median":{"secs":0,"nanos":4115456},"variance":{"secs":0,"nanos":884},"min":{"secs":0,"nanos":3677440},"max":{"secs":0,"nanos":7000576}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_max<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":2,"computation":{"mean":{"secs":0,"nanos":6815820},"median":{"secs":0,"nanos":6622976},"variance":{"secs":0,"nanos":225},"min":{"secs":0,"nanos":6160640},"max":{"secs":0,"nanos":7798272}}}},{"Ok":{"name":"cubecl_runtime::tune::function_tunable::FunctionTunable<burn_cubecl::kernel::matmul::tune::base::simple_unit_min<cubecl_wgpu::runtime::WgpuRuntime, f32>, fn(burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>, burn_cubecl::tensor::base::CubeTensor<cubecl_wgpu::runtime::WgpuRuntime>) -> core::result::Result<(), alloc::string::String>>","index":1,"computation":{"mean":{"secs":0,"nanos":6874035},"median":{"secs":0,"nanos":6903552},"variance":{"secs":0,"nanos":70},"min":{"secs":0,"nanos":6480128},"max":{"secs":0,"nanos":7405568}}}},{"Err":"Skip"},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}},{"Err":{"Unknown":"Unable to launch matmul because a required feature is unavailable: Cmma on lhs Scalar(Float(F32)) rhs Scalar(Float(F32)) and output Scalar(Float(F32)) with shape m=16, n=16, k=8 not supported.\n\n"}}]}}
